Rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr

adaLasso <- function(data,npInds = c(),preAlpha = 0) {
  
  'Adaptive LASSO for time-series'
  library("glmnet")
  
  #zoo <- na.omit(zoo)
  dataDF <- squareData(as.data.frame(data))
  
  T = NROW(dataDF)
  K = NCOL(dataDF)
  if (T < K) {
    preAlpha = 0
  }
  
  #Manual regression of 1st column on others
  #regressors <- names(dataDF)
  #form = paste(regressors[1],"~ 1 + .")
  #preOLS <- lm(formula(form),data=dataDF)
  
  #Ridge regression for preliminary weights
  y = as.matrix(dataDF[,1])
  x = as.matrix(dataDF[,-1])
  # preReg <- lm(dataDF) #R runs the first column by default
  
  penVec = rep(1,NCOL(x))
  if (is.null(npInds) == FALSE) {
    penVec[npInds] = 0
  }
  
  preReg <- glmnet(x,y,family="gaussian",alpha=preAlpha,intercept=TRUE,penalty.factor=penVec)
  preCoeffs <- preReg$beta
  nPath = ncol(preCoeffs)
  preCoeffs <- preCoeffs[,nPath] #ver como faz one-liner
  
  nx = length(preCoeffs)
  adaWeights = 1/abs(preCoeffs)
  if (is.null(npInds) == FALSE) {
    adaWeights[npInds] = 0
  }
  
  #infinite penalties
  excludeInds = which(adaWeights==Inf)
  adaWeights[excludeInds] = 1 #removing Inf weights to avoid problems
  # if (length(excludeInds) != 0) {
  #   x = x[,-excludeInds]
  # }
  # adaWeights = adaWeights[-excludeInds]
  
  #to run LASSO
  #adaWeights = rep(1,nx) 
  #excludeInds = rep()
  
  nLambdaProp = max(100,round(0.5*(K-1)))
  lassoObj <- glmnet(x,y,family="gaussian",alpha=1,exclude = excludeInds,
                     nlambda=nLambdaProp,penalty.factor=adaWeights,intercept=TRUE)
  
}

adaLassoTS <- function(zooObj,varUB = Inf,pen = NA) {

  
  'Adaptive LASSO for time-series'
  library("glmnet")
  #zoo <- na.omit(zoo)
  dataDF <- squareData(as.data.frame(zooObj))
  
  #Manual regression of 1st column on others
  #regressors <- names(dataDF)
  #form = paste(regressors[1],"~ 1 + .")
  #preOLS <- lm(formula(form),data=dataDF)
  
  #Ridge regression for preliminary weights
  y = as.matrix(dataDF[,1])
  x = as.matrix(dataDF[,-1])
  # preReg <- lm(dataDF) #R runs the first column by default
  preReg <- glmnet(x,y,family="gaussian",alpha=0.5,intercept=TRUE)
  preCoeffs <- preReg$beta
  nPath = ncol(preCoeffs)
  preCoeffs <- preCoeffs[,nPath] #ver como faz one-liner
  
  nx = length(preCoeffs)
  adaWeights = 1/abs(preCoeffs)
  #substituir infinitos
      
  excludeInds = which(adaWeights==Inf)
  adaWeights[excludeInds] = 10^100 #só pra garantir
  
  if (length(excludeInds) != 0) {
    x = x[,-excludeInds]
  }
  
  adaWeights = adaWeights[-excludeInds]
  
  #to run plain vanilla LASSO uncomment below
  #adaWeights = rep(1,nx) 
  #excludeInds = rep()
  
  lassoObj <- glmnet(x,y,family="gaussian",alpha=1,#exclude = excludeInds
                      nlambda=100,penalty.factor=adaWeights,intercept=TRUE)
  
  
  if (is.na(pen) == 1) {
    pen = log(nrow(x))  #defaults to BIC
  }
  
  #Cross-Validating
  print(pen)
  selCV <- lassoCV(lassoObj,x,y,pen)
  print(selCV)
  nVarsSel <- length(selCV)
  schCoeffs = lassoObj$beta
  
  selCVRestricted = selCV
  if (varUB != Inf) {
    
    if (nVarsSel > varUB) {
      
      selModRestricted <- max(which(lassoObj$df <= varUB))
      selCVRestricted = which(schCoeffs[,selModRestricted]!=0)
      
    } else {
      
      selCVRestricted = selCV
      
    }
    
  } 
  
  returnList = list(adaLassoObj = lassoObj, selectedIndcs = selCVRestricted, selectedUnres = selCV)
  invisible(returnList)
  
}

#Calls BAS

library("BAS")
library("ggplot2")
library("reshape")

setwd("C:/Users/Marcelo/Desktop/Itaú Asset/R/BAS")
source("basOOS.R")

T = 190
K = 10 #number of variables

signal = 10
#Regressors Variable
x = matrix(rnorm(T*K,0,signal),T,K)

#Outcome
noise = 3
beta = 0.5
y = beta*x[,1] + rnorm(T,0,noise)

basList <- basOOS(y,x)

#Bayesian Adaptive Sampling Example

library("BAS")
library("ggplot2")
library("reshape")
library("stats")

T = 150
K = 20000 #number of variables

signal = 10
#Regressors Variable
x = matrix(rnorm(T*K,0,signal),T,K)

#Outcome
noise = 3
beta = 0.5
y = beta*x[,1] + rnorm(T,0,noise)

#Out of Sample Restrictions
timePlot = 1:T
insT = floor(0.9*T)
xEst = x[1:insT,]
yEst = y[1:insT]

dfBAS = data.frame(yEst,xEst)

basObj <- bas.lm(yEst ~ . ,dfBAS,n.models=10^3,update=TRUE)

#Out of Sample Data
oosInd = seq(insT+1,T,1)
oosX = x[oosInd,]
oosY = y[oosInd]

fittedModel = predict.bas(basObj,oosX,top=10)
yBest = fittedModel$Ypred[1,]
yBMA = as.numeric(fittedModel$Ybma)

dfPlot<- data.frame(oosInd,oosY,yBMA,yBest)
dfMelted <- melt(dfPlot, id = "oosInd")

plotOOS <- ggplot(data = dfMelted, aes(x = oosInd, y = value, color = variable)) +
  geom_line()
plotOOS

#data for point estimate histogram
fittedModel = predict.bas(basObj,oosX)
yPred <- fittedModel$Ypred
#horizon option
horizon = 1
yPred <- yPred[,1]
yProb <- basObj$postprob
spline <- spline(yPred,yProb)
yProb = spline$y
yPred = spline$x
xStd = sd(yPred)
dropInds = which(abs(yPred)>0.5*xStd)
yProb = yProb[-dropInds]
yPred = yPred[-dropInds]
dfHist <- data.frame(yPred,yProb)
histPlot <- ggplot(data=dfHist,aes(x=yPred,y=yProb)) +
  geom_line()
histPlot <- ggplot(data=dfHist) +
  geom_histogram(aes(x=yPred,weight=yProb))
#Tentar com plot normal

basOOS <- function(y, x){
  
  library("BAS")
  library("ggplot2")
  library("reshape")
  
  T =nrow(x)
  
  #Out of Sample Restrictions
  timePlot = 1:T
  insT = floor(0.9*T)
  xEst = x[1:insT,]
  yEst = y[1:insT]
  
  dfBAS = data.frame(yEst,xEst)
  
  basObj <- bas.lm(yEst ~ . ,dfBAS)
  
  #Out of Sample Data
  oosInd = seq(insT+1,T,1)
  oosX = x[oosInd,]
  oosY = y[oosInd]
  
  fittedModel = predict.bas(basObj,oosX,top=3)
  yBest = fittedModel$Ypred[1,]
  yBMA = as.numeric(fittedModel$Ybma)
  
  dfPlot<- data.frame(oosInd,oosY,yBMA,yBest)
  dfMelted <- melt(dfPlot, id = "oosInd")
  
  oosPlot <- ggplot(data = dfMelted, aes(x = oosInd, y = value, color = variable)) +
    geom_line()
  
  #data for point estimate histogram
  fittedModel = predict.bas(basObj,oosX)
  yPred <- fitteModel$Ypred
  
  resultList <- list(oosPlot = oosPlot)
  
  return(resultList)
  
}

'Creates design matrix for candidates variables from Haver'

start.time <- Sys.time()
design = designPrediction('n273trs')
end.time <- Sys.time()
time.taken <- end.time - start.time

  rm(list = ls())

source('Z:/ASSET/ECO/LATAM/MX/Codes/R/sourceFuns.R', echo=TRUE)

library(Haver)
library(zoo)
library(gtools)
library(base)

updateDB = 1
depDB = 'emergela'
countryCode = '273' #country code for Haver; e.g. Mexico = 273
depStrVar = 'n273trs'
delFreq = 12 #e.g. 12 = YoY, 1 = MoM for monthly data
delLevel = 0 #differences data in levels
saDummy = 0 #dummy se for pra pegar dados SA
nLags = 0 #number of lags from candidate variables

dbList = c()
if (updateDB == 1) {

  'Main Database'
  dataAvailable <- haver.metadata(dat=depDB)
  dbList = c(dbList,dataAvailable$database)
  
  # 'US Database'
  # dataUS <- haver.metadata(dat='usecon')
  # dbList = c(dbList,dataUS$database)
  
  'All Database'
  dataAvailable <- mapply(c,dataAvailable,dataUS,SIMPLIFY=FALSE)
  
  save.image("Z:/ASSET/ECO/LATAM/MX/Codes/R/baseSelection.RData")
  
} else {
  
  load("Z:/ASSET/ECO/LATAM/MX/Codes/R/baseSelection.RData")
  
}

'Dependent Variable'
response <- as.zoo(haver.data(paste(depDB,':',depStrVar,sep=""),rtype='zoo'))

lastDateResponse <- index(response)[length(index(response))]

'Backtesting'
#response = response[-which(index(response)==lastDateResponse),]

lastDatePredictive <- index(response)[length(index(response))]
require(lubridate)
#lastDatePredictive <-  "2016-11-30"
#lastDatePredictive <- lastDatePredictive %m+% months(c(1)) #Series with 1 month ahead data

responsePC = delPC(response,delFreq,dLevel = delLevel)
# responsePC = response
# responsePC = delPC(response,delFreq,dLevel=1)

depInfo = dlxGetInfo(depStrVar,depDB)

'Determining frequency from Haver properties'
if (depInfo$frequency == 40) {
  freqStr = "M"
} else if (depInfo$frequency == 30) {
  freqStr = "Q"
}

ltRelTime = depInfo$dateTimeMod

'Filtering Series'
databaseList <- dataAvailable$database
codesList <- dataAvailable$code
freqList <- dataAvailable$frequency
lastDateList <- dataAvailable$enddate

#Filtering for NSA or SA
if (saDummy == 0) {
  seasAdjList = c("^N","^C")
} else {
  seasAdjList = c("^S","^G","^H")
}
countryCodeList = c(countryCode)
# countryCodeList = c(countryCode,"111")

#Appending codes
cSubDB = c()
for (s in seasAdjList) {
  for (p in countryCodeList) {
    cSubDB = c(cSubDB,paste(s,p,sep=""))
  }
}

matchNames = c()
for (b in cSubDB) {
  matchNamesCurr = grep(b, codesList,ignore.case=TRUE)
  matchNames = union(matchNames,matchNamesCurr)
}

matchNamesDep <- grep(depStrVar, codesList,ignore.case=TRUE)
matchFreq <- grep(freqStr, freqList,ignore.case=TRUE)

#Filtering dates: information set
matchLastDate1 <- grep(lastDatePredictive, lastDateList)
matchLastDate2 <- which(lastDateList>lastDateResponse)
#matchLastDate2 <- grep(lastDateResponse, lastDateList)
matchLastDate <- union(matchLastDate1,matchLastDate2)

'Selected Series List'
#matchedCodes <- intersect(matchFreq,matchNames)
matchedCodesLagged <- Reduce(intersect,list(matchFreq,matchNames,matchLastDate1))
matchedCodesContemp <- Reduce(intersect,list(matchFreq,matchNames,matchLastDate2))

namesWildCardLag <- codesList[matchedCodesLagged]
namesWildCardCtp <- codesList[matchedCodesContemp]
databaseListCtp <- databaseList[matchedCodesContemp]
databaseListLag <- databaseList[matchedCodesLagged]

if (length(namesWildCardCtp) != 0) {
  for (j in 1:length(namesWildCardCtp)) {
    currPred <- as.zoo(haver.data(namesWildCardCtp[j],databaseListCtp[j],rtype='zoo'))
    if (j > 1) {
      predictorsCtp <- merge(predictorsCtp,currPred)
    } else {
      predictorsCtp <- currPred
    }
  }
}

for (j in 1:length(namesWildCardLag)) {
  currPred <- as.zoo(haver.data(namesWildCardLag[j],databaseListLag[j],rtype='zoo'))
  if (j > 1) {
    predictorsLag <- merge(predictorsLag,currPred)
  } else {
    predictorsLag <- currPred
  }
}

# if (length(namesWildCardCtp) != 0) {
#     predictorsCtp <- as.zoo(haver.data(namesWildCardCtp,depDB,rtype='zoo'))
# }
# predictorsLag <- as.zoo(haver.data(namesWildCardLag,depDB,rtype='zoo'))

if (length(namesWildCardCtp) != 0) {
  predictorsCatchIndex = merge(predictorsLag,predictorsCtp)
  colnames(predictorsCtp) = paste(colnames(predictorsCtp), ".ctp", sep = "")
} else {
  predictorsCatchIndex = predictorsLag
  #adding last Date
  lastDateMonth = as.Date(paste(year(lastDatePredictive),"-",
                           month(lastDatePredictive)+2,"-","01",sep=""))
  foreDate = seq(lastDateMonth,length=1,by="months")-1
  indexPredictive = c(index(predictorsCatchIndex),foreDate)
  naSer = as.zoo(NA,order.by=indexPredictive)
  predictorsCatchIndex = as.zoo(merge(naSer,predictorsLag))
  # predictorsCatchIndex = predictorsCatchIndex[,seq(2,NCOL(predictorsCatchIndex))]
}

predictorsLag <- lag(predictorsCatchIndex[,2:NCOL(predictorsCatchIndex)],-1)
colnames(predictorsLag) = paste(colnames(predictorsLag), ".lag", sep = "")

if (length(namesWildCardCtp) != 0) {
  predictors = merge(predictorsCtp,predictorsLag)
} else {
  predictors = predictorsLag
}

#Pre-Filtering Data
removedSer = c()
predictorsBackup = predictors
#Filter 1 - remove irregular data at the margin
for (i in 1:NCOL(predictors)) {
  currSer = predictors[,i]
  lastNonNA = max(which(is.na(currSer)==0))
  currSerMargin = currSer[1:lastNonNA,]
  naMargin = sum(is.na(currSerMargin[(round(0.9*length(currSerMargin))):length(currSerMargin)]))
  if (naMargin > 3) {
    removedSer = c(removedSer,i)
  }
  print(i)
  
}

predictors = predictors[,-removedSer]
predictors = delPC_ur(predictors,delFreq)
# predictors <- delPC(predictors,delFreq)
zrVarInd = c()
for (i in 1:NCOL(predictors)) {
  currSer = squareData(predictors[,i])
  currVrnc = var(currSer)
  if (currVrnc == 0) {
    zrVarInd = c(zrVarInd,i) 
  }
}
predictors <- predictors[,-zrVarInd]
predictorsFiltered <- dfFilter(predictors)
predNamesCrude <- colnames(predictors)


# GERA ITERAÇÔES
# 'Precisa resolver colnames e index'
# predictorsTeste = predictors[,c(1,2,3,4)]
# predictorsTeste = t(apply(predictorsTeste, 1, combn, 2, prod))
# colnames(predictorsTeste) <- paste("Inter.V", combn(1:4, 2, paste, collapse="V"), sep="")
# 'Generating all interactions'
# iterations = t(apply(predictors, 1, combn, 2, prod))
# colnames(predictorsTeste) <- paste("Inter.V", combn(1:4, 2, paste, collapse="V"), sep="")

'Adding Lags'
predictorsLags = predictors
if (nLags>0) {
  for (l in 1:nLags) {
    lagsDF = lag(predictors,-l)
    colnames(lagsDF) = paste(colnames(lagsDF),"_l",l,sep="")
    predictorsLags = merge(predictorsLags,lagsDF)
  }
}
namePreds = toupper(colnames(predictorsLags))
depNameCTP = paste(toupper(depStrVar),'.CTP',sep="")
depReplicate = which(namePreds==depNameCTP)
if (length(depReplicate) != 0) {
  predictorsLags = predictorsLags[,-depReplicate]
}

#Removing dependent variable and breakdowns from candidates
namesWildCardLag <- codesList[matchedCodesLagged]

removeMatch = c()
i = 1
for (var in namesWildCardLag) {
  
  #Removing any predictor whose release time is the same as dependent variable
  #(in particular, a breakdown of the dependent variable).
  depInfoPred = dlxGetInfo(var,databaseListLag[i])
  i = i + 1
  ltRelTimePred = depInfoPred$dateTimeMod
  if (ltRelTimePred == ltRelTime) {
    removeMatch = c(removeMatch,paste(var,".lag",sep=""))
  }
  
}

removeInds = match(removeMatch,names(predictorsLags))
removeInds = removeInds[which(is.na(removeInds)==FALSE)]
predictorsLags = predictorsLags[,-removeInds]

'Adding Transformations'
#squared = predictorsLags^2
#colnames(squared) = paste(colnames(predictorsLags),"_t",2,sep="")
#cubic = predictorsLags^3
#colnames(cubic) = paste(colnames(predictorsLags),"_t",3,sep="")
#predictorsLags = merge(predictorsLags,squared)
#predictorsLags = merge(predictorsLags,cubic)

'Restricting Sample'
predictorsLags = predictorsLags[index(predictorsLags)>"2010-01-01"]

# save.image(paste("Z:/ASSET/ECO/LATAM/MX/Codes/R/baseSelection_",depStrVar,".RData",sep=""))

'LASSO Selection'
selDF <- as.data.frame(merge(responsePC,predictorsLags))
adaObj <- adaLasso(selDF)
y = selDF[,1]
x = selDF[,-1]
selCV <- lassoCV(adaObj,x,y,forcedNoVar=5)

# "n273fchd.lag" "n273pje5.lag" "n273pjeh.lag" "n273pjhn.lag" "n273pjm8.lag" "n273pjwx.lag"

# "c273ara.lag"  "n273pj2h.lag" "n273pje5.lag" "n273pjhn.lag" "n273pjwx.lag" "n273pjxq.lag"

#GFI selection threshold: a estudar: 4.709375

'Single-Equation Prediction'
selPreds = predictorsLags[,selCV]
dfPred = as.data.frame(merge(responsePC,selPreds))
#predEq = lm(data=dfPred)
#fitted = predict(predEq,dfPred[,-1])

selPreds = predictorsLags[,selCV]

jobCreationList = c("n273fchd.lag","n273pje5.lag","n273pjeh.lag","n273pjhn.lag")

imefnsaList = c("h273eean.lag","h273eedn.lag")

imefsaList = c("h273eddn.lag","h273eedn.lag","h273vm.lag")

remittList = c("c273xlyb.lag","n273eedn.ctp","n273edmz.ctp")

ipList = c("c273tmkd.ctp","n273bwt.ctp","n273eeat.ctp","n273eedp.ctp","n273epew.ctp",
           "n273pjaf.ctp","n273pjtk.ctp","n273e0m.lag","n273vlc2.lag")
ipPreds = predictorsLags[,ipList]


manufList = c("n273e0m.lag","n273vlc.lag","c273tmkd.ctp","n273bwt.ctp","n273e1at.ctp","n273ed.ctp",
              "n273eedp.ctp","n273fmcc.ctp") 
manufPreds = predictorsLags[,manufList]

retailList = c("n273bwt.ctp","n273eeat.ctp","n273esws.ctp","n273eswt.ctp","n273pjon.ctp",
               "n273vlc3.lag")
retailPreds = predictorsLags[,retailList]


ipList = c("c273tmkd.ctp","n273bwt.ctp","n273eeat.ctp","n273eedp.ctp","n273epew.ctp",
           "n273pjaf.ctp","n273pjtk.ctp","n273e0m.lag","n273vlc2.lag")

manufList = c("n273e0m.lag","n273vlc.lag","c273tmkd.ctp","n273bwt.ctp","n273e1at.ctp","n273ed.ctp",
              "n273eedp.ctp","n273fmcc.ctp") 


gfiList = c("c273esw.ctp","n273dr.ctp","n273e1sn.ctp","n273ehem.ctp","n273gvi.ctp",
            "n273pcm.ctp","n273pje.ctp","n273pjhn.ctp","n273pjm8.ctp","n273vlc.lag")

unempSAList = c("h273pcb.ctp","h273pce.ctp","h273pcm.ctp",
                "h273pj2h.ctp","h273pjer.ctp","h273epae.lag","s273dn.lag")

unempNSAList = c("n273esws.ctp","n273pce.ctp","n273pj7w.ctp","n273pjec.ctp",
                 "n273pjhn.ctp","n273eda.lag", "n273edmz.lag")

selCharVec = retailList

forePreds = predictorsLags[,selCharVec]

# #Attaching Principal Components
# nFac = 5
# pCompObj = prcomp(squareData(selPreds),scale=TRUE)
# pComp = as.zoo(pCompObj$x,order.by=index(selPreds))
# pComp = pComp[,seq(1,nFac)]
# selPreds = merge(selPreds,pComp)

#dias uteis
mx_du <- importEviews("Z:/ASSET/ECO/LATAM/MX/Codes/R/mx_du_eviews.csv")
#transformando indice de tempo no formato do Haver i.e. ultimo dia do Haver. o correto é ver se o zoo package tem algum padrão para dados de diferentes frequencias
index(mx_du) <- as.Date(as.yearmon(index(mx_du), "%b%Y"),frac = 1)

# dfPred = merge(responsePC,selPreds)
dfPred = merge(responsePC,forePreds,mx_du)
y = dfPred[,1]
x = dfPred[,-1]
totModels = 2^length(selCV)-1
depSAR <- lag(y,-12)
depAROne <- lag(y,-1)
depARTwo <- lag(y,-2)
x = merge(depSAR,depAROne,depARTwo,x)
bestSbst = bestSubsetFn(y,x,window = 36, subsetMin = 2, subsetMax = 5)
# GA = geneticSelection(y,x)

#Auto-Correct Prediction: mean of recent errors

recErrors = bestSbst$modelsAvg - responsePC
addFac = mean(recErrors[(NROW(recErrors)-4):NROW(recErrors)])
adjFore = bestSbst$modelsAvg – addFac

bestSubsetFn <- function(y,x,nModels = NA, window = NA, subsetMin = 1,subsetMax = NA,
                         recordModels = FALSE, horizon = 1, dRolling = 1, forcedVarsInds = c()) {

'Best Subset Regression Function'

'to do: generalizar para constante e modelo sem variavel'
  
#source('Z:/ASSET/ECO/LATAM/MX/Codes/R/sourceFuns.R', echo=TRUE)

library(Haver)
library(zoo)
library(gtools)
library(base)
library(ggplot2)
library(matrixStats)

'To do: o que fazer quando nao tem variavel'
if (NCOL(x) == 0) {
  returnList = list(modelsAvg = NA, oosPlot = NA)
  return(returnList)
  #x = as.data.frame(matrix(1,NROW(y),1))
  #row.names(x) <- index(y)
}

if (typeof(forcedVarsInds) == "character") {
  forcedVarsInds = match(forcedVarsInds,names(x))
}

responsePC <- as.zoo(y)
if (length(forcedVarsInds) > 0) {
  predictors <- x[,-forcedVarsInds]
  forcedVars <- x[,forcedVarsInds]
} else {
  predictors = x
}
namesForcedVars <- names(x)[forcedVarsInds]

'Standard Deviation of Response: Calculating on the final 30% of available sample'
responseCompletePC <- squareData(responsePC)
responseDescrSample <- (round(0.5*NROW(responseCompletePC)):NROW(responseCompletePC))
responseSD <- mean(responseCompletePC[responseDescrSample])

indsPred = seq(1,ncol(predictors)) #predictors indices

if (is.na(subsetMax) == TRUE) {
  subsetMax = ncol(predictors)
}

'List where each element is a vector with all models enumeration with j variables'
allModels = list()
totModels = 0
if (subsetMin >= NCOL(predictors)) {
  subsetMin = NCOL(predictors)
  subsetMax = NCOL(predictors)
}
if (subsetMax > NCOL(predictors)) {
  subsetMax = NCOL(predictors)
}

for (j in subsetMin:subsetMax) {
  currEnum = list(combinations(ncol(predictors),j))[[1]]
  totModels = totModels + NROW(currEnum)
  allModels <-c(allModels,list(combinations(ncol(predictors),j)))
}

'Counting total number of Models'
#totModels = 2^ncol(predictors)

#Regressions with all models
modCounter = 1
timeIndex = index(predictors)
oosModel = zoo(matrix(NA,length(timeIndex),totModels),order.by=timeIndex)
lossModel = matrix(NA,totModels,2)
dfList = list()
modelNames = list()
for (k in 1:length(allModels)) {
  kModels = allModels[[k]]
  for (j in 1:nrow(kModels)) {
    
    currModel = kModels[j,]
    currNames = c(names(predictors)[currModel],namesForcedVars)
    modelNames = c(modelNames,list(currNames))
    dfRegression = merge(responsePC,predictors[,currModel])
    if (length(forcedVarsInds > 0)) {
      dfRegression = merge(responsePC,predictors[,currModel],forcedVars)
    }
    oosList = oosLin(dfRegression,window = window, horizon = horizon, dRolling = dRolling)
    currOOS = oosList$oosTS
    currLossList = oosList$lossFns
    currSqLoss = currLossList$squared
    currAbsLoss = currLossList$abs
    oosModel[index(currOOS),modCounter] = currOOS
    lossModel[modCounter,1] = currSqLoss
    lossModel[modCounter,2] = currAbsLoss
    modCounter = modCounter + 1
    
    #testing lapply
    #dfList = c(dfList,list(dfRegression))
    
  }
}

'lapply form'
# timeIndex = index(predictors)
# oosModel = zoo(matrix(NA,length(timeIndex),totModels),order.by=timeIndex)
# lossModel = matrix(NA,totModels,1)
# modelsRes <- lapply(dfList,oosLin)
# for (n in 1:length(modelsRes)) {
#   
#   currModel = modelsRes[[n]]
#   oosModel[index(currOOS),modCounter] = currModel$oosTS
#   lossModel[n] = currModel$lossFns$squared
#   
# }

rankModels = match(sort(lossModel[,1],decreasing = FALSE),lossModel[,1]) #mth best is the nth model
rankModelsInv = match(lossModel[,1],sort(lossModel[,1],decreasing = FALSE)) #nth model is the mth best

modelsPerformance = NA
if (recordModels == TRUE) {
  
  modelsPerformance = as.data.frame(matrix(NA,NROW(lossModel),100))
  colnames(modelsPerformance) <- c("Normalized Squared Loss","Normalized Absolute Loss", "Rank")
  for (j in 1:(NROW(lossModel))) {
    
    modelsPerformance[j,1] = lossModel[j,1]/responseSD
    modelsPerformance[j,2] = lossModel[j,2]/responseSD
    modelsPerformance[j,3] = rankModelsInv[j]
    currModel = modelNames[[j]]
    for (i in 1:length(currModel)) {
      modelsPerformance[j,3+i] = currModel[i]
    }
    
  }
  
  modelsPerformance = modelsPerformance[order(modelsPerformance$Rank),]

}

if (is.na(nModels) == TRUE) {
  nModels = round(0.1*length(rankModels))
}
if (nModels == 0) {
  nModels = 1
}
selModels = rankModels[seq(1,nModels)]

# 'Mapping loss functions into model weights with generalized function'
# 'solver nao linear dando problema
# source('Z:/ASSET/ECO/LATAM/MX/Codes/R/oosWeights.R')
# source('Z:/ASSET/ECO/LATAM/MX/Codes/R/oosWeightsJacobian.R')
# w0 = rep(0.5,nModels)
# rmseVec = lossModel[selModels]
# weightsSolver = nleqslv(w0, oosWeights, oosWeightsJacobian, rmseVec)
# modelWeights2 = weightsSolver$x
#to do: better solution; the solver should converge
#modelWeights[which(modelWeights<0)] = 0
# if (sum(modelWeights) > 1) {
#   modelWeights = modelWeights/sum(modelWeights)
# }

rmseVec = lossModel[selModels]
modelWeights = sort(rmseVec/sum(rmseVec),decreasing=TRUE)

#to do: rankear combinações de modelo tambem

if (NCOL(oosModel[,selModels]) > 1) {
  modelsAvg = zoo(rowWeightedMeans(oosModel[,selModels], w=modelWeights),order.by=timeIndex)
  #modelsAvg = zoo(rowMeans(oosModel[,selModels]),order.by=timeIndex)
} else {
  modelsAvg = zoo(oosModel[,selModels],order.by=timeIndex)
}

modelsAvgCF = sd(modelsAvg[which(is.na(modelsAvg)==FALSE)])/responseSD

zooComp = zoo(responsePC,order.by=index(responsePC))
zooComp = merge(responsePC,modelsAvg)
zooComp = zooComp[rowSums(is.na(zooComp))<2,] #removing rows with 2 NAs
plotSmpl = seq(nrow(zooComp)-24,nrow(zooComp))
plotComp = zooComp[plotSmpl,]

plotZoo <- autoplot(plotComp,facet=NULL)

lossSel = lossModel[selModels]
oosSel = oosModel[,selModels]

returnList = list(modelsAvg = modelsAvg, modelWeights = modelWeights,
                  oosPlot = plotZoo, loss = lossSel,oosModels = oosSel,selModels = selModels,
                  modelsPerformance = modelsPerformance, modelsAvgCF = modelsAvgCF)

return(returnList)

}

calcBIC <- function(regression) {
  
  coeffs = regression$coefficients
  ncoeffs = length(coeffs)
  residuals <- regression$residuals
  sample = length(residuals)
  ssr = sum(residuals^2)
  df = regression$df.residual
  sigma = sqrt((1/df)*sum(ssr))
  llik = sum(log(dnorm(residuals,0,sigma)))
  bic = -2*llik +  ncoeffs*(log(sample) + log(2*pi))
  return(bic)
  'Obs: Not exact to the R BIC function.'
  
}

delPC <- function(zooObj,lagDel=1,dLevel = 0,dPerc = 0) {
  
  scale = 1
  if (dPerc == 1) {
    scale = 100
  }
  
  'Takes the lagDel difference of zooObj. For non-negative variables it takes %change.'
  
  'To do: generalizar para uma dimensao.'
  'Checking which columns are always positive'
  plusInds = which(colSums(zooObj<=0,na.rm=TRUE)==0)
  totInds = NCOL(zooObj)
  negzerInds = setdiff(seq(1,totInds),plusInds)
  
  'Calculates Delta of zooObj'
  if (length(negzerInds) > 0) {
    zooObjDel = zooObj[,negzerInds]
    zooObjDel = zooObjDel - lag(zooObjDel,k=-lagDel)
  }
  
  'Calculates %Change of zooObj'
  zooObjPC = zooObj[,plusInds]
  if (dLevel == 0) {
    zooObjPC <- ((zooObjPC - lag(zooObjPC,k=-lagDel))/lag(zooObjPC,k=-lagDel))*scale
  } else {
    zooObjPC <- zooObjPC - lag(zooObjPC,k=-lagDel)
  }
  
  zooObjDiff = as.data.frame(matrix(0,NROW(zooObjPC),NCOL(zooObj)))
  zooObjDiff[,plusInds] = zooObjPC
  if (length(negzerInds) > 0) {
    zooObjDiff[,negzerInds] = zooObjDel
  }
  
  varNames = names(zooObj)
  colnames(zooObjDiff) <- varNames
  zooObjDiff = zoo(zooObjDiff,order.by=index(zooObjPC))
  
  return(zooObjDiff)
  
  
}

delPC_ur <- function(zooObj,lagDel=1,dLevel = 0) {
  
  source("Z:/ASSET/ECO/LATAM/MX/Codes/R/urConsTest.R")
  
  indSeries = index(zooObj) 
  varNames = names(zooObj)
  
  'Makes dataframe square (drops all non data rows)'
  # if (class(zooObj) == "data.frame") {
    zooObjMat = as.matrix(zooObj)
    zooObj = as.matrix(zooObj)
  # }
  
  #Apply
  urInds = apply(zooObj,2,urConsTest)  

  #Loop
  urInds = c()
  for (i in 1:NCOL(zooObj)) {
    urInds = c(urInds,urConsTest(zooObj[,i]))
    print(i)
  }
  
  #Identifying series to difference
  indDiff = which(urInds==1)
  indNoDiff = which(urInds==0)
  
  noDiffObj = as.zoo(zooObj[,indNoDiff])
  diffObj = as.zoo(zooObj[,indDiff])
  
  'Takes the lagDel difference of zooObj. For non-negative variables it takes %change.'
  
  'To do: generalizar para uma dimensão.'
  'Checking which columns are always positive'
  if (NCOL(diffObj) > 1) {
    plusInds = which(colSums(diffObj<=0,na.rm=TRUE)==0)
  } else {
    plusInds = which(sum(diffObj<=0,na.rm=TRUE)==0)
  }
  totInds = NCOL(diffObj)
  negzerInds = setdiff(seq(1,totInds),plusInds)
  
  'Calculates Delta of zooObj'
  diffObjDel = diffObj[,negzerInds]
  diffObjDel = diffObjDel - lag(diffObjDel,k=-lagDel)
  
  'Calculates %Change of zooObj'
  diffObjPC = diffObj[,plusInds]
  if (dLevel == 0) {
    diffObjPC <- (diffObjPC - lag(diffObjPC,k=-lagDel))/lag(diffObjPC,k=-lagDel)
  } else {
    diffObjPC <- diffObjPC - lag(diffObjPC,k=-lagDel)
  }
  
  zooObjDiff = matrix(0,NROW(diffObjPC),NCOL(diffObj))
  zooObjDiff[,plusInds] = as.matrix(diffObjPC)
  zooObjDiff[,negzerInds] = as.matrix(diffObjDel)
  
  zooStationary = as.zoo(matrix(0,NROW(zooObj),NCOL(zooObj)))
  colnames(zooStationary) <- varNames
  index(zooStationary) <- indSeries
  
  zooStationary[,indNoDiff] <- zooObj[,indNoDiff]
  zooStationary[((lagDel+1):NROW(zooStationary)),indDiff] <- zooObjDiff
  
  return(zooStationary)
  
  
}

designPrediction <- function(depStrVar,depDB='emergela',countryCode='273',delFreq=12,
                             saDummy=0,nLags=0,delLevel=0,updateDB=1) {

# rm(list = ls())

source('Z:/ASSET/ECO/LATAM/MX/Codes/R/sourceFuns.R', echo=TRUE)

library(Haver)
library(zoo)
library(gtools)
library(base)

# updateDB = 1
# depDB = 'emergela'
# countryCode = '273' #country code for Haver; e.g. Mexico = 273
# depStrVar = 'n273trs'
# delFreq = 12 #e.g. 12 = YoY, 1 = MoM for monthly data
# delLevel = 0 #differences data in levels
# saDummy = 0 #dummy se for pra pegar dados SA
# nLags = 0 #number of lags from candidate variables

dbList = c()
if (updateDB == 1) {
  
  'Main Database'
  dataAvailable <- haver.metadata(dat=depDB)
  dbList = c(dbList,dataAvailable$database)
  
  # 'US Database'
  dataUS <- haver.metadata(dat='usecon')
  dbList = c(dbList,dataUS$database)
  
  'All Database'
  dataAvailable <- mapply(c,dataAvailable,dataUS,SIMPLIFY=FALSE)
  
  save.image("Z:/ASSET/ECO/LATAM/MX/Codes/R/baseSelection.RData")
  
} else {
  
  load("Z:/ASSET/ECO/LATAM/MX/Codes/R/baseSelection.RData")
  
}

'Dependent Variable'
response <- as.zoo(haver.data(paste(depDB,':',depStrVar,sep=""),rtype='zoo'))

lastDateResponse <- index(response)[length(index(response))]

'Backtesting'
#response = response[-which(index(response)==lastDateResponse),]

lastDatePredictive <- index(response)[length(index(response))]
require(lubridate)
#lastDatePredictive <-  "2016-11-30"
#lastDatePredictive <- lastDatePredictive %m+% months(c(1)) #Series with 1 month ahead data

responsePC = delPC(response,delFreq,dLevel = delLevel)
# responsePC = response
# responsePC = delPC(response,delFreq,dLevel=1)

depInfo = dlxGetInfo(depStrVar,depDB)

'Determining frequency from Haver properties'
if (depInfo$frequency == 40) {
  freqStr = "M"
} else if (depInfo$frequency == 30) {
  freqStr = "Q"
}

ltRelTime = depInfo$dateTimeMod

'Filtering Series'
databaseList <- dataAvailable$database
codesList <- dataAvailable$code
freqList <- dataAvailable$frequency
lastDateList <- dataAvailable$enddate

#Filtering for NSA or SA
if (saDummy == 0) {
  seasAdjList = c("^N","^C")
} else {
  seasAdjList = c("^S","^G","^H")
}
countryCodeList = c(countryCode)
# countryCodeList = c(countryCode,"111")

#Appending codes
cSubDB = c()
for (s in seasAdjList) {
  for (p in countryCodeList) {
    cSubDB = c(cSubDB,paste(s,p,sep=""))
  }
}

matchNames = c()
for (b in cSubDB) {
  matchNamesCurr = grep(b, codesList,ignore.case=TRUE)
  matchNames = union(matchNames,matchNamesCurr)
}

matchNamesDep <- grep(depStrVar, codesList,ignore.case=TRUE)
matchFreq <- grep(freqStr, freqList,ignore.case=TRUE)

#Filtering dates: information set
matchLastDate1 <- grep(lastDatePredictive, lastDateList)
matchLastDate2 <- which(lastDateList>lastDateResponse)
#matchLastDate2 <- grep(lastDateResponse, lastDateList)
matchLastDate <- union(matchLastDate1,matchLastDate2)

'Selected Series List'
#matchedCodes <- intersect(matchFreq,matchNames)
matchedCodesLagged <- Reduce(intersect,list(matchFreq,matchNames,matchLastDate1))
matchedCodesContemp <- Reduce(intersect,list(matchFreq,matchNames,matchLastDate2))

namesWildCardLag <- codesList[matchedCodesLagged]
namesWildCardCtp <- codesList[matchedCodesContemp]
databaseListCtp <- databaseList[matchedCodesContemp]
databaseListLag <- databaseList[matchedCodesLagged]

if (length(namesWildCardCtp) != 0) {
  for (j in 1:length(namesWildCardCtp)) {
    currPred <- as.zoo(haver.data(namesWildCardCtp[j],databaseListCtp[j],rtype='zoo'))
    if (j > 1) {
      predictorsCtp <- merge(predictorsCtp,currPred)
    } else {
      predictorsCtp <- currPred
    }
  }
}

for (j in 1:length(namesWildCardLag)) {
  currPred <- as.zoo(haver.data(namesWildCardLag[j],databaseListLag[j],rtype='zoo'))
  if (j > 1) {
    predictorsLag <- merge(predictorsLag,currPred)
  } else {
    predictorsLag <- currPred
  }
}

# if (length(namesWildCardCtp) != 0) {
#     predictorsCtp <- as.zoo(haver.data(namesWildCardCtp,depDB,rtype='zoo'))
# }
# predictorsLag <- as.zoo(haver.data(namesWildCardLag,depDB,rtype='zoo'))

if (length(namesWildCardCtp) != 0) {
  predictorsCatchIndex = merge(predictorsLag,predictorsCtp)
  colnames(predictorsCtp) = paste(colnames(predictorsCtp), ".ctp", sep = "")
} else {
  predictorsCatchIndex = predictorsLag
  #adding last Date
  lastDateMonth = as.Date(paste(year(lastDatePredictive),"-",
                                month(lastDatePredictive)+2,"-","01",sep=""))
  foreDate = seq(lastDateMonth,length=1,by="months")-1
  indexPredictive = c(index(predictorsCatchIndex),foreDate)
  naSer = as.zoo(NA,order.by=indexPredictive)
  predictorsCatchIndex = as.zoo(merge(naSer,predictorsLag))
  # predictorsCatchIndex = predictorsCatchIndex[,seq(2,NCOL(predictorsCatchIndex))]
}

predictorsLag <- lag(predictorsCatchIndex[,2:NCOL(predictorsCatchIndex)],-1)
colnames(predictorsLag) = paste(colnames(predictorsLag), ".lag", sep = "")

if (length(namesWildCardCtp) != 0) {
  predictors = merge(predictorsCtp,predictorsLag)
} else {
  predictors = predictorsLag
}

#Pre-Filtering Data
removedSer = c()
predictorsBackup = predictors
#Filter 1 - remove irregular data at the margin
for (i in 1:NCOL(predictors)) {
  currSer = predictors[,i]
  lastNonNA = max(which(is.na(currSer)==0))
  currSerMargin = currSer[1:lastNonNA,]
  naMargin = sum(is.na(currSerMargin[(round(0.9*length(currSerMargin))):length(currSerMargin)]))
  if (naMargin > 3) {
    removedSer = c(removedSer,i)
  }
  
}

predictors = predictors[,-removedSer]
# predictors = delPC_ur(predictors,delFreq)
predictors <- delPC(predictors,delFreq)
zrVarInd = c()
for (i in 1:NCOL(predictors)) {
  currSer = squareData(predictors[,i])
  currVrnc = var(currSer)
  if (currVrnc == 0) {
    zrVarInd = c(zrVarInd,i) 
  }
}

predictors <- predictors[,-zrVarInd]
predictorsFiltered <- dfFilter(predictors)
predNamesCrude <- colnames(predictors)

# GERA ITERAÇÔES
# 'Precisa resolver colnames e index'
# predictorsTeste = predictors[,c(1,2,3,4)]
# predictorsTeste = t(apply(predictorsTeste, 1, combn, 2, prod))
# colnames(predictorsTeste) <- paste("Inter.V", combn(1:4, 2, paste, collapse="V"), sep="")
# 'Generating all interactions'
# iterations = t(apply(predictors, 1, combn, 2, prod))
# colnames(predictorsTeste) <- paste("Inter.V", combn(1:4, 2, paste, collapse="V"), sep="")

'Adding Lags'
predictorsLags = predictors
if (nLags>0) {
  for (l in 1:nLags) {
    lagsDF = lag(predictors,-l)
    colnames(lagsDF) = paste(colnames(lagsDF),"_l",l,sep="")
    predictorsLags = merge(predictorsLags,lagsDF)
  }
}
namePreds = toupper(colnames(predictorsLags))
depNameCTP = paste(toupper(depStrVar),'.CTP',sep="")
depReplicate = which(namePreds==depNameCTP)
if (length(depReplicate) != 0) {
  predictorsLags = predictorsLags[,-depReplicate]
}

#Removing dependent variable and breakdowns from candidates
namesWildCardLag <- codesList[matchedCodesLagged]

removeMatch = c()
i = 1
for (var in namesWildCardLag) {
  
  #Removing any predictor whose release time is the same as dependent variable
  #(in particular, a breakdown of the dependent variable).
  depInfoPred = dlxGetInfo(var,databaseListLag[i])
  i = i + 1
  ltRelTimePred = depInfoPred$dateTimeMod
  if (ltRelTimePred == ltRelTime) {
    removeMatch = c(removeMatch,paste(var,".lag",sep=""))
  }
  
}

removeInds = match(removeMatch,names(predictorsLags))
removeInds = removeInds[which(is.na(removeInds)==FALSE)]
predictorsLags = predictorsLags[,-removeInds]

'Adding Transformations'
#squared = predictorsLags^2
#colnames(squared) = paste(colnames(predictorsLags),"_t",2,sep="")
#cubic = predictorsLags^3
#colnames(cubic) = paste(colnames(predictorsLags),"_t",3,sep="")
#predictorsLags = merge(predictorsLags,squared)
#predictorsLags = merge(predictorsLags,cubic)

'Restricting Sample'
predictorsLags = predictorsLags[index(predictorsLags)>"2010-01-01"]

return(list(y=responsePC,x=predictorsLags))

}

dfFilter <- function(df) {
  
  library(fields)
  
  'To do: vectorize. Easy: df[,numNA<40]'
  
  'Restricts data.frame to the last latestObs observations'
  latestObs = 150
  lastObs = NROW(df)
  firstObs = NROW(df)-latestObs
  dfReturn = df[seq(firstObs+1,lastObs),]
  
  'Filters data-frame to have at least obsThresh observations'
  obsThresh = 60
  delList = c()
  for (i in 1:NCOL(dfReturn)) {
    
    currCol = dfReturn[,i]
    naNum = sum(is.na(currCol))
    obsNum = NROW(dfReturn) - naNum
    if (obsNum <  obsThresh) {
      delList = c(delList,i)
    }
  }
  
  if (is.null(delList) == FALSE) {
    dfReturn = dfReturn[,-delList]
  }
  
  # 'Removes if last observation is NA/NaN'
  # for (i in 1:NCOL(dfReturn)) {
  # 
  #   if (is.na(dfReturn[NROW(dfReturn),i]) == TRUE) {
  #     delList = c(delList,i)
  #   }
  # 
  # }
  # 
  # if (is.null(delList) == FALSE) {
  #   dfReturn = dfReturn[,-delList]
  # }
  
  dates = index(df)
  dfReturn = as.data.frame(dfReturn)
  
  # 'Transforms Inf values into NA'
  is.na(dfReturn) <- do.call(cbind,lapply(dfReturn, is.infinite))
  
  #Removing Columns with more than 3 NAs
  dfReturn <- dfReturn[,!colSums(is.na(dfReturn)) > 25] #to do: ajustar esse filtro, precisa ser mais esperto
  
  dates = dates[seq(length(dates)-NROW(dfReturn)+1,length(dates))]
  
  #Filtrar variancia de série
  
  # rm var variable
  # varSeries = apply(dfReturn,2,var)
  
  zooReturn <- zoo(dfReturn,order.by=dates)
  
  invisible(zooReturn)
  
  
}

library(Haver)

#source('Z:/ASSET/ECO/LATAM/MX/Codes/R/bestSubsetFn.R', echo=TRUE)

ip = as.zoo(haver.data('USECON:ip',rtype='zoo'))

ar1 = lag(ip,k=-1)
ar2 = lag(ip,k=-2)
ar3 = lag(ip,k=-3)
car_prod = as.zoo(haver.data('USECON:PCRTM',rtype='zoo'))

y = ip
x = merge(ar1,ar2,ar3,car_prod)

  #Isso aqui é se vc quiser fazer TRUE out of sample; pelo que entendi não é isso que voce quer agora, mas deixei aqui...  #
  # y = y[seq(1,length(y)-i)] #tirando a amostra da variável dependente
  
  #voce pode escolher o horizonte de estimação no horizon; imagin que vá manter 1
  #a janela é opcional, mas o default é usar um tamanho 50% do amostra total rolling
  #essa é a janela que estima pra cada passo do out-of-sample
  #ele faz sempre o máximo possível de sub-amostras de tamanho window
  #dRolling é auto-explicativo; se voce colocar dRolling = 0 ele vai começar sempre da primeira observação possível, isto é, até onde tem dados para todos os previsores
  bestObj = bestSubsetFn(y,x,recordModels = TRUE, window = 10, horizon = 1,dRolling = 1,
                         subsetMin = 2,subsetMax = 2)
  
  models = bestObj$modelsPerformance
  library(xlsx)
  write.xlsx(models, "modelsPerformance.xlsx")

fitnessGASel <- function(selDummies) {
  
  incl <- which(selDummies==1)
  xSel <- cbind(x[,incl])
  mod <- lm.fit(xSel,y)
  class(mod) <- "lm"
  -AIC(mod)
  
}

#Genetic Algorithm Example

library("GA")
f <- function(x) abs(x) + cos(x)
min = -20
max = 20
curve(f,min,max)

fitness <- function(x) -f(x)

GA <- ga(type = "real-valued",fitness = fitness, min=min,max=max)

#Generic Forecasting

'Creates design matrix for candidates variables from Haver'

dd = designPrediction('n273trs')
predictorsLags = dd$x

# save.image(paste("Z:/ASSET/ECO/LATAM/MX/Codes/R/baseSelection_",depStrVar,".RData",sep=""))

'LASSO Selection'
selDF <- as.data.frame(merge(dd$y,dd$x))
adaObj <- adaLasso(selDF)
y = selDF[,1]
x = selDF[,-1]
selCV <- lassoCV(adaObj,x,y,forcedNoVar=5)

'Single-Equation Prediction'
selPreds = predictorsLags[,selCV]
dfPred = as.data.frame(merge(responsePC,selPreds))
#predEq = lm(data=dfPred)
#fitted = predict(predEq,dfPred[,-1])

selPreds = predictorsLags[,selCV]

jobCreationList = c("n273fchd.lag","n273pje5.lag","n273pjeh.lag","n273pjhn.lag")

imefnsaList = c("h273eean.lag","h273eedn.lag")

imefsaList = c("h273eddn.lag","h273eedn.lag","h273vm.lag")

remittList = c("c273xlyb.lag","n273eedn.ctp","n273edmz.ctp")

ipList = c("c273tmkd.ctp","n273bwt.ctp","n273eeat.ctp","n273eedp.ctp","n273epew.ctp",
           "n273pjaf.ctp","n273pjtk.ctp","n273e0m.lag","n273vlc2.lag")
ipPreds = predictorsLags[,ipList]


manufList = c("n273e0m.lag","n273vlc.lag","c273tmkd.ctp","n273bwt.ctp","n273e1at.ctp","n273ed.ctp",
              "n273eedp.ctp","n273fmcc.ctp") 
manufPreds = predictorsLags[,manufList]

retailList = c("n273bwt.ctp","n273eeat.ctp","n273esws.ctp","n273eswt.ctp","n273pjon.ctp",
               "n273vlc3.lag")
retailPreds = predictorsLags[,retailList]


ipList = c("c273tmkd.ctp","n273bwt.ctp","n273eeat.ctp","n273eedp.ctp","n273epew.ctp",
           "n273pjaf.ctp","n273pjtk.ctp","n273e0m.lag","n273vlc2.lag")

manufList = c("n273e0m.lag","n273vlc.lag","c273tmkd.ctp","n273bwt.ctp","n273e1at.ctp","n273ed.ctp",
              "n273eedp.ctp","n273fmcc.ctp") 


gfiList = c("c273esw.ctp","n273dr.ctp","n273e1sn.ctp","n273ehem.ctp","n273gvi.ctp",
            "n273pcm.ctp","n273pje.ctp","n273pjhn.ctp","n273pjm8.ctp","n273vlc.lag")

unempSAList = c("h273pcb.ctp","h273pce.ctp","h273pcm.ctp",
                "h273pj2h.ctp","h273pjer.ctp","h273epae.lag","s273dn.lag")

unempNSAList = c("n273esws.ctp","n273pce.ctp","n273pj7w.ctp","n273pjec.ctp",
                 "n273pjhn.ctp","n273eda.lag", "n273edmz.lag")

selCharVec = manufList

forePreds = predictorsLags[,selCharVec]

# #Attaching Principal Components
# nFac = 5
# pCompObj = prcomp(squareData(selPreds),scale=TRUE)
# pComp = as.zoo(pCompObj$x,order.by=index(selPreds))
# pComp = pComp[,seq(1,nFac)]
# selPreds = merge(selPreds,pComp)

#dias uteis
mx_du <- importEviews("Z:/ASSET/ECO/LATAM/MX/Codes/R/mx_du_eviews.csv")
#transformando indice de tempo no formato do Haver i.e. ultimo dia do Haver. o correto é ver se o zoo package tem algum padrão para dados de diferentes frequencias
index(mx_du) <- as.Date(as.yearmon(index(mx_du), "%b%Y"),frac = 1)

manuf_y = as.zoo(haver.data('emergela:n273dm',rtype='zoo'))
manuf_y = delPC(manuf_y,12)

# dfPred = merge(responsePC,selPreds)
dfPred = merge(manuf_y,forePreds,mx_du)
y = dfPred[,1]
x = dfPred[,-1]
totModels = 2^length(selCV)-1
depSAR <- lag(y,-12)
depAROne <- lag(y,-1)
depARTwo <- lag(y,-2)
x = merge(depSAR,depAROne,depARTwo,x)
bestSbst = bestSubsetFn(y,x,window = 36, subsetMin = 2, subsetMax = 5,recordModels = FALSE)
# GA = geneticSelection(y,x)

#Auto-Correct Prediction: mean of recent errors

recErrors = bestSbst$modelsAvg - responsePC
addFac = mean(recErrors[(NROW(recErrors)-4):NROW(recErrors)])
adjFore = bestSbst$modelsAvg – addFac

geneticSelection <- function(y,x,loss="aic") {
  
  library("GA")
  
  dfAll = merge(y,x)
  dfAll = as.data.frame(squareData(dfAll))
  y = dfAll[,1]
  x = dfAll[,-1]
  
  fitness <- function(selDummies) {
    
      incl <- which(selDummies==1)
      #xSel <- as.data.frame(cbind(1,x[,incl])) #aic
      xSel <- as.data.frame(x[,incl])
      dfReg = as.data.frame(cbind(y,xSel))
       
         #mod <- lm(y ~ .,dfReg)
         #class(mod) <- "lm"
         #return(-AIC(mod))
       # if (loss == "aic") {
       #   mod <- lm(y ~ .,dfReg)
       #   class(mod) <- "lm"
       #   return(-AIC(mod))
       # } else if (loss = "bic") {
       #   mod <- lm(y ~ .,dfReg)
       #   class(mod) <- "lm"
       #   return(-BIC(mod))
       # } else if (loss == "oos") { #ainda não funciona...
       #   oos <- oosLin(dfReg)
       #   return(oos$lossFns$squared)
       #  }
         
         oos <- oosLin(dfReg)
         return(oos$lossFns$squared)
    
  }
  
  GA <- ga("binary", fitness = fitness, nBits = ncol(x), names = colnames(x), monitor = plot)
  return(summary(GA))
  
}

gridLasso <- function(x,y,minVar=0,maxVar = Inf,npInds = c()) {
  
  'Adaptive LASSO that "grids" over the regularization parameter until degress of freedom df
  is minVar <= df <= maxvar'
  
  #npInds are independent variables indices which shall not be penalized. Defaults to empty.
  
  library("glmnet")
  
  selDF <- as.data.frame(cbind(y,x))
  lassoObj <- adaLasso(selDF,npInds = npInds)
  selDFSquare = squareData(selDF)
  y = as.matrix(selDFSquare[,1])
  x = as.matrix(selDFSquare[,-1])
  selCV <- lassoCV(lassoObj,x,y)
  nVarsSel = length(selCV)
  
  # #Faster but innacurate way: problem: may start an infinite loop
  # if (nVarsSel < minVar) {
  #   selCV <- lassoCV(lassoObj,x,y,forcedNoVar=minVar)
  # } 
  # 
  # if (maxVar != Inf) {
  # 
  #   if (nVarsSel > maxVar) {
  #     selCV <- lassoCV(lassoObj,x,y,forcedNoVar=maxVar)
  #   }
  #   
  # }
  
  pen = log(nrow(x))
  penTol = 10^(-1)
  shrinkRate = 0.9
  while ((nVarsSel < minVar | is.na(nVarsSel) == 1) & pen > penTol) {
     selDF <- as.data.frame(cbind(y,x))
     lassoObj <- adaLasso(selDF,npInds = npInds)
     selDFSquare = squareData(selDF)
     y = as.matrix(selDFSquare[,1])
     x = as.matrix(selDFSquare[,-1])
     selCV <- lassoCV(lassoObj,x,y,pen)
     nVarsSel = length(selCV)
     pen = shrinkRate*pen
  
  }

   selCVRestricted = selCV
   if (maxVar != Inf) {
  
     if (nVarsSel > maxVar) {
  
       schCoeffs = lassoObj$beta
       selModRestricted <- max(which(lassoObj$df <= maxVar))
       selCVRestricted = which(schCoeffs[,selModRestricted]!=0)
  
     } else {
  
       selCVRestricted = selCV
  
     }
  
   }
  xSlct = x
  
  returnList <- list(lassoObj = lassoObj, selectedIndcs = selCV)
  invisible(returnList)
  
}

#Growth Pricing

setwd("Z:/ASSET/ECO/Marcelo/Growth Pricing")

library(zoo)
library(xlsx)
library(vars)
library(VARsignR)

source('Z:/ASSET/ECO/LATAM/MX/Codes/R/sourceFuns.R', echo=TRUE)
data <- importEviews("Z:/ASSET/ECO/Marcelo/Growth Pricing/growth_pricing_data.csv")

data = data[index(data)>"2005-01-01"]

freq = 261
cDummy= TRUE

#Downloading data
yld <- data[,24]
msci <- data[,8]
#msci <- (msci - lag(msci,k=-freq))/lag(msci,k=-freq)
#msciInteg = integPC(msciPC,freq,msci)
#df = as.data.frame(merge(msci,msciInteg))
msci = log(msci)
#yld <- (yld - lag(yld,k=-freq))

simContr = 10
noLags = 1
startSample = "2016-01-01"

refDateDelta = "2016-11-08"

#######################################
#Identifying Policy Shock
dataZoo = merge(yld,msci)
dataSRSVAR = dataZoo
dataZoo = dataZoo[index(dataZoo)>startSample]
dataZoo = squareData(dataZoo)

dataSRSVAR = dataSRSVAR[index(dataSRSVAR)>startSample]
dataSRSVAR = squareData(dataSRSVAR)
dfSRSVAR = as.data.frame(dataSRSVAR)
dataSRSVAR = ts(dataSRSVAR)

constr <- c(+1,-2)

modelSRSVAR <- uhlig.reject(Y=dataSRSVAR, nlags=noLags, draws=simContr, subdraws=simContr,
                            nkeep=simContr*10, KMIN=1,KMAX=3, constrained=constr, constant=cDummy, steps=100)

fevd = modelSRSVAR$FEVDS
irfs = modelSRSVAR$IRFS

vl <- c("10 Yr Yield","MSCI")

  
irfplot(irfdraws=irfs, type="median", labels=vl, save=FALSE, bands=c(0.16, 0.84),
        grid=TRUE, bw=FALSE)


fevdplot(fevd, label=vl, save=FALSE, bands=c(0.16, 0.84), grid=TRUE,
         bw=FALSE, table=FALSE, periods=NULL)

fevd.table <- fevdplot(fevd, table=TRUE, label=vl, periods=c(1,10,20,30,40,50,60))

print(fevd.table)

depVarInd = 1
depVar= dataZoo[,depVarInd]
irfDraws = modelSRSVAR$IRFS[,,depVarInd]
irfMedian = apply(irfDraws,2,median)
nObs = NROW(dataZoo) - noLags

#Reduced Form
varOLS <- VAR(dataSRSVAR,p=noLags)
redRes = residuals(varOLS)
yldRes = redRes[,depVarInd]

#Identified Shock
idShockDraws = modelSRSVAR$SHOCKS
idShockMedian = apply(idShockDraws,2,median)
#stdShock = 1
stdShock = sd(idShockMedian)

coeffsDraws = modelSRSVAR$BDraws[,,depVarInd]
coeffsMedian = apply(coeffsDraws,2,median)

dateStart = refDateDelta
iniYld = depVar[which(index(depVar)==dateStart)]
#iniYld = window(yld,start = as.Date(dateStart), end = as.Date(dateStart))

dateEnd = index(dataZoo)[NROW(dataZoo)]
posDateStart = which(index(dataZoo)==dateStart)
posDateEnd = which(index(dataZoo)==dateEnd)
nShocksElem = posDateEnd - posDateStart
datesContrib = index(dataZoo[seq(posDateStart,posDateEnd)])
endYld = depVar[which(index(depVar)==dateEnd)]

contRedRes = yldRes[posDateStart]
totDepContrib = as.numeric(endYld[1]) - as.numeric(iniYld[1])
iniShock = idShockMedian[posDateStart-noLags]/stdShock
iniPolicyShockContrib = irfMedian[1]*iniShock
#iniResid - calcular do VAR

dataZooCreate = dataZoo
'Adding Lags to dataZoo'
for (l in 1:noLags) {
  currLag = lag(dataZooCreate,k=-l)
  colnames(currLag) <- c(paste("yld_l",l,sep=""),paste("msci_l",l,sep=""))
  dataZoo = merge(dataZoo,currLag)
}

shockContrib = rep(NA,nShocksElem)
policyLevContrib = rep(NA,nShocksElem)
deltaShockContrib = rep(NA,nShocksElem)
for (j in 1:nShocksElem) {
  
  policyLevContrib[j] = dataZoo[posDateStart+j-1,seq(3,(noLags+1)*2,by=2)]%*%coeffsMedian[seq(1,noLags*2,by=2)]
  currShock = idShockMedian[posDateStart+j-noLags]/stdShock
  shockContrib[j] = currShock%*%irfMedian[1]
  
  currVecShock = idShockMedian[(posDateStart+1-noLags):(posDateStart+j-noLags)]/stdShock
  currIrf = rev(irfMedian[1:j])
  deltaShockContrib[j] = currVecShock%*%currIrf
  
}

deltaPolicyContrib = deltaShockContrib
shockPolicyCumContrib = sum(shockContrib)
policyContrib = policyLevContrib + shockContrib
policyContrib = as.zoo(policyContrib,order.by = datesContrib)

#############################################
#Identifying Growth Shock
dataZoo = merge(msci,yld)
dataSRSVAR = dataZoo
dataZoo = dataZoo[index(dataZoo)>startSample]
dataZoo = squareData(dataZoo)

dataSRSVAR = dataSRSVAR[index(dataSRSVAR)>startSample]
dataSRSVAR = squareData(dataSRSVAR)
dfSRSVAR = as.data.frame(dataSRSVAR)
dataSRSVAR = ts(dataSRSVAR)

constr <- c(+1,+2)

modelSRSVAR <- uhlig.reject(Y=dataSRSVAR, nlags=noLags, draws=simContr, subdraws=simContr, nkeep=simContr*10, KMIN=1,
                       KMAX=3, constrained=constr, constant=cDummy, steps=100)


fevd = modelSRSVAR$FEVDS
irfs = modelSRSVAR$IRFS

vl <- c("MSCI","10 Yr Yield")


irfplot(irfdraws=irfs, type="median", labels=vl, save=FALSE, bands=c(0.16, 0.84),
        grid=TRUE, bw=FALSE)


fevdplot(fevd, label=vl, save=FALSE, bands=c(0.16, 0.84), grid=TRUE,
         bw=FALSE, table=FALSE, periods=NULL)

fevd.table <- fevdplot(fevd, table=TRUE, label=vl, periods=c(1,10,20,30,40,50,60))

print(fevd.table)

depVarInd = 2
depVar= dataZoo[,depVarInd]
irfDraws = modelSRSVAR$IRFS[,,depVarInd]
irfMedian = apply(irfDraws,2,median)

idShockDraws = modelSRSVAR$SHOCKS
idShockMedian = apply(idShockDraws,2,median)
#stdShock = 1
stdShock = sd(idShockMedian)

coeffsDraws = modelSRSVAR$BDraws[,,depVarInd]
coeffsMedian = apply(coeffsDraws,2,median)

dateStart = refDateDelta
iniYld = depVar[which(index(depVar)==dateStart)]
#iniYld = window(yld,start = as.Date(dateStart), end = as.Date(dateStart))

dateEnd = index(dataZoo)[NROW(dataZoo)]
posDateStart = which(index(dataZoo)==dateStart)
posDateEnd = which(index(dataZoo)==dateEnd)
nShocksElem = posDateEnd - posDateStart
datesContrib = index(dataZoo[seq(posDateStart,posDateEnd)])

iniShock = idShockMedian[posDateStart-noLags]/stdShock
iniGrowthShockContrib = irfMedian[1]*iniShock

dataZooCreate = dataZoo
'Adding Lags to dataZoo'
for (l in 1:noLags) {
  currLag = lag(dataZooCreate,k=-l)
  colnames(currLag) <- c(paste("msci_l",l,sep=""),paste("yld_l",l,sep=""))
  dataZoo = merge(dataZoo,currLag)
}

shockContrib = rep(NA,nShocksElem)
growthLevContrib = rep(NA,nShocksElem)
deltaShockContrib = rep(NA,nShocksElem)
deltaHead = rep(NA,nShocksElem)
deltaHeadSeq = rep(NA,nShocksElem)
for (j in 1:nShocksElem) {
  
  growthLevContrib[j] = dataZoo[posDateStart+j-1,seq(3,(noLags+1)*2,by=2)]%*%coeffsMedian[seq(1,noLags*2,by=2)]
  currShock = idShockMedian[posDateStart+j-noLags]/stdShock #shock lost due to lag creation
  shockContrib[j] = currShock%*%irfMedian[1]
  
  currVecShock = idShockMedian[(posDateStart+1-noLags):(posDateStart+j-noLags)]/stdShock
  currIrf = rev(irfMedian[1:j])
  deltaShockContrib[j] = currVecShock%*%currIrf
  
  deltaHead[j] = as.numeric(depVar[posDateStart+j]) - as.numeric(iniYld)
  deltaHeadSeq[j] = as.numeric(depVar[posDateStart+j]) - as.numeric(depVar[posDateStart+j-1])
  
}

deltaGrowthContrib = deltaShockContrib
shockGrowthCumContrib = sum(shockContrib)
growthContrib = growthLevContrib + shockContrib
growthContrib = as.zoo(growthContrib,order.by = datesContrib)

library(ggplot2)
library(reshape2)

contShock = iniPolicyShockContrib + iniGrowthShockContrib
explDepContrib = shockGrowthCumContrib + shockPolicyCumContrib

datesContribPlot = datesContrib[2:length(datesContrib)]
datesPlot = as.Date(datesContribPlot,'%m/%d/%Y')
value <- deltaGrowthContrib + deltaPolicyContrib
#value <- dataZoo[datesContrib,2]
variable = "Actual"
actualDF <- as.data.frame(value)
actualDF = cbind(actualDF,datesPlot,variable)
plotDF <- as.data.frame(cbind(deltaPolicyContrib,deltaGrowthContrib))
plotDF = cbind(plotDF,datesPlot)
meltedPlot <- melt(plotDF, id.vars = c('datesPlot'))

plot = ggplot(meltedPlot, aes(x=datesPlot,y=value)) +
        geom_area(aes(colour=variable, fill=variable)) +
        geom_line(data = actualDF,aes(y=value))
        #scale_y_continuous(limits = c(-0.5, 1))

plot


#######################################
#Identifying Growth Shock
dataZoo = merge(msci,yld)
dataSRSVAR = dataZoo
dataZoo = dataZoo[index(dataZoo)>"2015-01-01"]
dataZoo = squareData(dataZoo)

dataSRSVAR = dataSRSVAR[index(dataSRSVAR)>"2015-01-01"]
dataSRSVAR = squareData(dataSRSVAR)
dfSRSVAR = as.data.frame(dataSRSVAR)
dataSRSVAR = ts(dataSRSVAR)

constr <- c(+1,+2)
varOLS <- VAR(dataSRSVAR)
modelSRSVAR <- uhlig.reject(Y=dataSRSVAR, nlags=noLags, draws=simContr, subdraws=simContr,
                            nkeep=simContr*10, KMIN=1,KMAX=3, constrained=constr, constant=cDummy, steps=60)

fevd = modelSRSVAR$FEVDS
irfs = modelSRSVAR$IRFS

vl <- c("MSCI","10 Yr Yield")


irfplot(irfdraws=irfs, type="median", labels=vl, save=FALSE, bands=c(0.16, 0.84),
        grid=TRUE, bw=FALSE)


fevdplot(fevd, label=vl, save=FALSE, bands=c(0.16, 0.84), grid=TRUE,
         bw=FALSE, table=FALSE, periods=NULL)

fevd.table <- fevdplot(fevd, table=TRUE, label=vl, periods=c(1,10,20,30,40,50,60))

print(fevd.table)

depVarInd = 1
depVar= dataZoo[,depVarInd]
irfDraws = modelSRSVAR$IRFS[,,depVarInd]
irfMedian = apply(irfDraws,2,median)

idShockDraws = modelSRSVAR$SHOCKS
idShockMedian = apply(idShockDraws,2,median)

coeffsDraws = modelSRSVAR$BDraws[,,depVarInd]
coeffsMedian = apply(coeffsDraws,2,median)

dateStart = "2016-11-08"
iniYld = depVar[which(index(depVar)==dateStart)]
#iniYld = window(yld,start = as.Date(dateStart), end = as.Date(dateStart))

dateEnd = index(dataZoo)[NROW(dataZoo)]
posDateStart = which(index(dataZoo)==dateStart)
posDateEnd = which(index(dataZoo)==dateEnd)
nShocksElem = posDateEnd - posDateStart + 1
datesContrib = index(dataZoo[seq(posDateStart,posDateEnd)])

dataZooCreate = dataZoo
'Adding Lags to dataZoo'
for (l in 1:noLags) {
  currLag = lag(dataZooCreate,k=-l)
  colnames(currLag) <- c(paste("yld_l",l,sep=""),paste("msci_l",l,sep=""))
  dataZoo = merge(dataZoo,currLag)
}

#stdShock = 1
stdShock = sd(idShockMedian)
shockContrib = rep(NA,nShocksElem)
growthLevContrib = rep(NA,nShocksElem)
for (j in 1:nShocksElem) {
  
  growthLevContrib[j] = dataZoo[posDateStart+j-1,seq(3,(noLags+1)*2,by=2)]%*%coeffsMedian[seq(1,noLags*2,by=2)]
  currShock = idShockMedian[posDateStart+j-1-noLags]/stdShock
  shockContrib[j] = currShock%*%irfMedian[1]
  
}

shockCumContrib = cumsum(shockContrib)
growthContrib = growthLevContrib + shockContrib
growthContrib = as.zoo(growthContrib,order.by = datesContrib)

#############################################
#Identifying Policy Shock
dataZoo = merge(yld,msci)
dataSRSVAR = dataZoo
dataZoo = dataZoo[index(dataZoo)>"2010-01-01"]
dataZoo = squareData(dataZoo)

dataSRSVAR = dataSRSVAR[index(dataSRSVAR)>"2010-01-01"]
dataSRSVAR = squareData(dataSRSVAR)
dfSRSVAR = as.data.frame(dataSRSVAR)
dataSRSVAR = ts(dataSRSVAR)

constr <- c(+1,-2)

modelSRSVAR <- uhlig.reject(Y=dataSRSVAR, nlags=noLags, draws=simContr, subdraws=simContr, nkeep=simContr*10, KMIN=1,
                            KMAX=3, constrained=constr, constant=cDummy, steps=60)


fevd = modelSRSVAR$FEVDS
irfs = modelSRSVAR$IRFS

vl <- c("10 Yr Yield","MSCI")


irfplot(irfdraws=irfs, type="median", labels=vl, save=FALSE, bands=c(0.16, 0.84),
        grid=TRUE, bw=FALSE)


fevdplot(fevd, label=vl, save=FALSE, bands=c(0.16, 0.84), grid=TRUE,
         bw=FALSE, table=FALSE, periods=NULL)

fevd.table <- fevdplot(fevd, table=TRUE, label=vl, periods=c(1,10,20,30,40,50,60))

print(fevd.table)

depVarInd = 2
depVar= dataZoo[,depVarInd]
irfDraws = modelSRSVAR$IRFS[,,depVarInd]
irfMedian = apply(irfDraws,2,median)

idShockDraws = modelSRSVAR$SHOCKS
idShockMedian = apply(idShockDraws,2,median)

coeffsDraws = modelSRSVAR$BDraws[,,depVarInd]
coeffsMedian = apply(coeffsDraws,2,median)

dateStart = "2016-11-08"
iniYld = depVar[which(index(depVar)==dateStart)]
#iniYld = window(yld,start = as.Date(dateStart), end = as.Date(dateStart))

dateEnd = index(dataZoo)[NROW(dataZoo)]
posDateStart = which(index(dataZoo)==dateStart)
posDateEnd = which(index(dataZoo)==dateEnd)
nShocksElem = posDateEnd - posDateStart + 1
datesContrib = index(dataZoo[seq(posDateStart,posDateEnd)])

dataZooCreate = dataZoo
'Adding Lags to dataZoo'
for (l in 1:noLags) {
  currLag = lag(dataZooCreate,k=-l)
  colnames(currLag) <- c(paste("msci_l",l,sep=""),paste("yld_l",l,sep=""))
  dataZoo = merge(dataZoo,currLag)
}

#stdShock = 1
stdShock = sd(idShockMedian)
shockContrib = rep(NA,nShocksElem)
polictLevContrib = rep(NA,nShocksElem)
for (j in 1:nShocksElem) {
  
  policyLevContrib[j] = dataZoo[posDateStart+j-1,seq(3,(noLags+1)*2,by=2)]%*%coeffsMedian[seq(1,noLags*2,by=2)]
  currShock = idShockMedian[posDateStart+j-1-noLags]/stdShock #first shock lost due to lag creation
  shockContrib[j] = currShock%*%irfMedian[1]
  
}

shockCumContrib = cumsum(shockContrib)
policyContrib = policyLevContrib + shockContrib
policyContrib = as.zoo(policyContrib,order.by = datesContrib)

library(ggplot2)
library(reshape2)

datesPlot = as.Date(datesContrib,'%m/%d/%Y')
value <- dataZoo[datesContrib,2]
variable = "Actual"
actualDF <- as.data.frame(value)
actualDF = cbind(actualDF,datesPlot,variable)
plotDF <- as.data.frame(cbind(policyContrib,growthContrib))
plotDF = cbind(plotDF,datesPlot)
meltedPlot <- melt(plotDF, id.vars = c('datesPlot'))

plot = ggplot(meltedPlot, aes(x=datesPlot,y=value)) +
  geom_area(aes(colour=variable, fill=variable)) +
  geom_line(data = actualDF,aes(y=value))
#scale_y_continuous(limits = c(-0.5, 1))

Plot

#Haver Data Example

'Some examples do download data from Haver.'

library("Haver")
'Different plain vanilla options'
data1 <- haver.data('usecon:gdp')
data2 <- haver.data('gdp','usecon')
dataZoo <- haver.data('gdp','usecon',rtype='zoo')

'Using wildcards'
dataAvailable <- haver.metadata(dat="EMERGELA")
codesList <- dataAvailable$code
matchNames <- grep("^C273RB", codesList,ignore.case=TRUE) 'all names beginning with c273rb'
namesWildCard <- codesList[matchNames]
dataWildCard <- haver.data(namesWildCard,'EMERGELA',rtype='zoo')
  
haverZoo = function(haverStr) {
  
  library(R.utils)
  
  if (isPackageLoaded('Haver') == FALSE) {
    library(Haver)
  }
  return(as.zoo(haver.data(haverStr,rtype='zoo')))
  
}

#Holston-Laubach-Williams State Space Model

library("FKF")
library("Ryacas")

#Creating Lags
ibc_br = as.zoo(haver.data('EMERGELA:N223GVI',rtype='zoo'))
ibc_br_yoy = delPC(ibc_br,lagDel=12)
y = ibc_br_yoy
y_l1 = lag(y,-1)
y_l2 = lag(y,-2)

pi = ipca_yoy
pi_l1 = lag(pi,-1)
pi_l2 = lag(pi,-2)

r = real_exAnte
r_l1 = lag(r,-1)
r_l2 = lag(r,-2)

i_nom = selic

#Definitional State Equations
eqDS1 <- y_tilde - 100*(y - y_star)
eqDS2 <- rtilde - (r - r_star)
eqDS3 <- r_tilde_ld - (r_tilde_l1 + r_tilde_l2)
eqDS4 <- r_tilde_l1 - (r_l1 - r_star_l1)
eqDS4 <- r_tilde_l2 - (r_l2 - r_star_l2)

#Definitional Observation Equations
eqDO1 <- pi_l1

#Model Equations

r_star <- Sym("r_star")
y_star <- Sym("y_star")
r_tilde <- Sym("r_tilde")
y_tilde <- Sym("y_tilde")
g <- Sym("g")
z <- Sym("z")
stateVec <- expression(y,pi,y_l1,y_l2,pi_l1,
                       pi_l2,r_star, y_star, r_tilde, y_tilde,
                       g, z,r_star_l1, r_star_l2, y_star_l1,
                       y_star_l2,r_tilde_l1, r_tilde_l2,r_tilde_ld,y_tilde_l1,
                       y_tilde_l2,r,i_nom)

obsVec <- c(y,pi,r)

nState = 23
dt = c(0,0,0,0,0,0,0)

b_y = runif(1)
phi_pi = runif(1)
phi_y = runif(1)
b_pi = runif(1)
ar = runif(1)
ay1 = runif(1)
ay2 = runif(1)

#Contemporaneous Matrix
eq1_lhs = c(1,0,0,0,0,0,0,-1,0,-1/100,0,0,0,0,0,0,0,0,0,0,0,0,0)
eq2_lhs = c(0,1,0,0,0,0,0,0,0,-b_y,0,0,0,0,0,0,0,0,0,0,0,0,0)
eq3_lhs = c(0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
eq4_lhs = c(0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
eq5_lhs = c(0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)

eq6_lhs = c(0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
eq7_lhs = c(0,0,0,0,0,0,1,0,0,0,-1,-1,0,0,0,0,0,0,0,0,0,0,0)
eq8_lhs = c(0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
eq9_lhs = c(0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,-1,0)
eq10_lhs = c(0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0)

eq11_lhs = c(0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0)
eq12_lhs = c(0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0)
eq13_lhs = c(0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0)
eq14_lhs = c(0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0)
eq15_lhs = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0)

eq16_lhs = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0)
eq17_lhs = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0)
eq18_lhs = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0)
eq19_lhs = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-1,-1,1,0,0,0,0)
eq20_lhs = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0)
eq21_lhs = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0)
eq22_lhs = c(0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,-1)
eq23_lhs = c(0,-phi_pi,0,0,0,0,0,0,0,-phi_y,0,0,0,0,0,0,0,0,0,0,0,0,1)

TT_toInv = c()
for (i in 1:23) {
  # print(i)
  # exprStr = parse(text=paste("print(length(eq",i,"_lhs))",sep=""))
  # eval(exprStr)
  exprStr = parse(text=paste("TT_toInv = c(TT_toInv,eq",i,"_lhs)",sep=""))
  eval(exprStr)
}
TT_toInv = matrix(TT_toInv,nState,nState)
TT_aux = TT_toInv
TT_toInv = solve(TT_toInv)

#Lagged Matrix
eq1_rhs = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
eq2_rhs = c(0,0,0,0,b_pi,(1-b_pi),0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
eq3_rhs = c(1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
eq4_rhs = c(0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
eq5_rhs = c(0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)

eq6_rhs = c(0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
eq7_rhs = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
eq8_rhs = c(0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0)
eq9_rhs = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
eq10_rhs = c(0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,ar/2,ay1,ay2,0,0)

eq11_rhs = c(0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0)
eq12_rhs = c(0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0)
eq13_rhs = c(0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
eq14_rhs = c(0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0)
eq15_rhs = c(0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)

eq16_rhs = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0)
eq17_rhs = c(0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
eq18_rhs = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0)
eq19_rhs = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
eq20_rhs = c(0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0)

eq21_rhs = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0)
eq22_rhs = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
eq23_rhs = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)

TT_rhs = c()
for (i in 1:23) {
  # print(i)
  # exprStr = parse(text=paste("print(length(eq",i,"_rhs))",sep=""))
  # eval(exprStr)
  exprStr = parse(text=paste("TT_rhs = c(TT_rhs,eq",i,"_rhs)",sep=""))
  eval(exprStr)
}
TT_rhs = matrix(TT_rhs,nState,nState)

Tt = TT_toInv%*%TT_rhs

obsVarInds = c(1,2,22,23) #in State equation

nObsVar = length(obsVarInds)
Zt = matrix(0,nObsVar,nState)
for (i in 1:length(obsVarInds)) {
  Zt[i,obsVarInds[i]] = 1
}

ct = matrix(0,nObsVar,1)
dt = matrix(0,nState,1)

#Shock Matrices
Gt = matrix(0,nObsVar,nObsVar) #observation equations are all identities; all structure as state

Ht = matrix(0,nState,nState)
shockEq = c(10,2,8,11,12,23)
for (i in 1:length(shockEq)) {
  exprStr = parse(text=paste("sd",i," = runif(1)",sep=""))
  eval(exprStr)
  exprStr = parse(text=paste("Ht[shockEq[i],shockEq[i]] = sd",i,sep=""))
  eval(exprStr)
}

#State Equation Steady State
state_ss = rep(0,nState)
#state_ss = solve(diag(nState)-Tt)%*%dt

#Initial Values
a0 = state_ss
P0 = diag(nState)

#Observations
zooObs = merge(y,pi,r,i_nom)
yt = t(as.matrix(zooObs[,1:4]))

GGt = Gt%*%t(Gt)
HHt = Ht%*%t(Ht)
kf_obj = fkf(a0,P0,dt,ct,Tt,Zt,HHt,GGt,yt)

importEviews <- function(csvFilePath) {
  
  'Imports Eviews data saved as .csv file'
    library(zoo)
  rawCSV = read.csv(csvFilePath,header = TRUE, stringsAsFactors = FALSE)
  datesCSV <- as.Date(rawCSV[,1])
  rawCSV <- rawCSV[,-1]
  zooCSVObj = zoo(x=rawCSV,order.by=datesCSV)
  invisible(zooCSVObj)
  
}

  'Inflation Prediction'
  
  rm(list = ls())
  source('Z:/ASSET/ECO/LATAM/MX/Codes/R/sourceFuns.R', echo=TRUE)
  
  require(lubridate)
  library(xlsx)
  
  dataDepPC <- importEviews("Z:/ASSET/ECO/LATAM/MX/Inflação/REL/R Selection/inpcLines.csv")
  candidates <- importEviews("Z:/ASSET/ECO/LATAM/MX/Inflação/REL/R Selection/candidates.csv")
  #dfSel <- read.table("Z:/ASSET/ECO/LATAM/MX/Trackings/Selection/csvSel.csv",header = TRUE,sep=",")
  dfSel <- read.xlsx("Z:/ASSET/ECO/LATAM/MX/Inflação/Forecast Sheet/sniimSelected.xlsx",
                      sheetIndex = 2)
  
  'To do: smarter way to find first forecast Date NA after data begins.'
  completeData = dataDepPC[,1]
  btmSquareData = completeData[400:length(completeData),]
  allIndex = index(btmSquareData)
  naData = is.na(btmSquareData)
  naData = cumsum(naData)
  foreDateInd = which(naData==1)
  foreDate = allIndex[foreDateInd]
  
  foreTable = as.data.frame(matrix(NA,NCOL(dataDepPC),2))
  colnames(foreTable) <- c("Variable",paste("Forecast:",foreDate))
  
  totPred = NROW(dfSel)
  bestList = list()
  errorTab = as.data.frame(matrix(NA,NCOL(dataDepPC),2))
  colnames(errorTab) <- c("Variable","Prop Error")
  
  ini = 1
  fin = totPred
  for (i in ini:fin) {
    
    currLine = as.matrix(dfSel[i,])
    depName = currLine[1,1]
    
    varName = "begin"
    varList = c()
    j = 3
    while (varName != "" & j <= NCOL(currLine) & is.na(varName) == FALSE) {
      varName = currLine[1,j]
      if (varName == "" | is.na(varName) == TRUE) {
      } else {
        varList = c(varList,varName)
      }
      j = j+1
    }
    
    y = dataDepPC[,depName]
    #Default predictors
    depSAR <- lag(y,-24)
    depAROne <- lag(y,-1)
    depARTwo <- lag(y,-2)
    
    #limit maximum number of variables per line
    nVarMax = 3
    varList = varList[seq(1,min(length(varList),nVarMax))]
    varList = varList[seq(1,max(min(length(varList),nVarMax),1))]
    
    x = merge(depSAR,depAROne,depARTwo,candidates[,varList])
    bestSbst = bestSubsetFn(y,x,subsetMin = 2,subsetMax=4)
    bestList = c(bestList,list(bestSbst))
  
    currPred = bestSbst
    
    #errorTab[i,1] = depName
    #errorTab[i,2] = bestSbst$modelsPerformance[1,1]
    foreTable[i,1] = depName
    foreTable[i,2] = bestSbst$modelsAvg[foreDate]
    
    print(paste("Forecasting Date: ",foreDate,sep=""))
    print(paste(i,"/",totPred,sep=""))
    print(paste(bestSbst$modelsAvg[foreDate]))
  
  }
  
  library(xlsx)
  write.xlsx(foreTable, "Z:/ASSET/ECO/LATAM/MX/Inflação/Forecast Sheet/rFore.xlsx",
             row.names = FALSE)

'Best Call'

dataDepPC <- importEviews("Z:/ASSET/ECO/LATAM/MX/Inflação/REL/R Selection/inpclines.csv")
candidates <- importEviews("Z:/ASSET/ECO/LATAM/MX/Inflação/REL/R Selection/candidates.csv")

y = dataDepPC[,1]

ySeas = delPC(y,24)
predictorsName = c("pc_id_1_1_2_108_l1_l2","pc_id_1_1_2_110_l1_l2","pc_id_1_1_2_98_l24",
                "pc_id_1_2_2_14_l24_l2","pc_id_1_2_2_17_l1","pc_id_1_2_2_27_l1_l1",
               "pc_id_2_2_1_0_l24","pc_m2_l1","pc_pcutvn_l1","pc_pcuw1n_l1_l1")
x = candidates[,predictorsName]
x = cbind(x,ySeas)

plotINPC <- bestSubsetFn(y,x)

integPC <- function(zooPC,lagDel = 1,zooLevel) {
  
  #Não está robusto, as vezes faz errado (provavelmente buraco nas séries).
  
  pcSquare = squareData(zooPC)
  pcSquare_num = as.numeric(pcSquare)
  levelSquare = squareData(zooLevel)
  
  pcSquare = na.omit(pcSquare)
  levelSquare = na.omit(levelSquare)
  tsPC = index(pcSquare)
  tsLevel = index(levelSquare)
  
  firstPC_inLevel = which(tsLevel==tsPC[1])
  zooIntegAux = as.numeric(rep(NA,length(pcSquare_num)+lagDel))
  
  zooIntegAux[1:lagDel] = as.numeric(levelSquare[(firstPC_inLevel-lagDel):(firstPC_inLevel-1)])
  zooIntegAuxZoo = zoo(zooIntegAux,order.by = tsLevel[(firstPC_inLevel-lagDel):(firstPC_inLevel-1)])
  
  for (i in 1:length(pcSquare_num)) {
    currDate = tsPC[i]
    currDateLag = index(zooIntegAuxZoo)[i]
    currPC = as.numeric(window(pcSquare,start = currDate, end = currDate))
    zooIntegAux[lagDel+i] <- zooIntegAux[i]*(1+currPC)
    currDates = c(index(zooIntegAuxZoo),currDate)
    zooIntegAuxZoo = zoo(zooIntegAux,order.by = currDates)
  }
  
  dates = c(tsLevel[(firstPC_inLevel-lagDel):(firstPC_inLevel-1)],tsPC)
  zooInteg = as.zoo(zooIntegAux,order.by = dates)

  return(zooInteg)
  
}

iterativeLasso <- function(x,y,threshVars=10) {
  
  nVarsSel = NA
  while (nVarsSel > threshVars & is.na(nVarsSel) == 1) {
    
    selDF <- as.data.frame(cbind(y,x))
    lassoObj <- adaLassoTS(selDF)
    selDFSquare = squareData(selDF)
    y = as.matrix(selDFSquare[,1])
    x = as.matrix(selDFSquare[,-1])
    selCV <- lassoCV(lassoObj,x,y)
    nVarsSel = length(selCV)
    x = x[,selCV]
    
  }
  
  xSlct = x
  returnList <- list(selectedX = xSlct,selectedIndcs = selCV)
  invisible(returnList)
  
}

library("glmnet")

simMC = 100
seqT = seq(100,1000,100)
seqM = seq(2,100,5)
nGridT = length(seqT)
nGridM = length(seqM)
succDummy = array(0,c(nGridT,nGridM,simMC))
nTrue = 1

totIter = nGridT*nGridM*simMC
counter = 0

i = 0
for (T in seqT) {
  
  i = i+1
  j = 0
  
  for (M in seqM) {

    j = j+1
    
    for (m in 1:simMC) {
      
      counter = counter + 1
      
      x = matrix( rnorm(T*M,mean=0,sd=1), T, M) 
      xTrue = x[,seq(1,nTrue)]
      beta = matrix(rnorm(nTrue,mean=0,sd=0.5),nTrue,1)
      y = xTrue%*%beta + matrix(rnorm(T,0,1),T,1)
      lassoObj = glmnet(x,y)
      selCV <- lassoCV(lassoObj,x,y,forcedNoVar=1)
      
      succDummy[i,j,m] = 0
      if (length(names(selCV)) > 0) {
        if (sum(names(selCV) == "V1")==1) {
          succDummy[i,j,m] = 1
        }
      } else {
        succDummy[i,j,m] = 0
      }
      print(counter/totIter)

    }
    
  }

}

successMapping= apply(succDummy,c(1,2),mean)

lassoCV <- function(lassoObj,x,y,pen = NA,forcedNoVar = NA) {
  
  #Performs Post-Lasso cross validation via information criteria.
  
  lambdas = lassoObj$lambda
  nRegPath = length(lambdas)
  iCrit = rep(NA,nRegPath)
  schCoeffs = lassoObj$beta
  T = nrow(x)
  bicPen = log(T)
  
  if (is.na(pen) == TRUE) {
    pen = bicPen
  }
  
  for (i in 1:nRegPath) {
    
    skCoeffs = schCoeffs[,i]
    selVars = which(skCoeffs!=0)
    xPost = x[,selVars]
    postDF = as.data.frame(cbind(y,xPost))
    postReg <- lm(y ~ 1 + .,data=postDF)
    iCrit[i] = AIC(postReg,k=pen)
    
  }
  
  'Minimizing information criteria'
  selMod = which(iCrit==min(iCrit))
  selMod = min(selMod) #obtaining most parsimonious model if there are ties
  
  selVarsIndcs = which(schCoeffs[,selMod]!=0)
  
  'In case you want to force number of variables.'
  if (is.na(forcedNoVar) == FALSE) {
    noMatSels = colSums(schCoeffs!=0)
    forcedInds = which(noMatSels<=forcedNoVar) #finding regularization number which leads to forcedNoVars variables
    forcedInds = max(forcedInds) #avoiding ties
    selVarsIndcs = which(schCoeffs[,forcedInds]!=0)
  }
  
  invisible(selVarsIndcs)      
}
rmseVec = c(10,0.08,0.15,0.89)
w0 = c(0.5,0.5,0.5,0.5)
teste = nleqslv(w0, oosWeights, oosWeightsJacobian, rmseVec)
jacteste = oosWeightsJacobian(w0,rmseVec)

oosLin <- function(dfAll,depVarInd = 1,horizon=1,window=NA,dRolling=0) {
  
'Performs out-of-sample prediction for linear model.'
  
  library(zoo)
  
  dfAllOri = as.data.frame(dfAll)
  dfAll = as.data.frame(dfAll)
  dfAll = squareData(dfAll)
  
  dfDepVar = as.data.frame(dfAllOri[,depVarInd],row.names=rownames(dfAllOri))
  dfDepVarSqrd = squareData(dfDepVar)
  timeIndexY = rownames(dfDepVarSqrd)
  
  dfPredictors = as.data.frame(dfAllOri[,-depVarInd],row.names=rownames(dfAllOri))
  dfPredictorsSqrd = squareData(dfPredictors)
  timeIndexX = rownames(dfPredictorsSqrd)
  
  # timeIndex = union(timeIndexY,timeIndexX)
  timeIndex = timeIndexX
  
  dfEstimate = as.data.frame(dfAllOri[timeIndex,])
  oosSeries = data.frame(rep(NA,length(timeIndex)),row.names=timeIndex)
  row.names(oosSeries) <- timeIndex
  smplT = length(timeIndex)
  
  if (is.na(window)==TRUE) {
    window = round(0.5*length(timeIndex))
  }
  
  h = horizon
  oosInitT = window + h
  
  sumFirst = 0
  if (dRolling == 1) {
    sumFirst = 1
  }
  counterInitSmpl = 0

  for (j in oosInitT:smplT) {
    
    dfRestr <- dfEstimate[seq(1+counterInitSmpl,j-h),]
    namesReg <- colnames(dfAll)
    formulaReg <- as.formula(paste(namesReg[depVarInd]," ~ 1 + ."))
    currReg <- lm(formulaReg,dfRestr)
    dfPredict <- dfEstimate[seq(1,smplT),] #note that here we predict for longer horizons assuming we know the predictors value
    oosPredict <- predict(currReg,dfPredict)
    insmplPredict <- predict(currReg,dfRestr)

    oosSeries[j,] <- oosPredict[j] 
    
    counterInitSmpl = counterInitSmpl + sumFirst #will only sum if dRolling = 1
  
  }
  
  #Calculating loss function - to do: mod
  oosZoo = zoo(oosSeries,order.by=timeIndex)
  actualsZoo = zoo(dfPredict[,depVarInd],order.by=timeIndex)
  squaredLoss = as.numeric((oosZoo - actualsZoo)^2)
  squaredLoss = mean(squaredLoss[complete.cases(squaredLoss)])
  absLoss = as.numeric(abs(oosZoo - actualsZoo))
  absLoss = mean(absLoss[complete.cases(absLoss)])
  loss = list(squared = squaredLoss, abs = absLoss)

  datesZoo <- as.Date(timeIndex)
  #To do: organize outptut for general index.
  oosSeriesZoo = zoo(oosSeries,order.by=datesZoo)
  oosSeriesZoo = oosSeriesZoo[complete.cases(oosSeriesZoo)]
  
  returnList <- list(lossFns = loss,oosTS = oosSeriesZoo)
  
  return(returnList)
  
}

oosWeights <- function(w,...) {
  args = list(...)
  oos = args[[1]]
  p = args[[2]]
  y <- numeric(length(w))
  for (i in (1:length(w)-1)) {
    y[i] <- (w[i]/w[i+1])^p - oos[i+1]/oos[i]
  }
  y[length(w)] = sum(w)-1
  y
}

oosWeightsJacobian <- function(w,...) {
  args = list(...)
  oos = args[[1]]
  p = args[[2]]
  n <- length(w)
  Df <- matrix(numeric(n*n),n,n)
  for (i in 1:n-1) {
    for (j in 1:n) {
      Df[i,j] = 0
      if (i==j) {
        Df[i,j] = p*((w[i]/w[i+1])^(p-1))*(1/w[i+1])
      }
      if (i+1==j) {
        Df[i,j] = p*((w[i]/w[i+1])^(p-1))*-1*w[j-1]*w[j]^(-2)
      }
    }
  }
  Df[n,] = rep(1,n,1)
  Df
}

# Particular case with p=1.
# oosWeightsJacobian <- function(w,...) {
#   args = list(...)
#   oos = args[[1]]
#   n <- length(w)
#   Df <- matrix(numeric(n*n),n,n)
#   for (i in 1:n-1) {
#     for (j in 1:n) {
#       Df[i,j] = 0
#       if (i==j) {
#         Df[i,j] = 1/w[i+1]
#       }
#       if (i+1==j) {
#         Df[i,j] = -1*w[j-1]*w[j]^(-2)
#       }
#     }
#   }
#   Df[n,] = rep(1,n,1)
#   Df
# }

#OOS Weights Teste

library("nleqslv")

source('Z:/ASSET/ECO/LATAM/MX/Codes/R/oosWeights.R', echo=TRUE)
source('Z:/ASSET/ECO/LATAM/MX/Codes/R/oosWeightsJacobian.R', echo=TRUE)

rmseVec = c(0.2,5.5,0.1)
#rmseVec = rnorm(3)^2
w0 = rep(0.5,length(rmseVec))
p = 1 # p < 1 still not working as expected
weightsTeste = oosWeights(w0,rmseVec,p)
jacTeste = oosWeightsJacobian(w0,rmseVec,p)
weightsSolver = nleqslv(w0, oosWeights, oosWeightsJacobian, rmseVec, p)
print(weightsSolver$x)

'Risk-Free Bond: Maturity, Price and Face Value'

T = 10
F = 1000
r = 0.05
P = matrix(0,T,1)
for (t in 0:9) {
  
  P[10-t] = F/(1+r)^t 
  
}
time = seq(1,10)

#Growth Pricing

setwd("Z:/ASSET/ECO/Marcelo/Growth Pricing")

library(Haver)
library(zoo)
library(xlsx)
library(vars)
library(VARsignR)

source('Z:/ASSET/ECO/LATAM/MX/Codes/R/sourceFuns.R', echo=TRUE)

mxn <- as.zoo(haver.data('daily:x273smc',rtype='zoo'))
yld <- as.zoo(haver.data('intdaily:r273cag',rtype='zoo'))

labelsAxis = c("MXN","1Yr Cetes")

v1 = mxn
v2 = yld

freq = 30
cDummy= FALSE

#Downloading data
v1 <- (v1 - lag(v1,k=-freq))/lag(v1,k=-freq)
v2 <- (v2 - lag(v2,k=-freq))

simContr = 100
noLags = 12
startSample = "2016-01-01"
refDateDelta = "2016-11-08"

v1Plot = v1[index(v1)>=refDateDelta]
v2Plot = v2[index(v2)>=refDateDelta]

dataZoo = merge(v1,v2)
dataSRSVAR = dataZoo
dataZoo = dataZoo[index(dataZoo)>startSample]
dataZoo = squareData(dataZoo)

dataSRSVAR = dataSRSVAR[index(dataSRSVAR)>startSample]
dataSRSVAR = squareData(dataSRSVAR)
dfSRSVAR = as.data.frame(dataSRSVAR)
dataSRSVAR = ts(dataSRSVAR)

refDateInd = which(index(dataZoo)==refDateDelta)

depVarInd = 1

#Reduced Form
varOLS <- VAR(dataSRSVAR,p=noLags,type="none")
if (cDummy == TRUE) {
  varOLS <- VAR(dataSRSVAR,p=noLags,type="const")
}
redRes = residuals(varOLS)
v1Res = redRes[,depVarInd]

#######################################
#Identifying Shock 1
constr <- c(-1,+2)

modelSRSVAR <- uhlig.reject(Y=dataSRSVAR, nlags=noLags, draws=simContr, subdraws=simContr,
                            nkeep=simContr*10, KMIN=1,KMAX=3, constrained=constr, constant=cDummy,
                            steps=100)

fevd = modelSRSVAR$FEVDS
irfs = modelSRSVAR$IRFS

vl <- c(labelsAxis[1],labelsAxis[2])


irfplot(irfdraws=irfs, type="median", labels=vl, save=FALSE, bands=c(0.16, 0.84),
        grid=TRUE, bw=FALSE)


fevdplot(fevd, label=vl, save=FALSE, bands=c(0.16, 0.84), grid=TRUE,
         bw=FALSE, table=FALSE, periods=NULL)

fevd.table <- fevdplot(fevd, table=TRUE, label=vl, periods=c(1,10,20,30,40,50,60))

#print(fevd.table)

depVar= dataZoo[,depVarInd]
irfDraws = modelSRSVAR$IRFS[,,depVarInd]
irfMedian = apply(irfDraws,2,median)
nObs = NROW(dataZoo) - noLags

#Identified Shock
idShockDraws = modelSRSVAR$SHOCKS
idShockMedian = apply(idShockDraws,2,median)
#stdShock = 1
stdShock = sd(idShockMedian)

coeffsDraws = modelSRSVAR$BDraws[,,depVarInd]
coeffsMedian = apply(coeffsDraws,2,median)

dateStart = refDateDelta
iniv1 = depVar[which(index(depVar)==dateStart)]

dateEnd = index(dataZoo)[NROW(dataZoo)]
posDateStart = which(index(dataZoo)==dateStart)
posDateEnd = which(index(dataZoo)==dateEnd)
nShocksElem = posDateEnd - posDateStart
datesContrib = index(dataZoo[seq(posDateStart,posDateEnd)])
endv1 = depVar[which(index(depVar)==dateEnd)]

contRedRes = redRes[posDateStart]
totDepContrib = as.numeric(endv1[1]) - as.numeric(iniv1[1])
iniShock = idShockMedian[posDateStart-noLags]/stdShock
iniPolicyShockContrib = irfMedian[1]*iniShock
#iniResid - calcular do VAR

dataZooCreate = dataZoo
'Adding Lags to dataZoo'
for (l in 1:noLags) {
  currLag = lag(dataZooCreate,k=-l)
  colnames(currLag) <- c(paste("v1_l",l,sep=""),paste("v2_l",l,sep=""))
  dataZoo = merge(dataZoo,currLag)
}

shockContrib = rep(NA,nShocksElem+1)
policyLevContrib = rep(NA,nShocksElem+1)
deltaShockContrib = rep(NA,nShocksElem+1)

for (j in 0:nShocksElem) {
  
  policyLevContrib[j] = dataZoo[posDateStart+j-1,seq(3,(noLags+1)*2,by=2)]%*%coeffsMedian[seq(1,noLags*2,by=2)]
  currShock = idShockMedian[posDateStart+j-noLags]/stdShock
  shockContrib[j] = currShock%*%irfMedian[1]
  
  currVecShock = idShockMedian[(posDateStart+0-noLags):(posDateStart+j-noLags)]/stdShock
  currIrf = rev(irfMedian[1:(j+1)])
  deltaShockContrib[j+1] = currVecShock%*%currIrf
  
}

deltaPolicyContrib = deltaShockContrib
#shockCumContrib = cumsum(shockContrib)
#policyContrib = policyLevContrib + shockContrib
#policyContrib = as.zoo(policyContrib,order.by = datesContrib)

#############################################
#Identifying Shock 2
depVarInd = 2
dataZoo = merge(v2,v1)
dataSRSVAR = dataZoo
dataZoo = dataZoo[index(dataZoo)>startSample]
dataZoo = squareData(dataZoo)

dataSRSVAR = dataSRSVAR[index(dataSRSVAR)>startSample]
dataSRSVAR = squareData(dataSRSVAR)
dfSRSVAR = as.data.frame(dataSRSVAR)
dataSRSVAR = ts(dataSRSVAR)

constr <- c(+1,+2)

modelSRSVAR <- uhlig.reject(Y=dataSRSVAR, nlags=noLags, draws=simContr, subdraws=simContr, nkeep=simContr*10, KMIN=1,
                            KMAX=3, constrained=constr, constant=cDummy, steps=100)

fevd = modelSRSVAR$FEVDS
irfs = modelSRSVAR$IRFS

vl <- c(labelsAxis[2],labelsAxis[1])

irfplot(irfdraws=irfs, type="median", labels=vl, save=FALSE, bands=c(0.16, 0.84),
        grid=TRUE, bw=FALSE)


fevdplot(fevd, label=vl, save=FALSE, bands=c(0.16, 0.84), grid=TRUE,
         bw=FALSE, table=FALSE, periods=NULL)

fevd.table <- fevdplot(fevd, table=TRUE, label=vl, periods=c(1,10,20,30,40,50,60))

print(fevd.table)

depVar= dataZoo[,depVarInd]
irfDraws = modelSRSVAR$IRFS[,,depVarInd]
irfMedian = apply(irfDraws,2,median)

idShockDraws = modelSRSVAR$SHOCKS
idShockMedian = apply(idShockDraws,2,median)
#stdShock = 1
stdShock = sd(idShockMedian)

coeffsDraws = modelSRSVAR$BDraws[,,depVarInd]
coeffsMedian = apply(coeffsDraws,2,median)

dateStart = refDateDelta
iniYld = depVar[which(index(depVar)==dateStart)]
#iniYld = window(yld,start = as.Date(dateStart), end = as.Date(dateStart))

dateEnd = index(dataZoo)[NROW(dataZoo)]
posDateStart = which(index(dataZoo)==dateStart)
posDateEnd = which(index(dataZoo)==dateEnd)
nShocksElem = posDateEnd - posDateStart
datesContrib = index(dataZoo[seq(posDateStart,posDateEnd)])

iniShock = idShockMedian[posDateStart-noLags]/stdShock
iniGrowthShockContrib = irfMedian[1]*iniShock

dataZooCreate = dataZoo
'Adding Lags to dataZoo'
for (l in 1:noLags) {
  currLag = lag(dataZooCreate,k=-l)
  colnames(currLag) <- c(paste("v2_l",l,sep=""),paste("v1_l",l,sep=""))
  dataZoo = merge(dataZoo,currLag)
}

shockContrib = rep(NA,nShocksElem+1)
growthLevContrib = rep(NA,nShocksElem+1)
deltaShockContrib = rep(NA,nShocksElem+1)
for (j in 0:nShocksElem) {
  
  growthLevContrib[j] = dataZoo[posDateStart+j-1,seq(3,(noLags+1)*2,by=2)]%*%coeffsMedian[seq(1,noLags*2,by=2)]
  currShock = idShockMedian[posDateStart+j-noLags]/stdShock #shock lost due to lag creation
  shockContrib[j] = currShock%*%irfMedian[1]
  
  currVecShock = idShockMedian[(posDateStart+0-noLags):(posDateStart+j-noLags)]/stdShock
  currIrf = rev(irfMedian[1:(j+1)])
  deltaShockContrib[j+1] = currVecShock%*%currIrf
  
}

deltaGrowthContrib = deltaShockContrib
#shockCumContrib = cumsum(shockContrib)
#growthContrib = growthLevContrib + shockContrib
#growthContrib = as.zoo(growthContrib,order.by = datesContrib)

library(ggplot2)
library(reshape2)

contShock = iniPolicyShockContrib + iniGrowthShockContrib
totDepContrib = totDepContrib + contShock

datesContribPlot = datesContrib
datesPlot = as.Date(datesContribPlot,'%m/%d/%Y')
value <- deltaGrowthContrib + deltaPolicyContrib
variable = "Actual"
actualDF <- as.data.frame(value)
actualDF = cbind(actualDF,datesPlot,variable)
plotDF <- as.data.frame(cbind(deltaPolicyContrib,deltaGrowthContrib))
plotDF = cbind(plotDF,datesPlot)
meltedPlot <- melt(plotDF, id.vars = c('datesPlot'))

plot = ggplot(meltedPlot, aes(x=datesPlot,y=value)) +
  geom_area(aes(colour=variable, fill=variable)) +
  geom_line(data = actualDF,aes(y=value)) +
  ylab("MXN Shocks") + xlab("") +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) + theme_bw()

plot

sourceEntireFolder <- function(folderName, verbose=FALSE, showWarnings=TRUE) { 
  files <- list.files(folderName, full.names=TRUE)
  
  # Grab only R files
  files <- files[ grepl("\\.[rR]$", files) ]
  
  if (!length(files) && showWarnings)
    warning("No R files in ", folderName)
  
  for (f in files) {
    if (verbose)
      cat("sourcing: ", f, "\n")
    ## TODO:  add caught whether error or not and return that
    try(source(f, local=FALSE, echo=FALSE), silent=!verbose)
  }
  return(invisible(NULL))
}

#Source useful functions

source('Z:/ASSET/ECO/LATAM/MX/Codes/R/importEviews.R', echo=TRUE)
source('Z:/ASSET/ECO/LATAM/MX/Codes/R/squareData.R', echo=TRUE)
source('Z:/ASSET/ECO/LATAM/MX/Codes/R/adaLasso.R', echo=TRUE)
source('Z:/ASSET/ECO/LATAM/MX/Codes/R/adaLassoTS.R', echo=TRUE)
source('Z:/ASSET/ECO/LATAM/MX/Codes/R/calcBIC.R', echo=TRUE)
source('Z:/ASSET/ECO/LATAM/MX/Codes/R/lassoCV.R', echo=TRUE)
source('Z:/ASSET/ECO/LATAM/MX/Codes/R/iterativeLasso.R', echo=TRUE)
source('Z:/ASSET/ECO/LATAM/MX/Codes/R/delPC.R', echo=TRUE)
source('Z:/ASSET/ECO/LATAM/MX/Codes/R/gridLasso.R', echo=TRUE)
source('Z:/ASSET/ECO/LATAM/MX/Codes/R/oosLin.R', echo=TRUE)
source('Z:/ASSET/ECO/LATAM/MX/Codes/R/dfFilter.R', echo=TRUE)
source('Z:/ASSET/ECO/LATAM/MX/Codes/R/bestSubsetFn.R', echo=TRUE)
source('Z:/ASSET/ECO/LATAM/MX/Codes/R/delPC_ur.R', echo=TRUE)

squareData <- function(data) {
  
  completeInds <- complete.cases(data)
  if (NCOL(data) > 1) {
    dataComplete <- data[complete.cases(data),]
  } else {
    if (class(data) == "data.frame") {
      dataComplete <- data[complete.cases(data),]
    } else {
      dataComplete <- data[complete.cases(data)]
    }
  }
  
  if (class(data) == "data.frame") {
    
    names = rownames(data)
    namesComplete = names[completeInds]
    dataComplete = as.data.frame(dataComplete,row.names=namesComplete)
    
  }
  
  invisible(dataComplete)
  
}

stochLasso <- function(selDF,depInd = 1,n_s = 100, k_j = NA) {
  
  x = selDF[,-depInd]
  y = selDF[,depInd]
  
  namesX = names(x)
  
  T = NROW(y)
  K = NCOL(x)
  if (is.na(k_j)) {
    k_j = round(0.3*K)
  }
  
  stochSel = list()
  for (i in 1:n_s) {
    
    indSub = sample(seq(1,K),k_j)
    xSub = x[,indSub]
    dfSub = merge(y,xSub)
    adaObj <- adaLasso(dfSub)
    selCV <- lassoCV(adaObj,xSub,y,forcedNoVar = 5)
    stochSel = c(stochSel,list(selCV))
    print(i)
    
  }
  
  tabSumm = table(unlist(stochSel))
  namesInds = names(tabSumm)
  names(tabSumm) <- namesX[as.numeric(namesInds)]
  
  return(tabSumm)
  
}

library("Ryacas")

ys <- Sym("ys")
pis <- Sym("pis")

phipis = Sym("phipis")
phiys = Sym("phiys")

eq1 = ys - phipis*pis
eq2 = phiys*ys+phiys

symbSys = List(eq1,eq2)

J = deriv(symbSys,phiys)

#TS Example
startDate = 1990
frequency = 24
T = 48
finalDate = floor(startDate + (1/frequency)*T)
finalDec = T%%frequency
data <- as.data.frame(rnorm(T, mean = 0, sd = 1))
tsObj <- ts(data = data, frequency = frequency, start=c(startDate,1), end=c(finalDate,finalDec))
plot(tsObj)

#ZOO Example
library("zoo")
td = seq(as.Date("1990/1/1"), as.Date("1992/12/1"), "months")
zooObj <- zoo(x=data,order.by=td)

#Importing from CSV for ZOO
zooCSV = read.zoo("C:/Users/teixjar/Desktop/eviewsrNA.csv",format="%m/%d/%Y", sep=",",header=T,
                  index.column = 1)
rawCSV = read.csv("C:/Users/teixjar/Desktop/eviewsrNA.csv",header = TRUE, stringsAsFactors = FALSE)
datesCSV <- as.Date(rawCSV[,1])
rawCSV <- rawCSV[,-1]
zooCSVObj = zoo(x=rawCSV,order.by=datesCSV)

#Using as a function
source("C:/Users/teixjar/Desktop/importEviews.R")
eviewsData <- importEviews("C:/Users/teixjar/Desktop/inpc.csv")

#Unit Root Test
library(fUnitRoots)
library(zoo)
library(Haver)

#No Trend with Constant
urSer1 = as.zoo(haver.data('emergela:N273VM',rtype='zoo')) #IMEF Mfg NSA

#With Trend
urSer2 = as.zoo(haver.data('emergela:N273GVI',rtype='zoo')) #Nominal GDP

#No Trend no Constant
urSer3 = as.zoo(haver.data('usecon:MPCUFF',rtype='zoo')) #CPI Item %Change

zooObj = merge(urSer1,urSer2,urSer3)

deltaZoo = delPC_ur(zooObj)

# urTes = adfTest(urSer,type="c")
# urRes = urTes@test
# urPal = urRes$p.value

urConsTest <- function(urSer,threshVal = 0.05) {
  
  library(fUnitRoots)
  #Tests whether a series rejects a unit root test with constant with significance level of threshVal.
  #Returns a dummy = 1 if does not reject null of unit root.
  
  source("Z:/ASSET/ECO/LATAM/MX/Codes/R/squareData.R")
  
  urSerTest = squareData(urSer)
  
  if (var(urSerTest) > 0) {
    
    urTes = adfTest(urSer,type="c")
    urRes = urTes@test
    urPal = urRes$p.value
    
    returnVal = 1
    if (urPal < threshVal) {
      returnVal = 0 
    }
  
  } else {
    returnVal = 0
  }
  
  return(returnVal)
  
}

  #Selecting predictors for INPC Lines
  
  #Clears everything.
  rm(list = ls())
  
  source('Z:/ASSET/ECO/LATAM/MX/Codes/R/sourceFuns.R', echo=TRUE)
  
  library("zoo")
  
  'Inflation'
  dataDepPC <- importEviews("Z:/ASSET/ECO/LATAM/MX/Inflação/REL/R Selection/inpclines.csv")
  candidates <- importEviews("Z:/ASSET/ECO/LATAM/MX/Inflação/REL/R Selection/candidates.csv")
  
  'Generic'
  #dataDepPC <- importEviews("Z:/ASSET/ECO/LATAM/MX/Trackings/Selection/depVar.csv")
  #candidates <- importEviews("Z:/ASSET/ECO/LATAM/MX/Trackings/Selection/candidates_predictive.csv")
  
  minVar = 7
  maxVar = 12
  
  selList = list()
  nDep = NCOL(dataDepPC)
  depNames = names(dataDepPC)
  predNames = names(as.data.frame(candidates))
  
  candidates = candidates[index(candidates) >= "2010-01-01"]
  
  for (i in 1:nDep) {
  
    dep <- dataDepPC[,i]
    #Seasonality Term for Inflation
    depSAR <- lag(dep,-24)
    depAROne <- lag(dep,-1)
    
    selDF <- as.data.frame(cbind(dep,depSAR,depAROne,candidates))
    dataDF <- squareData(as.data.frame(selDF))
    y = as.matrix(dataDF[,1])
    x = as.matrix(dataDF[,-1])
    npInds = c(1)
    
    #Grid Lasso
    #adaObj <- gridLasso(x,y,minVar,maxVar,npInds)
    #selCV <- adaObj$selectedIndcs
    #lassoObj <- adaObj$lassoObj
    
    #Ada Lasso
    adaObj <- adaLasso(selDF,npInds)
    selCV <- lassoCV(adaObj,x,y,pen=4)
    nVarsSel = length(selCV)
  
    selNames = names(selCV)
    
    currList = c(depNames[i],"Selected Variables",selNames)
    selList[[i]] = currList
    
    print(i)
    
  }
  
  'Organzing output into Rel Table'
  lenSel <- sapply(selList,length)
  maxSel <- max( lenSel )
  naFill <- maxSel - lenSel
  tableSel <- mapply( function(x,y) c( x , rep( "" , y ) ) , selList , naFill)
  tableSel <- data.frame(t(tableSel))

library(xlsx)
if (ncol(tableSel) > 1) {
  write.xlsx(tableSel, "Z:/ASSET/ECO/LATAM/MX/Trackings/Selection/lassoSel.xlsx",
             row.names=FALSE,col.names = FALSE)
  write.csv(tableSel, "Z:/ASSET/ECO/LATAM/MX/Trackings/Selection/csvSel.csv",
            row.names=FALSE)
}

tr = "@pcy"
selTr = list()
library("stringr")

for (j in 2:ncol(tableSel)) {

  varName = as.character(tableSel[,j])
  
  #Identifying number of lags
  lagsStr = str_locate_all(varName,"_l")
  
  lagNum = 0
  lags = lagsStr[[1]]
  if (is.na(lags[1]) == FALSE) {
    for (k in 1:nrow(lags)) {
      lagNum = lagNum + as.integer(substr(varName,lags[k,2]+1,lags[k,2]+1))
    }
    lagNum = paste("(-",as.character(lagNum),")")  
  } else {
    lagNum = as.character()  
  }
  
  varHaver = substr(varName,1,regexpr('_',varName)-1)
  if (substr(varName,nchar(varName)-1,nchar(varName)-1) == "p") {
    power = substr(varName,nchar(varName),nchar(varName))
    trVar = paste("@pcy(EMERGELA::",varHaver,lagNum,")^",as.character(power))
  } else {
    trVar = paste("EMERGELA::",varHaver,lagNum)
  }
  trVar = gsub(" ", "", trVar, fixed = TRUE)
  selTr[[j-1]] = trVar
}

lenSel <- sapply(selTr,length)
maxSel <- max( lenSel )
naFill <- maxSel - lenSel
tableSelTr <- mapply( function(x,y) c( x , rep( "" , y ) ) , selTr , naFill)
tableSelTr <- data.frame(t(tableSelTr))
# 'Plotting Selected Series'
# plotList <- selList[[2]]
# plotDep <- plotList[1]
# plotPred <- plotList[3]
# depPC.df <- as.data.frame(dataDepPC)
# pred.df <- as.data.frame(candidates[-1,])
# plotSeriesDep <- depPC.df[plotDep]
# plotSeriesPred <- pred.df[plotPred]
# dates = rownames(depPC.df)
# plotZoo <- na.omit(zoo(as.zoo(cbind(plotSeriesDep,plotSeriesPred)),order.by=dates))
# p <- autoplot.zoo(plotZoo, facet = NULL)

library(Haver)
library(apt)
source('Z:/ASSET/ECO/LATAM/MX/Codes/R/haverZoo.R', echo=TRUE)

brep_rate = haverZoo('EMERGELA:N233RDE')
cloans_rate = haverZoo('EMERGELA:N233RLC')

tarDF = squareData(merge(brep_rate,cloans_rate))

timeIndex = index(tarDF[,1])

y = ts(tarDF[,2])
x = ts(tarDF[,1])

tarObj <- ciTarFit(y, x, model = c('tar'), lag = 0, thresh = 0)
mtarObj <- ciTarFit(y, x, model = c('mtar'), lag = 0, thresh = 0)

mktRate = zoo(y,order.by = timeIndex)
residual = zoo(mtarObj$z,order.by=timeIndex)
cb_rate = zoo(x,order.by = timeIndex)

require(lubridate)
#appending scenario to cb_rate
newDates = timeIndex[length(timeIndex)] %m+% months(1)
values = as.numeric(cb_rate)
cb_rate_scn = as.zoo(c(values,5.25),order.by = c(timeIndex,newDates))

expRates = as.data.frame(merge(cb_rate,residual),row.names=timeIndex)
library(xlsx)
write.xlsx(expRates,'Z:\\ASSET\\ECO\\Marcelo\\TAR Rates\\resTar.xlsx')

nLag = 6
lagReg = as.data.frame(squareData(merge(lag(cb_rate,k=-nLag),residual)))
colnames(lagReg) <- c(paste("cb_rate_l",nLag,sep=""),"residual")
lagEff = lm(residual ~ 0 + cb_rate_l6,data=lagReg)
summary(lagEff)

tarObj = mtarObj

#Forecasting residual
ciCoefs = summary(tarObj$CI)$coefficients[,1]
horizon = 12
currRes = residual[length(residual)]
preRes = residual[length(residual)-1]
critRes = as.numeric(currRes) - as.numeric(preRes) #momentum model
fctRes = c()
for (t in 1:horizon) {
  if (critRes > 0) {
    uptRes = (1+ciCoefs[1])*currRes
  }
  else {
    uptRes = (1+ciCoefs[2])*currRes
  }
  # if (t==6) { #epsilon shock in residual, if wanted
  #   uptRes = uptRes + summary(lagEff)$coefficients[1]*(-0.25)
  # }
  critRes = as.numeric(uptRes) - as.numeric(currRes)
  fctRes = c(fctRes,uptRes)
  currRes = uptRes
}
fctResSer = appZoo(fctRes,residual)

#Forecasting from tarObj - Long-Run
lrCoefs = summary(tarObj$LR)$coefficients[,1]
horizon = 12
cbScen = rep(5.25,horizon)
currSer = mktRate[length(mktRate)]
fctSer = c()
for (t in 1:horizon) {
  currSer = sum(c(1,cbScen[t])*lrCoefs) + fctRes[t]
  fctSer = c(fctSer,currSer)
  currSer = currSer
}
fctRate = appZoo(fctSer,mktRate)
fctCbRate = appZoo(cbScen,cb_rate)

expRates = as.data.frame(merge(fctCbRate,fctResSer),row.names=index(fctCbRate))
library(xlsx)
write.xlsx(expRates,'Z:\\ASSET\\ECO\\Marcelo\\TAR Rates\\resTar.xlsx')

appZoo = function(newValues,zooObj) {
  
  require(lubridate)
  newDates = index(zooObj)[length(zooObj)] %m+% months(seq(1,length(newValues)))
  newTimeIndex = c(timeIndex,newDates)
  newSer = as.zoo(c(as.numeric(zooObj),newValues),order.by = newTimeIndex)
  return(newSer)
  
}
#Estimates HLW

nParaMod = 8
nParaSD = 6
nPara = nParaMod+nParaSD #6 standard deviations

de_param <- list(min	= c(rep(-2,nParaMod),rep(0,nParaSD)),
                 max	= c(rep(2,nParaMod),rep(100,nParaSD)))

para_0 = rep(0.5,nPara)

#Regular Optimizaton
sol_reg <- optim(para_0,fn=hlw)

#Genetic Optimization
library("GA")
sol <- ga(type = "real-valued", 
             fitness = hlw, 
             popSize = 100, 
             maxiter = 2,
             min = de_param$min, max = de_param$max
          )

library("mFilter")
library("Haver")
library("tempdisagg")

gdp = as.zoo(haver.data('EMERGELA:N223NGPI',rtype='zoo'))
gdp_yoy = delPC(gdp,4,dPerc=1)
      
selic = as.zoo(haver.data('emergela:N223RD',rtype='zoo'))
inf_exp = as.zoo(haver.data('emergela:N223VCP1',rtype='zoo'))

dfRates = merge(selic,inf_exp)
real_exAnte = as.zoo(squareData(dfRates[,1]-dfRates[,2]))

selic_cfFilter = cffilter(selic,type="asymmetric")
selic_hpFilter = hpfilter(selic,type="lambda",freq=129600)
real_hpFilter = hpfilter(real_exAnte,type="lambda",freq=129600)
  
real_trend_hp = as.zoo(real_hpFilter$trend,order.by=index(real_exAnte))
real_gap = real_exAnte - real_trend_hp
barplot(real_gap)

selic_trend_hp = as.zoo(selic_hpFilter$trend,order.by=index(selic))
df = merge(selic_trend_hp,inf_exp)
real_deflated_hp = as.zoo(squareData(df[,1]-df[,2]))

selic_trend_cf = as.zoo(selic_cfFilter$trend,order.by=index(selic))
df = merge(selic_trend_cf,inf_exp)
real_deflated_cf = as.zoo(squareData(df[,1]-df[,2]))

#Neutral Rate Decomposition
prodGth = as.ts(haver.data('emergela:h223elpi',rtype='ts'))
prodGth_m = td(prodGth ~ 1, method = "denton-cholette", conversion = "average",to="monthly")
prodGth_m = as.zoo(prodGth_m$values,order.by=seq(as.Date("2002/4/1"),as.Date("2012/12/1"),
                                                 "months"))

#Disaggregating annual data to monthly
infTgt = zoo(haver.data('emergela:a223vmcp',rtype='zoo'),order.by=seq(1999,2018),freq=1)
infTgt.na =  merge(as.zoo(infTgt),
                           + zoo(, seq(start(infTgt)[1], end(infTgt)[1], 1/12)))
infTgt_mon = na.locf(infTgt.na)
infTgt_mon = zoo(infTgt_mon,order.by=as.yearmon(index(infTgt_mon)))
ipca = as.zoo(haver.data('emergela:c223pca',rtype='zoo'))
ipca_yoy = delPC(ipca,12,dPerc = 1)
ipca_yoy = zoo(ipca_yoy,order.by=as.yearmon(index(ipca_yoy)))

df = merge(ipca_yoy,infTgt_mon)
infGap = df[,1] - df[,2]

inf_exp = zoo(inf_exp,order.by=as.yearmon(index(inf_exp)))
df = merge(lag(inf_exp,-12),ipca_yoy)
infSurprise = df[,1] - df[,2]

#Debt-to-GDP Ratio
dbt = as.zoo(haver.data('emergela:c223gdtp',rtype='zoo'))

#Credit-to-GDP Ratio
credit = as.zoo(haver.data('emergela:n223fct',rtype='zoo'))
nominal_gdp = as.zoo(haver.data('emergela:c223gpm',rtype='zoo'))
nominal_gdp_12m = rollapply(nominal_gdp,12,sum,align="right")
df = merge(credit,nominal_gdp_12m)
credit_gdp = df[,1]/df[,2]

#Global Interest Rates
tsy_3m = as.zoo(haver.data('usecon:fcm3me',rtype='zoo'))
tsy_10y = as.zoo(haver.data('usecon:fcm10e',rtype='zoo'))

dfReg = as.data.frame(squareData(merge(real_exAnte,#nfGap,infSurprise)))
                                       dbt,credit_gdp,tsy_3m,tsy_10y)))

fundReg = lm(real_exAnte ~ 1 + .,data=dfReg)
fitted = predict(fundReg,type="response")

regDecomp <- function(lmObj) {
  designMat = as.matrix(data.frame(1,lmObj$model[,-1]))
  print(colnames(designMat))
  colnames(designMat)[1] = "constant"
  coeffs = lmObj$coefficients
  fittedValue = designMat%*%coeffs
  components = designMat%*%diag(coeffs)
  colnames(components) = colnames(designMat)
  fittedDecomp = data.frame(fittedValue,components)
  return(fittedDecomp)
}
exportDecomp = regDecomp(fundReg)
write.csv(exportDecomp, "C:/Users/teixjar/Desktop/compsTeste.csv")

#td package offers more convoluted disaggregation methods
infTgt = as.ts(haver.data('emergela:a223vmcp',rtype='ts'))
infTgt = td(infTgt ~ 1,conversion='first',to='monthly',method='ols')

hlw <- function(parameters) {

#Holston-Laubach-Williams State Space Model

library("FKF")
library("Ryacas")

#Data
ibc_br = as.zoo(haver.data('EMERGELA:N223GVI',rtype='zoo'))
ibc_br_yoy = delPC(ibc_br,lagDel=12)
y = ibc_br_yoy

ipca = as.zoo(haver.data('emergela:c223pca',rtype='zoo'))
ipca_yoy = delPC(ipca,12)
pi = ipca_yoy

selic = as.zoo(haver.data('emergela:N223RD',rtype='zoo'))
inf_exp = as.zoo(haver.data('emergela:N223VCP1',rtype='zoo'))
dfRates = merge(selic,inf_exp)
real_exAnte = as.zoo(squareData(dfRates[,1]-dfRates[,2]))
r = real_exAnte

i_nom = selic

#Observables
zooObs = merge(y,pi,r,i_nom)
yt = t(as.matrix(zooObs[,1:4]))

#Model Equations
stateVec <- expression(y,pi,y_l1,y_l2,pi_l1,
                       pi_l2,r_star, y_star, r_tilde, y_tilde,
                       g, z,r_star_l1, r_star_l2, y_star_l1,
                       y_star_l2,r_tilde_l1, r_tilde_l2,r_tilde_ld,y_tilde_l1,
                       y_tilde_l2,r,i_nom)

nState = 23

b_y = parameters[1]
phi_pi = parameters[2]
phi_y = parameters[3]
b_pi = parameters[4]
ar = parameters[5]
ay1 = parameters[6]
ay2 = parameters[7]
# taylorCons = parameters[8]
lastPara = 7

#Contemporaneous Matrix
eq1_lhs = c(1,0,0,0,0,0,0,-1,0,-1/100,0,0,0,0,0,0,0,0,0,0,0,0,0)
eq2_lhs = c(0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
eq3_lhs = c(0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
eq4_lhs = c(0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
eq5_lhs = c(0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)

eq6_lhs = c(0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
eq7_lhs = c(0,0,0,0,0,0,1,0,0,0,-1,-1,0,0,0,0,0,0,0,0,0,0,0)
eq8_lhs = c(0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
eq9_lhs = c(0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,-1,0)
eq10_lhs = c(0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0)

eq11_lhs = c(0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0)
eq12_lhs = c(0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0)
eq13_lhs = c(0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0)
eq14_lhs = c(0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0)
eq15_lhs = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0)

eq16_lhs = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0)
eq17_lhs = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0)
eq18_lhs = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0)
eq19_lhs = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-1,-1,1,0,0,0,0)
eq20_lhs = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0)
eq21_lhs = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0)
eq22_lhs = c(0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,-1)
eq23_lhs = c(0,-phi_pi,0,0,0,0,0,0,0,-phi_y,0,0,0,0,0,0,0,0,0,0,0,0,1)

removeEq = c(22,23,17,16)
nStateLiq = nState-length(removeEq)

TT_toInv = c()
eqsLoop = seq(1,nState)[is.na(pmatch(seq(1,nState),removeEq))]
for (i in eqsLoop) {
  # print(i)
  # exprStr = parse(text=paste("print(length(eq",i,"_lhs))",sep=""))
  # eval(exprStr)
  if (length(removeEq) > 0) {
    for (j in removeEq) {
    exprStr = parse(text=paste("eq",i,"_lhs = ","eq",i,"_lhs[-",j,"]",sep=""))
    eval(exprStr)
    }
  }
  exprStr = parse(text=paste("TT_toInv = c(TT_toInv,eq",i,"_lhs)",sep=""))
  eval(exprStr)
}


TT_toInv = matrix(TT_toInv,nStateLiq,nStateLiq)
TT_aux = TT_toInv
TT_toInv = solve(TT_toInv)

#Lagged Matrix
eq1_rhs = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
eq2_rhs = c(0,0,0,0,b_pi,(1-b_pi),0,0,0,b_y,0,0,0,0,0,0,0,0,0,0,0,0,0)
eq3_rhs = c(1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
eq4_rhs = c(0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
eq5_rhs = c(0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)

eq6_rhs = c(0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
eq7_rhs = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
eq8_rhs = c(0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0)
eq9_rhs = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
eq10_rhs = c(0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,ar/2,ay1,ay2,0,0)

eq11_rhs = c(0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0)
eq12_rhs = c(0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0)
eq13_rhs = c(0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
eq14_rhs = c(0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0)
eq15_rhs = c(0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)

eq16_rhs = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0)
eq17_rhs = c(0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
eq18_rhs = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0)
eq19_rhs = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
eq20_rhs = c(0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0)

eq21_rhs = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0)
eq22_rhs = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
eq23_rhs = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)

TT_rhs = c()
for (i in 1:nState) {
  # print(i)
  # exprStr = parse(text=paste("print(length(eq",i,"_rhs))",sep=""))
  # eval(exprStr)
  if (length(removeEq) > 0) {
    for (j in removeEq)
      exprStr = parse(text=paste("eq",i,"_rhs = ","eq",i,"_rhs[-",j,"]",sep=""))
    eval(exprStr)
  }
  exprStr = parse(text=paste("TT_rhs = c(TT_rhs,eq",i,"_rhs)",sep=""))
  eval(exprStr)
}
TT_rhs = matrix(TT_rhs,nStateLiq,nStateLiq)

Tt = TT_toInv%*%TT_rhs

obsVarInds = c(1,2,22,23) #in State equation

obsVarInds = obsVarInds[-which(obsVarInds==removeEq)]
nObsVar = length(obsVarInds)
Zt = matrix(0,nObsVar,nStateLiq)
for (i in 1:length(obsVarInds)) {
  Zt[i,obsVarInds[i]] = 1
}

ct = matrix(0,nObsVar,1)
dt = matrix(0,nStateLiq,1)
#dt[23] = taylorCons

#Shock Matrices
Gt = matrix(0,nObsVar,nObsVar) #observation equations are all identities; all structure as state

Ht = matrix(0,nStateLiq,nStateLiq)
shockEq = c(10,2,8,11,12,23)

shockEq = shockEq[-which(shockEq==removeEq)]
for (i in 1:length(shockEq)) {
  exprStr = parse(text=paste("sd",i," = parameters[",i+lastPara,"]",sep=""))
  eval(exprStr)
  exprStr = parse(text=paste("Ht[shockEq[i],shockEq[i]] = sd",i,sep=""))
  eval(exprStr)
}

#State Equation Steady State
state_ss = rep(0,nStateLiq)
#state_ss = solve(diag(nState)-Tt)%*%dt

#Initial Values
a0 = state_ss
P0 = diag(nState)

GGt = Gt%*%t(Gt)
HHt = Ht%*%t(Ht)
kf_obj = fkf(a0,P0,dt,ct,Tt,Zt,HHt,GGt,yt)

return(kf_obj$logLik)

}

hlw <- function(parameters) {

#Holston-Laubach-Williams State Space Model

library("FKF")
library("Ryacas")

#Data
ibc_br = as.zoo(haver.data('EMERGELA:N223GVI',rtype='zoo'))
ibc_br_yoy = delPC(ibc_br,lagDel=12)
y = ibc_br_yoy

ipca = as.zoo(haver.data('emergela:c223pca',rtype='zoo'))
pi = ipca_yoy

selic = as.zoo(haver.data('emergela:N223RD',rtype='zoo'))
inf_exp = as.zoo(haver.data('emergela:N223VCP1',rtype='zoo'))
dfRates = merge(selic,inf_exp)
real_exAnte = as.zoo(squareData(dfRates[,1]-dfRates[,2]))
r = real_exAnte

i_nom = selic

#Observables
zooObs = merge(y,pi,r,i_nom)
yt = t(as.matrix(zooObs[,1:4]))

#Model Equations
stateVec <- expression(y,pi,y_l1,y_l2,pi_l1,
                       pi_l2,r_star, y_star, r_tilde, y_tilde,
                       g, z,r_star_l1, r_star_l2, y_star_l1,
                       y_star_l2,r_tilde_l1, r_tilde_l2,r_tilde_ld,y_tilde_l1,
                       y_tilde_l2,r,i_nom)

nState = 23
dt = c(0,0,0,0,0,0,0)

b_y = parameters[1]
phi_pi = parameters[2]
phi_y = parameters[3]
b_pi = parameters[4]
ar = parameters[5]
ay1 = parameters[6]
ay2 = parameters[7]
lastPara = 7

#Contemporaneous Matrix
eq1_lhs = c(1,0,0,0,0,0,0,-1,0,-1/100,0,0,0,0,0,0,0,0,0,0,0,0,0)
eq2_lhs = c(0,1,0,0,0,0,0,0,0,-b_y,0,0,0,0,0,0,0,0,0,0,0,0,0)
eq3_lhs = c(0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
eq4_lhs = c(0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
eq5_lhs = c(0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)

eq6_lhs = c(0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
eq7_lhs = c(0,0,0,0,0,0,1,0,0,0,-1,-1,0,0,0,0,0,0,0,0,0,0,0)
eq8_lhs = c(0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
eq9_lhs = c(0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,-1,0)
eq10_lhs = c(0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0)

eq11_lhs = c(0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0)
eq12_lhs = c(0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0)
eq13_lhs = c(0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0)
eq14_lhs = c(0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0)
eq15_lhs = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0)

eq16_lhs = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0)
eq17_lhs = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0)
eq18_lhs = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0)
eq19_lhs = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-1,-1,1,0,0,0,0)
eq20_lhs = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0)
eq21_lhs = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0)
eq22_lhs = c(0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,-1)
eq23_lhs = c(0,-phi_pi,0,0,0,0,0,0,0,-phi_y,0,0,0,0,0,0,0,0,0,0,0,0,1)

TT_toInv = c()
for (i in 1:23) {
  # print(i)
  # exprStr = parse(text=paste("print(length(eq",i,"_lhs))",sep=""))
  # eval(exprStr)
  exprStr = parse(text=paste("TT_toInv = c(TT_toInv,eq",i,"_lhs)",sep=""))
  eval(exprStr)
}
TT_toInv = matrix(TT_toInv,nState,nState)
TT_aux = TT_toInv
TT_toInv = solve(TT_toInv)

#Lagged Matrix
eq1_rhs = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
eq2_rhs = c(0,0,0,0,b_pi,(1-b_pi),0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
eq3_rhs = c(1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
eq4_rhs = c(0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
eq5_rhs = c(0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)

eq6_rhs = c(0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
eq7_rhs = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
eq8_rhs = c(0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0)
eq9_rhs = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
eq10_rhs = c(0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,ar/2,ay1,ay2,0,0)

eq11_rhs = c(0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0)
eq12_rhs = c(0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0)
eq13_rhs = c(0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
eq14_rhs = c(0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0)
eq15_rhs = c(0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)

eq16_rhs = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0)
eq17_rhs = c(0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
eq18_rhs = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0)
eq19_rhs = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
eq20_rhs = c(0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0)

eq21_rhs = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0)
eq22_rhs = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
eq23_rhs = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)

TT_rhs = c()
for (i in 1:23) {
  # print(i)
  # exprStr = parse(text=paste("print(length(eq",i,"_rhs))",sep=""))
  # eval(exprStr)
  exprStr = parse(text=paste("TT_rhs = c(TT_rhs,eq",i,"_rhs)",sep=""))
  eval(exprStr)
}
TT_rhs = matrix(TT_rhs,nState,nState)

Tt = TT_toInv%*%TT_rhs

obsVarInds = c(1,2,22,23) #in State equation

nObsVar = length(obsVarInds)
Zt = matrix(0,nObsVar,nState)
for (i in 1:length(obsVarInds)) {
  Zt[i,obsVarInds[i]] = 1
}

ct = matrix(0,nObsVar,1)
dt = matrix(0,nState,1)

#Shock Matrices
Gt = matrix(0,nObsVar,nObsVar) #observation equations are all identities; all structure as state

Ht = matrix(0,nState,nState)
shockEq = c(10,2,8,11,12,23)
for (i in 1:length(shockEq)) {
  exprStr = parse(text=paste("sd",i," = parameters[",i+lastPara,"]",sep=""))
  eval(exprStr)
  exprStr = parse(text=paste("Ht[shockEq[i],shockEq[i]] = sd",i,sep=""))
  eval(exprStr)
}

#State Equation Steady State
state_ss = rep(0,nState)
#state_ss = solve(diag(nState)-Tt)%*%dt

#Initial Values
a0 = state_ss
P0 = diag(nState)

GGt = Gt%*%t(Gt)
HHt = Ht%*%t(Ht)
kf_obj = fkf(a0,P0,dt,ct,Tt,Zt,HHt,GGt,yt)

return(kf_obj$logLik)

}

  #Growth Pricing
  
  setwd("Z:/ASSET/ECO/Marcelo/Growth Pricing")
  
  library(zoo)
  library(xlsx)
  library(vars)
  library(VARsignR)
  
  source('Z:/ASSET/ECO/LATAM/MX/Codes/R/sourceFuns.R', echo=TRUE)
  
  yield_10y_us = haver.data('weekly:fcm10',rtype='zoo')
  
  data = importEviews("Z:/ASSET/ECO/Marcelo/Financial VAR/signData.csv")
  data = data[index(data)>"2005-01-01"]
  
  freq = 12
  cDummy= TRUE
  
  #Downloading data
  yld_10y <- data[,4]
  brlusd <- data[,1]
  cds_br = data[,2]
  br_1y = data[,5]
  usd_tw = data[,6]
  
  # yld_10y <- (yld_10y - lag(yld_10y,k=-freq))
  brlusd <- ((brlusd - lag(brlusd,k=-freq))/lag(brlusd,k=-freq))*100
  cds_br <- ((cds_br - lag(cds_br,k=-freq))/lag(cds_br,k=-freq))*100
  # br_1y = (br_1y - lag(br_1y,k=-freq))
  usd_tw <- (usd_tw - lag(usd_tw,k=-freq))/lag(usd_tw,k=-freq)
  
  simContr = 20000
  noLags = 4
  startSample = "2005-01-01"
  
  refDateDelta = "2016-11-08"
  
  #######################################
  #Identifying Policy Shock
  dataZoo = merge(yld_10y,brlusd,cds_br,br_1y,usd_tw)
  dataSRSVAR = dataZoo
  dataZoo = dataZoo[index(dataZoo)>startSample]
  dataZoo = squareData(dataZoo)
  
  dataSRSVAR = dataSRSVAR[index(dataSRSVAR)>startSample]
  dataSRSVAR = squareData(dataSRSVAR)
  dfSRSVAR = as.data.frame(dataSRSVAR)
  dataSRSVAR = ts(dataSRSVAR)
  
  constr <- c(-1,+2,-4) # shock of interest enters first.
  
  modelSRSVAR <- uhlig.reject(Y=dataSRSVAR, nlags=noLags, draws=simContr,
                              KMIN=1,KMAX=4, constrained=constr, constant=cDummy,
                              steps=36)
  
  fevd = modelSRSVAR$FEVDS
  irfs = modelSRSVAR$IRFS
  idShockDraws = modelSRSVAR$SHOCKS
  idShockMedian = apply(idShockDraws,2,median)
  
  irfMedian = apply(irfs,c(2,3),median)
  
  vl <- c("10 Yr Yield","BRL USD","CDS_BR","BR_1Y","USD_TW")
  
  us10yr_irf = as.zoo(irfMedian[,1])
  brl_usd_irf = as.zoo(irfMedian[,2])

irfplot(irfdraws=irfs, type="median", labels=vl, save=FALSE, bands=c(0.16, 0.84),
        grid=TRUE, bw=FALSE)

fevdplot(fevd, label=vl, save=FALSE, bands=c(0.16, 0.84), grid=TRUE,
         bw=FALSE, table=FALSE, periods=NULL)

fevd.table <- fevdplot(fevd, table=TRUE, label=vl, periods=c(1,10,20,30,40,50,60))

print(fevd.table)

depVarInd = 2
depVar= dataZoo[,depVarInd]
irfDraws = modelSRSVAR$IRFS[,,depVarInd]
irfMedian = apply(irfDraws,2,median)
nObs = NROW(dataZoo) - noLags

#Reduced Form
varOLS <- VAR(dataSRSVAR,p=noLags)
redRes = residuals(varOLS)
yldRes = redRes[,depVarInd]

#Identified Shock
idShockDraws = modelSRSVAR$SHOCKS
idShockMedian = apply(idShockDraws,2,median)
#stdShock = 1
stdShock = sd(idShockMedian)

coeffsDraws = modelSRSVAR$BDraws[,,depVarInd]
coeffsMedian = apply(coeffsDraws,2,median)

dateStart = refDateDelta
iniYld = depVar[which(index(depVar)==dateStart)]
#iniYld = window(yld,start = as.Date(dateStart), end = as.Date(dateStart))

dateEnd = index(dataZoo)[NROW(dataZoo)]
posDateStart = which(index(dataZoo)==dateStart)
posDateEnd = which(index(dataZoo)==dateEnd)
nShocksElem = posDateEnd - posDateStart
datesContrib = index(dataZoo[seq(posDateStart,posDateEnd)])
endYld = depVar[which(index(depVar)==dateEnd)]

contRedRes = yldRes[posDateStart]
totDepContrib = as.numeric(endYld[1]) - as.numeric(iniYld[1])
iniShock = idShockMedian[posDateStart-noLags]/stdShock
iniPolicyShockContrib = irfMedian[1]*iniShock
#iniResid - calcular do VAR

dataZooCreate = dataZoo
'Adding Lags to dataZoo'
for (l in 1:noLags) {
  currLag = lag(dataZooCreate,k=-l)
  colnames(currLag) <- c(paste("yld_l",l,sep=""),paste("msci_l",l,sep=""))
  dataZoo = merge(dataZoo,currLag)
}

shockContrib = rep(NA,nShocksElem)
policyLevContrib = rep(NA,nShocksElem)
deltaShockContrib = rep(NA,nShocksElem)
for (j in 1:nShocksElem) {
  
  policyLevContrib[j] = dataZoo[posDateStart+j-1,seq(3,(noLags+1)*2,by=2)]%*%coeffsMedian[seq(1,noLags*2,by=2)]
  currShock = idShockMedian[posDateStart+j-noLags]/stdShock
  shockContrib[j] = currShock%*%irfMedian[1]
  
  currVecShock = idShockMedian[(posDateStart+1-noLags):(posDateStart+j-noLags)]/stdShock
  currIrf = rev(irfMedian[1:j])
  deltaShockContrib[j] = currVecShock%*%currIrf
  
}

deltaPolicyContrib = deltaShockContrib
shockPolicyCumContrib = sum(shockContrib)
policyContrib = policyLevContrib + shockContrib
policyContrib = as.zoo(policyContrib,order.by = datesContrib)

#############################################
#Identifying Growth Shock
dataZoo = merge(msci,yld)
dataSRSVAR = dataZoo
dataZoo = dataZoo[index(dataZoo)>startSample]
dataZoo = squareData(dataZoo)

dataSRSVAR = dataSRSVAR[index(dataSRSVAR)>startSample]
dataSRSVAR = squareData(dataSRSVAR)
dfSRSVAR = as.data.frame(dataSRSVAR)
dataSRSVAR = ts(dataSRSVAR)

constr <- c(+1,+2)

modelSRSVAR <- uhlig.reject(Y=dataSRSVAR, nlags=noLags, draws=simContr, subdraws=simContr, nkeep=simContr*10, KMIN=1,
                            KMAX=3, constrained=constr, constant=cDummy, steps=100)


fevd = modelSRSVAR$FEVDS
irfs = modelSRSVAR$IRFS

vl <- c("MSCI","10 Yr Yield")


irfplot(irfdraws=irfs, type="median", labels=vl, save=FALSE, bands=c(0.16, 0.84),
        grid=TRUE, bw=FALSE)


fevdplot(fevd, label=vl, save=FALSE, bands=c(0.16, 0.84), grid=TRUE,
         bw=FALSE, table=FALSE, periods=NULL)

fevd.table <- fevdplot(fevd, table=TRUE, label=vl, periods=c(1,10,20,30,40,50,60))

print(fevd.table)

depVarInd = 2
depVar= dataZoo[,depVarInd]
irfDraws = modelSRSVAR$IRFS[,,depVarInd]
irfMedian = apply(irfDraws,2,median)

idShockDraws = modelSRSVAR$SHOCKS
idShockMedian = apply(idShockDraws,2,median)
#stdShock = 1
stdShock = sd(idShockMedian)

coeffsDraws = modelSRSVAR$BDraws[,,depVarInd]
coeffsMedian = apply(coeffsDraws,2,median)

dateStart = refDateDelta
iniYld = depVar[which(index(depVar)==dateStart)]
#iniYld = window(yld,start = as.Date(dateStart), end = as.Date(dateStart))

dateEnd = index(dataZoo)[NROW(dataZoo)]
posDateStart = which(index(dataZoo)==dateStart)
posDateEnd = which(index(dataZoo)==dateEnd)
nShocksElem = posDateEnd - posDateStart
datesContrib = index(dataZoo[seq(posDateStart,posDateEnd)])

iniShock = idShockMedian[posDateStart-noLags]/stdShock
iniGrowthShockContrib = irfMedian[1]*iniShock

dataZooCreate = dataZoo
'Adding Lags to dataZoo'
for (l in 1:noLags) {
  currLag = lag(dataZooCreate,k=-l)
  colnames(currLag) <- c(paste("msci_l",l,sep=""),paste("yld_l",l,sep=""))
  dataZoo = merge(dataZoo,currLag)
}

shockContrib = rep(NA,nShocksElem)
growthLevContrib = rep(NA,nShocksElem)
deltaShockContrib = rep(NA,nShocksElem)
deltaHead = rep(NA,nShocksElem)
deltaHeadSeq = rep(NA,nShocksElem)
for (j in 1:nShocksElem) {
  
  growthLevContrib[j] = dataZoo[posDateStart+j-1,seq(3,(noLags+1)*2,by=2)]%*%coeffsMedian[seq(1,noLags*2,by=2)]
  currShock = idShockMedian[posDateStart+j-noLags]/stdShock #shock lost due to lag creation
  shockContrib[j] = currShock%*%irfMedian[1]
  
  currVecShock = idShockMedian[(posDateStart+1-noLags):(posDateStart+j-noLags)]/stdShock
  currIrf = rev(irfMedian[1:j])
  deltaShockContrib[j] = currVecShock%*%currIrf
  
  deltaHead[j] = as.numeric(depVar[posDateStart+j]) - as.numeric(iniYld)
  deltaHeadSeq[j] = as.numeric(depVar[posDateStart+j]) - as.numeric(depVar[posDateStart+j-1])
  
}

deltaGrowthContrib = deltaShockContrib
shockGrowthCumContrib = sum(shockContrib)
growthContrib = growthLevContrib + shockContrib
growthContrib = as.zoo(growthContrib,order.by = datesContrib)

library(ggplot2)
library(reshape2)

contShock = iniPolicyShockContrib + iniGrowthShockContrib
explDepContrib = shockGrowthCumContrib + shockPolicyCumContrib

datesContribPlot = datesContrib[2:length(datesContrib)]
datesPlot = as.Date(datesContribPlot,'%m/%d/%Y')
value <- deltaGrowthContrib + deltaPolicyContrib
#value <- dataZoo[datesContrib,2]
variable = "Actual"
actualDF <- as.data.frame(value)
actualDF = cbind(actualDF,datesPlot,variable)
plotDF <- as.data.frame(cbind(deltaPolicyContrib,deltaGrowthContrib))
plotDF = cbind(plotDF,datesPlot)
meltedPlot <- melt(plotDF, id.vars = c('datesPlot'))

plot = ggplot(meltedPlot, aes(x=datesPlot,y=value)) +
  geom_area(aes(colour=variable, fill=variable)) +
  geom_line(data = actualDF,aes(y=value))
#scale_y_continuous(limits = c(-0.5, 1))

plot


#######################################
#Identifying Growth Shock
dataZoo = merge(msci,yld)
dataSRSVAR = dataZoo
dataZoo = dataZoo[index(dataZoo)>"2015-01-01"]
dataZoo = squareData(dataZoo)

dataSRSVAR = dataSRSVAR[index(dataSRSVAR)>"2015-01-01"]
dataSRSVAR = squareData(dataSRSVAR)
dfSRSVAR = as.data.frame(dataSRSVAR)
dataSRSVAR = ts(dataSRSVAR)

constr <- c(+1,+2)
varOLS <- VAR(dataSRSVAR)
modelSRSVAR <- uhlig.reject(Y=dataSRSVAR, nlags=noLags, draws=simContr, subdraws=simContr,
                            nkeep=simContr*10, KMIN=1,KMAX=3, constrained=constr, constant=cDummy, steps=60)

fevd = modelSRSVAR$FEVDS
irfs = modelSRSVAR$IRFS

vl <- c("MSCI","10 Yr Yield")


irfplot(irfdraws=irfs, type="median", labels=vl, save=FALSE, bands=c(0.16, 0.84),
        grid=TRUE, bw=FALSE)


fevdplot(fevd, label=vl, save=FALSE, bands=c(0.16, 0.84), grid=TRUE,
         bw=FALSE, table=FALSE, periods=NULL)

fevd.table <- fevdplot(fevd, table=TRUE, label=vl, periods=c(1,10,20,30,40,50,60))

print(fevd.table)

depVarInd = 1
depVar= dataZoo[,depVarInd]
irfDraws = modelSRSVAR$IRFS[,,depVarInd]
irfMedian = apply(irfDraws,2,median)

idShockDraws = modelSRSVAR$SHOCKS
idShockMedian = apply(idShockDraws,2,median)

coeffsDraws = modelSRSVAR$BDraws[,,depVarInd]
coeffsMedian = apply(coeffsDraws,2,median)

dateStart = "2016-11-08"
iniYld = depVar[which(index(depVar)==dateStart)]
#iniYld = window(yld,start = as.Date(dateStart), end = as.Date(dateStart))

dateEnd = index(dataZoo)[NROW(dataZoo)]
posDateStart = which(index(dataZoo)==dateStart)
posDateEnd = which(index(dataZoo)==dateEnd)
nShocksElem = posDateEnd - posDateStart + 1
datesContrib = index(dataZoo[seq(posDateStart,posDateEnd)])

dataZooCreate = dataZoo
'Adding Lags to dataZoo'
for (l in 1:noLags) {
  currLag = lag(dataZooCreate,k=-l)
  colnames(currLag) <- c(paste("yld_l",l,sep=""),paste("msci_l",l,sep=""))
  dataZoo = merge(dataZoo,currLag)
}

#stdShock = 1
stdShock = sd(idShockMedian)
shockContrib = rep(NA,nShocksElem)
growthLevContrib = rep(NA,nShocksElem)
for (j in 1:nShocksElem) {
  
  growthLevContrib[j] = dataZoo[posDateStart+j-1,seq(3,(noLags+1)*2,by=2)]%*%coeffsMedian[seq(1,noLags*2,by=2)]
  currShock = idShockMedian[posDateStart+j-1-noLags]/stdShock
  shockContrib[j] = currShock%*%irfMedian[1]
  
}

shockCumContrib = cumsum(shockContrib)
growthContrib = growthLevContrib + shockContrib
growthContrib = as.zoo(growthContrib,order.by = datesContrib)

#############################################
#Identifying Policy Shock
dataZoo = merge(yld,msci)
dataSRSVAR = dataZoo
dataZoo = dataZoo[index(dataZoo)>"2010-01-01"]
dataZoo = squareData(dataZoo)

dataSRSVAR = dataSRSVAR[index(dataSRSVAR)>"2010-01-01"]
dataSRSVAR = squareData(dataSRSVAR)
dfSRSVAR = as.data.frame(dataSRSVAR)
dataSRSVAR = ts(dataSRSVAR)

constr <- c(+1,-2)

modelSRSVAR <- uhlig.reject(Y=dataSRSVAR, nlags=noLags, draws=simContr, subdraws=simContr, nkeep=simContr*10, KMIN=1,
                            KMAX=3, constrained=constr, constant=cDummy, steps=60)


fevd = modelSRSVAR$FEVDS
irfs = modelSRSVAR$IRFS

vl <- c("10 Yr Yield","MSCI")


irfplot(irfdraws=irfs, type="median", labels=vl, save=FALSE, bands=c(0.16, 0.84),
        grid=TRUE, bw=FALSE)


fevdplot(fevd, label=vl, save=FALSE, bands=c(0.16, 0.84), grid=TRUE,
         bw=FALSE, table=FALSE, periods=NULL)

fevd.table <- fevdplot(fevd, table=TRUE, label=vl, periods=c(1,10,20,30,40,50,60))

print(fevd.table)

depVarInd = 2
depVar= dataZoo[,depVarInd]
irfDraws = modelSRSVAR$IRFS[,,depVarInd]
irfMedian = apply(irfDraws,2,median)

idShockDraws = modelSRSVAR$SHOCKS
idShockMedian = apply(idShockDraws,2,median)

coeffsDraws = modelSRSVAR$BDraws[,,depVarInd]
coeffsMedian = apply(coeffsDraws,2,median)

dateStart = "2016-11-08"
iniYld = depVar[which(index(depVar)==dateStart)]
#iniYld = window(yld,start = as.Date(dateStart), end = as.Date(dateStart))

dateEnd = index(dataZoo)[NROW(dataZoo)]
posDateStart = which(index(dataZoo)==dateStart)
posDateEnd = which(index(dataZoo)==dateEnd)
nShocksElem = posDateEnd - posDateStart + 1
datesContrib = index(dataZoo[seq(posDateStart,posDateEnd)])

dataZooCreate = dataZoo
'Adding Lags to dataZoo'
for (l in 1:noLags) {
  currLag = lag(dataZooCreate,k=-l)
  colnames(currLag) <- c(paste("msci_l",l,sep=""),paste("yld_l",l,sep=""))
  dataZoo = merge(dataZoo,currLag)
}

#stdShock = 1
stdShock = sd(idShockMedian)
shockContrib = rep(NA,nShocksElem)
polictLevContrib = rep(NA,nShocksElem)
for (j in 1:nShocksElem) {
  
  policyLevContrib[j] = dataZoo[posDateStart+j-1,seq(3,(noLags+1)*2,by=2)]%*%coeffsMedian[seq(1,noLags*2,by=2)]
  currShock = idShockMedian[posDateStart+j-1-noLags]/stdShock #first shock lost due to lag creation
  shockContrib[j] = currShock%*%irfMedian[1]
  
}

shockCumContrib = cumsum(shockContrib)
policyContrib = policyLevContrib + shockContrib
policyContrib = as.zoo(policyContrib,order.by = datesContrib)

library(ggplot2)
library(reshape2)

datesPlot = as.Date(datesContrib,'%m/%d/%Y')
value <- dataZoo[datesContrib,2]
variable = "Actual"
actualDF <- as.data.frame(value)
actualDF = cbind(actualDF,datesPlot,variable)
plotDF <- as.data.frame(cbind(policyContrib,growthContrib))
plotDF = cbind(plotDF,datesPlot)
meltedPlot <- melt(plotDF, id.vars = c('datesPlot'))

plot = ggplot(meltedPlot, aes(x=datesPlot,y=value)) +
  geom_area(aes(colour=variable, fill=variable)) +
  geom_line(data = actualDF,aes(y=value))
#scale_y_continuous(limits = c(-0.5, 1))

plot
#Copom Pricing Sequencial

copomPricing_fn <- function(dt,noMeetings = 5) {
  
  #Puxar dados BMF do SQL e determinar datas daí.
  source('Z:/ASSET/ECO/ECOFIN/Chartbook Juros (Marcelo)/Códigos/matFiles.R')
  source('Z:/ASSET/ECO/ECOFIN/Chartbook Juros (Marcelo)/Códigos/sourceAux/downloadMatDB.R', echo=TRUE)
  loadMatDB(0)
  
  # dt = 42780
  dtStr = dt
  dt = dateDictionary(dt)
  
  objDB = ECOFIN.env$objConn
  # chaSQL = paste('SELECT [fltDate],[fltRateAverage] FROM [DB_ECO_FIN].[dbo].[tb_cdi_cetip]',
  #                ' WHERE fltDate = ',dt,sep="")
  # cdi_on <- sqlQuery(objDB,query=chaSQL)
  # cdi_on = cbind(cdi_on,dt)
  # names(cdi_on) = c("date","rateAv","settlementDt")
  # 
  # names(cdi_on)
  # 
  # chaSQL = paste('SELECT [fltDate],[fltRateAverage] FROM [DB_ECO_FIN].[dbo].[tb_cdi_cetip]',
  #                ' WHERE fltDate = ',dt,sep="")
  # 
  # 
  # bmf_fut = getBMFDIData(objDB,dt)
  # bmf_tab = bmf_fut[,c(1,9,3)]
  # names(bmf_tab) = c("date","rateAv","settlementDt")
  # ratesTab = rbind(cdi_on,bmf_tab)
  
  tabDI <- downloadCurveDI(dateDictionary(dt))
  
  date = tabDI$fltData
  rateAv = tabDI$fltSettlementRate
  settlementDt = tabDI$fltVencimento
  settlementDt[1] = dt
  ratesTab = data.frame(date,rateAv,settlementDt)
  
  datesCntrct = ratesTab$settlementDt
  avgRates = ratesTab$rateAv
  datesCntrctStr = dateDictionary(datesCntrct)
  
  rateD0 = avgRates[1]
  
  #Copom Meetings
  copomDates = getCOPOMMeetingDates(objDB)
  currDate = dateDictionary(dt)
  
  nextMeetings = copomDates[which(copomDates>currDate)]
  
  #Anbima Calendar
  anbCal = ECOFIN.env$calSQLAnbimaSP
  
  #Excluding contracts expiring before any COPOM meeting
  #these rates should be very very similar to the overnight CDI rate.
  excludeCtr = which(datesCntrctStr<nextMeetings[1])
  excludeCtr = excludeCtr[-1]
  
  if (length(excludeCtr) > 0) {
    avgRates = avgRates[-excludeCtr]
    datesCntrct = datesCntrct[-excludeCtr]
    datesCntrctStr = datesCntrctStr[-excludeCtr]
  }
  
  # #Keeping the first maturity between COPOM dates only
  # ctrKeep = c()
  # ratesPos = c()
  # contractAfterMeeting = 1
  # for (k in 1:noMeetings) {
  #   currMeeting = nextMeetings[k]
  #   follMeeting = nextMeetings[k+1]
  #   datesFollCurrMeeting = datesCntrct[which(datesCntrctStr>=currMeeting)]
  #   datesBefFollMeeting = datesCntrct[which(datesCntrctStr<follMeeting)]
  #   interMeetings = intersect(datesFollCurrMeeting,datesBefFollMeeting)
  #   ratesPos = c(ratesPos,which(datesCntrctStr==dateDictionary(interMeetings[1])))
  #   ctNum = min(contractAfterMeeting,length(interMeetings))
  #   ctrKeep = c(ctrKeep,interMeetings[ctNum])
  # }
  # ctrKeep = dateDictionary(ctrKeep)
  # avgRates = avgRates[c(1,ratesPos)]
  # datesCntrctStr = datesCntrctStr[c(1,ratesPos)]
  
  #Calculating maturities in working days
  mtyRates = c()
  for (k in 1:length(avgRates)) {
    if (k==1) {
      currMty_wd = bizdays(datesCntrctStr[1],nextMeetings[1],anbCal)
      mtyRates = c(mtyRates,currMty_wd)
    } else {
      currMty_wd = bizdays(datesCntrctStr[1],datesCntrctStr[k],anbCal)
      mtyRates = c(mtyRates,currMty_wd)
    }
  }
  
  # #Calculating distance between meetings and PREVIOUS contract in working days
  # meetingRateDist = c()
  # for (k in 1:noMeetings) {
  #     currDist_wd = bizdays(datesCntrctStr[k],nextMeetings[k],anbCal)
  #     meetingRateDist = c(meetingRateDist,currDist_wd)
  # }
  
  # #Optimizing numerically
  # #Caso Geral
  # likelihood_BBG = function(x,avgRates,datesCntrctStr,meetings,cal) {
  #   
  #   wdAdj = 252
  #   d0 = datesCntrctStr[1]
  #   ctExON = datesCntrctStr[-1]
  #   
  #   #finding first contract after meeting
  #   ctPtMeet = ctExON[ctExON>meetings[1]][1]
  #   ctMeetPos = which(datesCntrctStr==ctPtMeet)
  #   ctMty = bizdays(d0,ctPtMeet,cal)
  #   onMty = bizdays(datesCntrctStr[1],meetings[1],cal)
  #   onPer = (1+avgRates[1])^(onMty/wdAdj)
  #   ptMeetPer = (1+x[1])^((ctMty-onMty)/wdAdj)
  #   effRate = onPer*ptMeetPer
  #   annRate = effRate^(wdAdj/ctMty)
  #   
  #   guess = effRate-1
  #   truth = ((1+avgRates[ctMeetPos])^(ctMty/252))-1
  #   
  #   # print(guess)
  #   # print(truth)
  #   penalty = abs(guess-truth)
  #   return(penalty)
  #   
  # } 
  # fwRates = optimize(likelihood_BBG,c(-1,1),
  #                    avgRates,datesCntrctStr,meetings,anbCal) #for univariate
  # copomDelta = (fwRates$minimum - avgRates[1])*10000 #in bps
  # #BBG Formula
  # wdAdj = 252
  # ctExON = datesCntrctStr[-1]
  # ctPtMeet = ctExON[ctExON>meetings[1]][1]
  # ctMeetPos = which(datesCntrctStr==ctPtMeet)
  # ctMty = bizdays(d0,ctPtMeet,anbCal)
  # onMty = bizdays(datesCntrctStr[1],meetings[1],anbCal)
  # onPer = (1+avgRates[1])^(onMty/wdAdj)
  # ctMeetPer = (1+avgRates[ctMeetPos])^(ctMty/wdAdj)
  # copom_p_cdi = (ctMeetPer/onPer)^(252/(ctMty-onMty))
  # copomDelta_bbg = ((copom_p_cdi-1) -avgRates[1])*10^4 #in bps
  
  meetings = nextMeetings[seq(1,noMeetings)]
  mktYields = avgRates
  mktDates = datesCntrctStr
  cal = anbCal

  source('Z:/ASSET/ECO/ECOFIN/Chartbook Juros (Marcelo)/Códigos/sourceAux/copomRecursive_BBG.R', echo=TRUE)
  source('Z:/ASSET/ECO/ECOFIN/Chartbook Juros (Marcelo)/Códigos/sourceAux/likelihood_monPol_2Period.R', echo=TRUE)
  
  copomList = copomRecursive_BBG(meetings,mktYields,mktDates,cal)
  # copomList = copomMultiPeriod(dtStr)
  
  # meetings = copomList$meetingVec
  # copomDelta = copomList$sol[1,]
  # noMeetings = length(meetings)
  # methodID = 2 
  # for (l in 1:noMeetings) {
  #   ECOFIN.env$insertCOPOMMarketData(objDB=ECOFIN.env$objConn,methodID,dtStr,l,
  #                                  adjust.next(meetings[l]-1,cal=ECOFIN.env$calSQLAnbima),
  #                                  meetings[l],copomDelta[l]/(10^4))
  # }
    
  return(copomList)

}

copomRecursive_BBG = function(meetings,mktYields,mktDates,cal) {
  
  #Replica precificação no estilo da BBG
  #Precisa passar todas as reuniões e yields
  
  copomImplied = c()
  copomDelta = c()
  d0 = mktDates[1]
  d0Rate = mktYields[1]
  for (k in 1:length(meetings)) {
    
    currMeeting = meetings[k]
    dtPreMeet = mktDates[1]
    if (k == 1) {
      currON = mktYields[k]
      synthPricingAnn = currON
      onAdj = 1
    } else {
      onAdj = 0
      #Achando contrato que expira mais próximo da reunião, conforme BBG
      prevMeeting = meetings[k-1]
      #contratos antes da reunião atual
      datesCurrMeeting = mktDates[which(mktDates<currMeeting)]
      #contratos após a reunião anterior
      datesPrevMeeting = mktDates[which(mktDates>prevMeeting)]
      #contratos entre reuniões
      interMeetings = dateDictionary(intersect(dateDictionary(datesPrevMeeting),
                                               dateDictionary(datesCurrMeeting)))
      
      #constrói um contrato sintético para colocar no arcabouço recursivo
      deltaDaysMeeting = bizdays(d0,meetings[1],cal)
      deltaDaysMeetingd0 = deltaDaysMeeting
      synthRate = (1+d0Rate)^(deltaDaysMeeting/252)
      d0Period = synthRate
      synthMty = deltaDaysMeeting
      # if (k>1) {
      for (j in 1:(k-1)) {
        meetingRate = copomImplied[j]
        deltaDaysMeeting = bizdays(meetings[j],meetings[j+1],cal)
        synthRate = synthRate*(1+meetingRate)^(deltaDaysMeeting/252)
        synthMty = synthMty + deltaDaysMeeting
        if (j < (k-1)) {
          d0Period = synthRate
          deltaDaysMeetingd0 = deltaDaysMeetingd0 + deltaDaysMeeting
        }
      }
      # }
      synthRateAnn = synthRate^(252/synthMty)
      
      #to do: verificando consistência de taxas entre reuniões com contratos
      # if (k > 1) {
      
      synthContractsAnn = c()
      synthContractsMty = c()
      prevMty = bizdays(d0,prevMeeting,cal)
      for (c in 1:length(interMeetings)) {
        interMty = bizdays(prevMeeting,interMeetings[c],cal)
        carryRate = copomImplied[length(copomImplied)]
        currSynth = d0Period*(1+carryRate)^(interMty/252)
        currMty = deltaDaysMeetingd0 + interMty
        synthContractsAnn = c(synthContractsAnn,currSynth^(252/currMty))
        synthContractsMty = c(synthContractsMty,currMty)
      }
      
      ratesInterMeetings = mktYields[match(interMeetings,mktDates)]
      pricingErrors = ((synthContractsAnn-1)-ratesInterMeetings)*100
      interMeetPos = which(pricingErrors==min(pricingErrors))
      
      # }
      
      #Contrato sintético recursivo
      prevMeetingRate = copomImplied[k-1]
      lastDtSynth = interMeetings[interMeetPos] #pegando o último contrato entre reuniões
      residMty = bizdays(lastDtSynth,meetings[k],cal)
      synthPrev = synthContractsAnn[length(synthContractsAnn)]-1 #pode haver mais de um contrato intermeeting
      synthCarryMty = synthContractsMty[length(synthContractsMty)]
      
      # synthPricing = synthPrev
      # synthPricingMty = synthCarryMty
      synthPricing = ((1+synthPrev)^(synthCarryMty/252))*((1+prevMeetingRate)^(residMty/252))
      synthPricingMty = synthCarryMty+residMty
      synthPricingAnn = synthPricing^(252/synthPricingMty)-1
      # synthPricingAnn = synthRateAnn-1
    }
    
    #primeiro contrato após reunião sendo precificada
    datesPricingMeeting = mktDates[which(mktDates>currMeeting)][1]
    pricingCtPos = which(mktDates==datesPricingMeeting)
    ratePricing = mktYields[pricingCtPos]
    
    # if (k > 1) {
    #   synthPricingAnn = synthRateAnn-1
    # }
    
    # if (k > 1) {
    #   synthPricingAnn = synthContractsAnn[length(synthContractsAnn)]-1
    # }
    #optimize function for univariate problems
    fwRates = optimize(likelihood_monPol_2Period,c(-1,1),
                       meetings[k],synthPricingAnn,ratePricing,d0,datesPricingMeeting,cal,
                       onAdj)
    
    if (k == 1) {
      copomDelta = c(copomDelta,(fwRates$minimum-d0Rate)*10^4)
    } else {
      copomDelta = c(copomDelta,(fwRates$minimum-copomImplied[k-1])*10^4)
    }
    
    copomImplied = c(copomImplied,fwRates$minimum)
    
  }
  
  return(list(copomImplied=copomImplied,copomDelta=copomDelta))
  
}

copomPricing_fn <- function(dt,nIter = 1000) {
  
cat("\014");

dt1                        <- as.Date(dt);
dt2                        <- as.Date(dt);
timeHorizon                <- 20*252;
dayCountConventionBasis    <- 252;
durationRateShock          <- 0.01;
numberOfMeetingsToConsider <- 65;
methodID                   <- 1;

#Connecting to th SQL Server
ECOFIN.env$objConn <- ECOFIN.env$connectSQLServer(dbServer = ECOFIN.env$dbServer, dbName = ECOFIN.env$dbName); 

#Getting DI1 data
dfSQLOutputDI1FUT               <- ECOFIN.env$getBMFDIData(objDB = ECOFIN.env$objConn, 
                                                           dt1 = ECOFIN.env$Date_yyyymmdd2xlDateNumeric(dt1), 
                                                           dt2 = ECOFIN.env$Date_yyyymmdd2xlDateNumeric(dt2));

dfSQLOutputDI1FUT$fltData       <- ECOFIN.env$xlDateNumeric2RDate_yyyymmdd(numDate = dfSQLOutputDI1FUT$fltData);
dfSQLOutputDI1FUT$fltVencimento <- ECOFIN.env$xlDateNumeric2RDate_yyyymmdd(numDate = dfSQLOutputDI1FUT$fltVencimento);

#Getting DI-Over data
dfSQLOutputDI1         <- ECOFIN.env$getCDICETIPData(objDB = ECOFIN.env$objConn, 
                                                     dt1 = ECOFIN.env$Date_yyyymmdd2xlDateNumeric(dt1), 
                                                     dt2 = ECOFIN.env$Date_yyyymmdd2xlDateNumeric(dt2));

dfSQLOutputDI1$fltData <- ECOFIN.env$xlDateNumeric2RDate_yyyymmdd(numDate = dfSQLOutputDI1$fltData)

#Getting COPOM Meeting Dates
copomMeetingDates <- ECOFIN.env$getCOPOMMeetingDates(objDB = ECOFIN.env$objConn);  

#Closing SQL server connection
  # odbcClose(ECOFIN.env$objConn)
  # rm(objConn, envir= ECOFIN.env )


#OPTIMIZATION PARAMETERS ##########################################################################
################################################################################################### 
model2Select=ECOFIN.env$expectedTermStructureGivenScenario
  
###################################################################################################  
#MAIN ROUTINE #####################################################################################
################################################################################################### 
curveDates<-unique(dfSQLOutputDI1FUT$fltData)
lsOutput<-list(dates=matrix(,nrow=0,ncol=1), OFvalue=matrix(,nrow=0,ncol=1),optTime=matrix(,nrow=0,ncol=1),mktExpectation=list())    

for (k in 1:length(curveDates)){
#Creating the vector of Bdates
  tmpSeqBDates<- bizseq(from=curveDates[k], to=ECOFIN.env$xlDateNumeric2RDate_yyyymmdd(ECOFIN.env$Date_yyyymmdd2xlDateNumeric(curveDates[k])+round(x=(timeHorizon/dayCountConventionBasis)*370,0))   ,cal=ECOFIN.env$calSQLAnbima);
    SeqBDatesM<- tmpSeqBDates[1:timeHorizon+1]; 
      rm(tmpSeqBDates); 
  
#Getting data for a specific date - BMF - DI1
  objTMPDI1 <- dfSQLOutputDI1FUT[dfSQLOutputDI1FUT$fltData==curveDates[k],1:dim(dfSQLOutputDI1FUT)[2]]
  
#Getting next meeting dates
  objTMPNextMeetingDates <- copomMeetingDates[copomMeetingDates>=curveDates[k]]
  objTMPNextMeetingDates <- objTMPNextMeetingDates[1:numberOfMeetingsToConsider]

#Eliminating if there is a contract that has number of business days less or equal than 1   
  objTMPDI1<-objTMPDI1[objTMPDI1$fltSaques>1,]
    
#Getting data for a specific date - CETIP - CDI
  objTMPCDI<-dfSQLOutputDI1[dfSQLOutputDI1$fltData==curveDates[k],1:size(dfSQLOutputDI1,2)]
  fltData<-objTMPCDI$fltData
  vchSerie<-"CDI"
  fltVencimento<-adjust.next(ECOFIN.env$xlDateNumeric2RDate_yyyymmdd(ECOFIN.env$Date_yyyymmdd2xlDateNumeric(objTMPCDI$fltData)+1),cal=ECOFIN.env$calSQLAnbima);
  fltSaques<-1
  fltOpenInterest<-0
  fltNumberTradesDay<-0
  fltNumberDealsDay<-0
  fltSettlementPrice<-100000/((1+objTMPCDI$fltTaxaMedia)^(1/252))
  fltSettlementRate<-objTMPCDI$fltTaxaMedia
  fltOpenRate<-0
  fltHighRate<-objTMPCDI$fltTaxaMaxima
  fltLowRate<-objTMPCDI$fltTaxaMinima
  fltLastRate<-0
  
  objTMPCDI<-data.frame(fltData,vchSerie,fltVencimento,fltSaques,fltOpenInterest,fltNumberTradesDay,fltNumberDealsDay,fltSettlementPrice,fltSettlementRate,fltOpenRate,fltHighRate,fltLowRate,fltLastRate)    
  objTMP<-rbind(objTMPCDI,objTMPDI1)
    rm(objTMPCDI,objTMPDI1, fltData,vchSerie,fltVencimento,fltSaques,fltOpenInterest,fltNumberTradesDay,fltNumberDealsDay,fltSettlementPrice,fltSettlementRate,fltOpenRate,fltHighRate,fltLowRate,fltLastRate)    
  
#BudgetBounds
  BudgetBounds<-list(min=-0.01*as.numeric(ones(nx =length(objTMPNextMeetingDates),ny=1)) ,
                     max=0.01*as.numeric(ones(nx =length(objTMPNextMeetingDates),ny=1)))

#DataList
  dataList<-list(yM=objTMP$fltSettlementRate,            #Settlement Rate    
                 mats=objTMP$fltSaques/252,              #Maturities
                 model=model2Select,                     #Model
                 effectiveDuration=ECOFIN.env$effDur(objTMP$fltSettlementPrice,objTMP$fltSettlementRate,objTMP$fltSaques/252,durationRateShock),
                 fltSettlementPrice=objTMP$fltSettlementPrice,
                 dtExpiry=ECOFIN.env$Date_yyyymmdd2xlDateNumeric(objTMP$fltVencimento),
                 fltNumberTradesDay=objTMP$fltNumberTradesDay,
                 fltNumberDealsDay=objTMP$fltNumberDealsDay,
                 SeqBDatesM=ECOFIN.env$Date_yyyymmdd2xlDateNumeric(SeqBDatesM),
                 dtNextMeetingDates=ECOFIN.env$Date_yyyymmdd2xlDateNumeric(objTMPNextMeetingDates),
                 dayCountConventionBasis=dayCountConventionBasis)
  
    
  #Defining the fitness ("objective-function")  Com penalty 
    f<-function(x,data=dataList) -ECOFIN.env$ExpectativaMercadoCopom_OF(x,data)
  
  #Optimizing      
    elpsTime<-system.time(sol<-ga(type="real-valued",fitness=f,popSize=5000, maxiter=nIter,min=BudgetBounds$min,max=BudgetBounds$max))
    intMethod=(as.matrix(seq(from=1, to=length(objTMPNextMeetingDates))) )

  #Connecting to th SQL Server
    ECOFIN.env$objConn<-ECOFIN.env$connectSQLServer(ECOFIN.env$dbServer,ECOFIN.env$dbName)  
    
  #Inserting results into the SQL server
    for (l in 1:length(sol@solution)){
      ECOFIN.env$insertCOPOMMarketData(objDB=ECOFIN.env$objConn,methodID,objTMP$fltData[1],intMethod[l],objTMPNextMeetingDates[l],
                                      adjust.next(objTMPNextMeetingDates[l]+1,cal=ECOFIN.env$calSQLAnbima),sol@solution[l])
      
    }
    
    #Closing SQL server connection
      # odbcClose(ECOFIN.env$objConn)
      # rm(objConn, envir= ECOFIN.env)

}

}

copomMultiPeriod = function(dt,noMeetings=8) {

  #Puxar dados BMF do SQL e determinar datas daí.
  source('Z:/ASSET/ECO/ECOFIN/Chartbook Juros (Marcelo)/Códigos/matFiles.R')
  source('Z:/ASSET/ECO/ECOFIN/Chartbook Juros (Marcelo)/Códigos/sourceAux/downloadMatDB.R', echo=TRUE)
  loadMatDB(0)
  
  # dt = 42780
  dtStr = dt
  dt = dateDictionary(dt)
  
  objDB = ECOFIN.env$objConn
  
  tabDI <- downloadCurveDI(dateDictionary(dt))
  
  date = tabDI$fltData
  rateAv = tabDI$fltSettlementRate
  settlementDt = tabDI$fltVencimento
  settlementDt[1] = dt
  ratesTab = data.frame(date,rateAv,settlementDt)
  
  datesCntrct = ratesTab$settlementDt
  avgRates = ratesTab$rateAv
  datesCntrctStr = dateDictionary(datesCntrct)
  
  rateD0 = avgRates[1]
  
  #Copom Meetings
  copomDates = getCOPOMMeetingDates(objDB)
  currDate = dateDictionary(dt)
  
  nextMeetings = copomDates[which(copomDates>currDate)]
  
  #Anbima Calendar
  anbCal = ECOFIN.env$calSQLAnbimaSP
  
  #Excluding contracts expiring before any COPOM meeting
  #these rates should be very very similar to the overnight CDI rate.
  excludeCtr = which(datesCntrctStr<nextMeetings[1])
  excludeCtr = excludeCtr[-1]
  
  if (length(excludeCtr) > 0) {
    avgRates = avgRates[-excludeCtr]
    datesCntrct = datesCntrct[-excludeCtr]
    datesCntrctStr = datesCntrctStr[-excludeCtr]
  }
  
  meetings = nextMeetings[seq(1,noMeetings)]
  mktDates = datesCntrctStr
  mktYields = avgRates
  
  mktDatesUnique = c.Date()
  for (k in 1:length(meetings)) {
    
    currMeeting = meetings[k]
    dateRateMeeting = mktDates[which(mktDates>currMeeting)][1]
    mktDatesUnique = c.Date(mktDatesUnique,dateRateMeeting)
    
  }
  
  mktYieldsUnique = mktYields[match(mktDatesUnique,mktDates)]*100
  d0Rate = rateD0*100
  d0 = dtStr
  cal = ECOFIN.env$calSQLAnbimaSP

  source('Z:/ASSET/ECO/ECOFIN/Chartbook Juros (Marcelo)/Códigos/sourceAux/likelihood_monPol_MultiPeriod.R', echo=TRUE)
  x = rep(0,noMeetings)
  # copomOpt = optim(x,likelihood_monPol_MultiPeriod,gr=NULL,meetings,mktYieldsUnique,mktDatesUnique,
                                                        # d0Rate,d0,cal)
  fitGA = function(x) -1*likelihood_monPol_MultiPeriod(x,meetings,mktYieldsUnique,mktDatesUnique,
                                                    d0Rate,d0,cal) 
  
  minVec = rep(-125,noMeetings)
  maxVec = rep(125,noMeetings)
  copomOpt = ga(type="real-valued",fitness=fitGA,min=minVec,max=maxVec,maxiter = 200)
  
  return(list(sol = copomOpt@solution,meetingVec = meetings))
    
}

#Caso Geral
likelihood_monPol_2Period = function(x,dtMeet,ratePreMeet,mktYield,dtPreMeet,dtPostMeet,cal,
                                     onAdj=0) {
  
  #onAdj controls for overnight correction of rates, used by BBG to price the first meeting
  
  wdAdj = 252
  d0 = dtPreMeet
  ctExON = dtPostMeet
  
  #finding first contract after meeting
  ctMty = bizdays(d0,dtPostMeet,cal)
  onMty = bizdays(d0,dtMeet,cal)
  
  #Finding forward after overnight
  fwON_effPeriod = ((mktYield+1)^(ctMty/wdAdj))/((ratePreMeet+1)^(onAdj/wdAdj))
  fwON_ann = (fwON_effPeriod^(wdAdj/(ctMty-onAdj))-1)
  
  onPer = (1+ratePreMeet)^(onMty/wdAdj)
  ptMeetPer = (1+x)^((ctMty-onMty)/wdAdj)
  effRate = onPer*ptMeetPer
  annRate = effRate^(wdAdj/ctMty)
  
  guess = effRate-1
  truth = ((1+fwON_ann))^(ctMty/wdAdj)-1

  # Não muda resultadoo; transformações monotônicas da função objetivo.
  # guess = effRate^(wdAdj/ctMty)
  # truth = ((1+fwON_ann))
  
  penalty = crossprod(guess-truth)
  return(penalty)
  
}

#Caso Geral
likelihood_monPol_MultiPeriod = function(x,meetings,mktYieldsUnique,mktDatesUnique,d0Rate,d0,cal) {
  
  onRate = d0Rate
  onPath = c()
  d0Mty = bizdays(d0,meetings[1],cal)
  onRateFac = (1+onRate/100)^(d0Mty/252)
  onRateAnn = (onRateFac^(252/d0Mty)-1)*100
  onRate = onRateAnn
  onPathFac = c()
  onActualPath = c()
  onRateFacActualMtyAnn = c()
  onActualPathAnn = c()
  onRateFacActualMty = c()
  onMtyPath = c()
  onRateAnnPath = c()
  for (j in 1:(length(meetings)-1)) {
  
    onRateAnn = onRateAnn+x[j]/100 #x in bps
    onMty = bizdays(meetings[j],meetings[j+1],cal)
    onMtyPath = c(onMtyPath,onMty)
    onRateFac = (1+onRateAnn/100)^(onMty/252)
    onPathFac = c(onPathFac,onRateFac)
    onRateAnnPath = c(onRateAnnPath,onRateAnn)
    
    actualMty = bizdays(d0,mktDatesUnique[j],cal)
    interMty = bizdays(meetings[j],mktDatesUnique[j],cal)
    onActualPath = c(onActualPath,(1+mktYieldsUnique[j]/100)^(actualMty/252))
    if (j == 1) {
      onRateFacActualMty = ((1+d0Rate/100)^(d0Mty/252))*((1+onRateAnn/100)^(interMty/252))
    } else {
      onRateFacActualMty = c(onRateFacActualMty,
                             ((1+d0Rate/100)^(d0Mty/252))*prod(onPathFac[-length(onPathFac)])*((1+onRateAnn/100)^(interMty/252)))
    }
    onRateFacActualMtyAnn = c(onRateFacActualMtyAnn,onRateFacActualMty[j]^(252/actualMty))
    onActualPathAnn = c(onActualPathAnn,onActualPath[j]^(252/actualMty))
    
    onRateAnn = (onRateFacActualMtyAnn[j]-1)*100
    
  }
  
  guess = onRateFacActualMty
  truth = onActualPath

  penalty = crossprod(guess-truth)
  return(penalty)
  
}
#Estimates HLW

nParaMod = 8
nParaSD = 6
nPara = nParaMod+nParaSD #6 standard deviations

de_param <- list(min	= c(rep(-2,nParaMod),rep(0,nParaSD)),
                 max	= c(rep(2,nParaMod),rep(100,nParaSD)))

para_0 = rep(0.5,nPara)

#Regular Optimizaton
sol_reg <- optim(para_0,fn=hlw)

#Genetic Optimization
library("GA")
sol <- ga(type = "real-valued", 
             fitness = hlw, 
             popSize = 100, 
             maxiter = 2,
             min = de_param$min, max = de_param$max
          )

library("mFilter")
library("Haver")
library("tempdisagg")

gdp = as.zoo(haver.data('EMERGELA:N223NGPI',rtype='zoo'))
gdp_yoy = delPC(gdp,4,dPerc=1)
      
selic = as.zoo(haver.data('emergela:N223RD',rtype='zoo'))
inf_exp = as.zoo(haver.data('emergela:N223VCP1',rtype='zoo'))

dfRates = merge(selic,inf_exp)
real_exAnte = as.zoo(squareData(dfRates[,1]-dfRates[,2]))

selic_cfFilter = cffilter(selic,type="asymmetric")
selic_hpFilter = hpfilter(selic,type="lambda",freq=129600)
real_hpFilter = hpfilter(real_exAnte,type="lambda",freq=129600)
  
real_trend_hp = as.zoo(real_hpFilter$trend,order.by=index(real_exAnte))
real_gap = real_exAnte - real_trend_hp
barplot(real_gap)

selic_trend_hp = as.zoo(selic_hpFilter$trend,order.by=index(selic))
df = merge(selic_trend_hp,inf_exp)
real_deflated_hp = as.zoo(squareData(df[,1]-df[,2]))

selic_trend_cf = as.zoo(selic_cfFilter$trend,order.by=index(selic))
df = merge(selic_trend_cf,inf_exp)
real_deflated_cf = as.zoo(squareData(df[,1]-df[,2]))

#Neutral Rate Decomposition
prodGth = as.ts(haver.data('emergela:h223elpi',rtype='ts'))
prodGth_m = td(prodGth ~ 1, method = "denton-cholette", conversion = "average",to="monthly")
prodGth_m = as.zoo(prodGth_m$values,order.by=seq(as.Date("2002/4/1"),as.Date("2012/12/1"),
                                                 "months"))

#Disaggregating annual data to monthly
infTgt = zoo(haver.data('emergela:a223vmcp',rtype='zoo'),order.by=seq(1999,2018),freq=1)
infTgt.na =  merge(as.zoo(infTgt),
                           + zoo(, seq(start(infTgt)[1], end(infTgt)[1], 1/12)))
infTgt_mon = na.locf(infTgt.na)
infTgt_mon = zoo(infTgt_mon,order.by=as.yearmon(index(infTgt_mon)))
ipca = as.zoo(haver.data('emergela:c223pca',rtype='zoo'))
ipca_yoy = delPC(ipca,12,dPerc = 1)
ipca_yoy = zoo(ipca_yoy,order.by=as.yearmon(index(ipca_yoy)))

df = merge(ipca_yoy,infTgt_mon)
infGap = df[,1] - df[,2]

inf_exp = zoo(inf_exp,order.by=as.yearmon(index(inf_exp)))
df = merge(lag(inf_exp,-12),ipca_yoy)
infSurprise = df[,1] - df[,2]

#Debt-to-GDP Ratio
dbt = as.zoo(haver.data('emergela:c223gdtp',rtype='zoo'))

#Credit-to-GDP Ratio
credit = as.zoo(haver.data('emergela:n223fct',rtype='zoo'))
nominal_gdp = as.zoo(haver.data('emergela:c223gpm',rtype='zoo'))
nominal_gdp_12m = rollapply(nominal_gdp,12,sum,align="right")
df = merge(credit,nominal_gdp_12m)
credit_gdp = df[,1]/df[,2]

#Global Interest Rates
tsy_3m = as.zoo(haver.data('usecon:fcm3me',rtype='zoo'))
tsy_10y = as.zoo(haver.data('usecon:fcm10e',rtype='zoo'))

dfReg = as.data.frame(squareData(merge(real_exAnte,#nfGap,infSurprise)))
                                       dbt,credit_gdp,tsy_3m,tsy_10y)))

fundReg = lm(real_exAnte ~ 1 + .,data=dfReg)
fitted = predict(fundReg,type="response")

regDecomp <- function(lmObj) {
  designMat = as.matrix(data.frame(1,lmObj$model[,-1]))
  print(colnames(designMat))
  colnames(designMat)[1] = "constant"
  coeffs = lmObj$coefficients
  fittedValue = designMat%*%coeffs
  components = designMat%*%diag(coeffs)
  colnames(components) = colnames(designMat)
  fittedDecomp = data.frame(fittedValue,components)
  return(fittedDecomp)
}
exportDecomp = regDecomp(fundReg)
write.csv(exportDecomp, "C:/Users/teixjar/Desktop/compsTeste.csv")

#td package offers more convoluted disaggregation methods
infTgt = as.ts(haver.data('emergela:a223vmcp',rtype='ts'))
infTgt = td(infTgt ~ 1,conversion='first',to='monthly',method='ols')

hlw <- function(parameters) {

#Holston-Laubach-Williams State Space Model

library("FKF")
library("Ryacas")

#Data
ibc_br = as.zoo(haver.data('EMERGELA:N223GVI',rtype='zoo'))
ibc_br_yoy = delPC(ibc_br,lagDel=12)
y = ibc_br_yoy

ipca = as.zoo(haver.data('emergela:c223pca',rtype='zoo'))
ipca_yoy = delPC(ipca,12)
pi = ipca_yoy

selic = as.zoo(haver.data('emergela:N223RD',rtype='zoo'))
inf_exp = as.zoo(haver.data('emergela:N223VCP1',rtype='zoo'))
dfRates = merge(selic,inf_exp)
real_exAnte = as.zoo(squareData(dfRates[,1]-dfRates[,2]))
r = real_exAnte

i_nom = selic

#Observables
zooObs = merge(y,pi,r,i_nom)
yt = t(as.matrix(zooObs[,1:4]))

#Model Equations
stateVec <- expression(y,pi,y_l1,y_l2,pi_l1,
                       pi_l2,r_star, y_star, r_tilde, y_tilde,
                       g, z,r_star_l1, r_star_l2, y_star_l1,
                       y_star_l2,r_tilde_l1, r_tilde_l2,r_tilde_ld,y_tilde_l1,
                       y_tilde_l2,r,i_nom)

nState = 23

b_y = parameters[1]
phi_pi = parameters[2]
phi_y = parameters[3]
b_pi = parameters[4]
ar = parameters[5]
ay1 = parameters[6]
ay2 = parameters[7]
# taylorCons = parameters[8]
lastPara = 7

#Contemporaneous Matrix
eq1_lhs = c(1,0,0,0,0,0,0,-1,0,-1/100,0,0,0,0,0,0,0,0,0,0,0,0,0)
eq2_lhs = c(0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
eq3_lhs = c(0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
eq4_lhs = c(0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
eq5_lhs = c(0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)

eq6_lhs = c(0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
eq7_lhs = c(0,0,0,0,0,0,1,0,0,0,-1,-1,0,0,0,0,0,0,0,0,0,0,0)
eq8_lhs = c(0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
eq9_lhs = c(0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,-1,0)
eq10_lhs = c(0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0)

eq11_lhs = c(0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0)
eq12_lhs = c(0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0)
eq13_lhs = c(0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0)
eq14_lhs = c(0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0)
eq15_lhs = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0)

eq16_lhs = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0)
eq17_lhs = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0)
eq18_lhs = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0)
eq19_lhs = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-1,-1,1,0,0,0,0)
eq20_lhs = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0)
eq21_lhs = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0)
eq22_lhs = c(0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,-1)
eq23_lhs = c(0,-phi_pi,0,0,0,0,0,0,0,-phi_y,0,0,0,0,0,0,0,0,0,0,0,0,1)

removeEq = c(22,23,17,16)
nStateLiq = nState-length(removeEq)

TT_toInv = c()
eqsLoop = seq(1,nState)[is.na(pmatch(seq(1,nState),removeEq))]
for (i in eqsLoop) {
  # print(i)
  # exprStr = parse(text=paste("print(length(eq",i,"_lhs))",sep=""))
  # eval(exprStr)
  if (length(removeEq) > 0) {
    for (j in removeEq) {
    exprStr = parse(text=paste("eq",i,"_lhs = ","eq",i,"_lhs[-",j,"]",sep=""))
    eval(exprStr)
    }
  }
  exprStr = parse(text=paste("TT_toInv = c(TT_toInv,eq",i,"_lhs)",sep=""))
  eval(exprStr)
}


TT_toInv = matrix(TT_toInv,nStateLiq,nStateLiq)
TT_aux = TT_toInv
TT_toInv = solve(TT_toInv)

#Lagged Matrix
eq1_rhs = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
eq2_rhs = c(0,0,0,0,b_pi,(1-b_pi),0,0,0,b_y,0,0,0,0,0,0,0,0,0,0,0,0,0)
eq3_rhs = c(1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
eq4_rhs = c(0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
eq5_rhs = c(0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)

eq6_rhs = c(0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
eq7_rhs = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
eq8_rhs = c(0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0)
eq9_rhs = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
eq10_rhs = c(0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,ar/2,ay1,ay2,0,0)

eq11_rhs = c(0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0)
eq12_rhs = c(0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0)
eq13_rhs = c(0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
eq14_rhs = c(0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0)
eq15_rhs = c(0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)

eq16_rhs = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0)
eq17_rhs = c(0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
eq18_rhs = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0)
eq19_rhs = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
eq20_rhs = c(0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0)

eq21_rhs = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0)
eq22_rhs = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
eq23_rhs = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)

TT_rhs = c()
for (i in 1:nState) {
  # print(i)
  # exprStr = parse(text=paste("print(length(eq",i,"_rhs))",sep=""))
  # eval(exprStr)
  if (length(removeEq) > 0) {
    for (j in removeEq)
      exprStr = parse(text=paste("eq",i,"_rhs = ","eq",i,"_rhs[-",j,"]",sep=""))
    eval(exprStr)
  }
  exprStr = parse(text=paste("TT_rhs = c(TT_rhs,eq",i,"_rhs)",sep=""))
  eval(exprStr)
}
TT_rhs = matrix(TT_rhs,nStateLiq,nStateLiq)

Tt = TT_toInv%*%TT_rhs

obsVarInds = c(1,2,22,23) #in State equation

obsVarInds = obsVarInds[-which(obsVarInds==removeEq)]
nObsVar = length(obsVarInds)
Zt = matrix(0,nObsVar,nStateLiq)
for (i in 1:length(obsVarInds)) {
  Zt[i,obsVarInds[i]] = 1
}

ct = matrix(0,nObsVar,1)
dt = matrix(0,nStateLiq,1)
#dt[23] = taylorCons

#Shock Matrices
Gt = matrix(0,nObsVar,nObsVar) #observation equations are all identities; all structure as state

Ht = matrix(0,nStateLiq,nStateLiq)
shockEq = c(10,2,8,11,12,23)

shockEq = shockEq[-which(shockEq==removeEq)]
for (i in 1:length(shockEq)) {
  exprStr = parse(text=paste("sd",i," = parameters[",i+lastPara,"]",sep=""))
  eval(exprStr)
  exprStr = parse(text=paste("Ht[shockEq[i],shockEq[i]] = sd",i,sep=""))
  eval(exprStr)
}

#State Equation Steady State
state_ss = rep(0,nStateLiq)
#state_ss = solve(diag(nState)-Tt)%*%dt

#Initial Values
a0 = state_ss
P0 = diag(nState)

GGt = Gt%*%t(Gt)
HHt = Ht%*%t(Ht)
kf_obj = fkf(a0,P0,dt,ct,Tt,Zt,HHt,GGt,yt)

return(kf_obj$logLik)

}

hlw <- function(parameters) {

#Holston-Laubach-Williams State Space Model

library("FKF")
library("Ryacas")

#Data
ibc_br = as.zoo(haver.data('EMERGELA:N223GVI',rtype='zoo'))
ibc_br_yoy = delPC(ibc_br,lagDel=12)
y = ibc_br_yoy

ipca = as.zoo(haver.data('emergela:c223pca',rtype='zoo'))
pi = ipca_yoy

selic = as.zoo(haver.data('emergela:N223RD',rtype='zoo'))
inf_exp = as.zoo(haver.data('emergela:N223VCP1',rtype='zoo'))
dfRates = merge(selic,inf_exp)
real_exAnte = as.zoo(squareData(dfRates[,1]-dfRates[,2]))
r = real_exAnte

i_nom = selic

#Observables
zooObs = merge(y,pi,r,i_nom)
yt = t(as.matrix(zooObs[,1:4]))

#Model Equations
stateVec <- expression(y,pi,y_l1,y_l2,pi_l1,
                       pi_l2,r_star, y_star, r_tilde, y_tilde,
                       g, z,r_star_l1, r_star_l2, y_star_l1,
                       y_star_l2,r_tilde_l1, r_tilde_l2,r_tilde_ld,y_tilde_l1,
                       y_tilde_l2,r,i_nom)

nState = 23
dt = c(0,0,0,0,0,0,0)

b_y = parameters[1]
phi_pi = parameters[2]
phi_y = parameters[3]
b_pi = parameters[4]
ar = parameters[5]
ay1 = parameters[6]
ay2 = parameters[7]
lastPara = 7

#Contemporaneous Matrix
eq1_lhs = c(1,0,0,0,0,0,0,-1,0,-1/100,0,0,0,0,0,0,0,0,0,0,0,0,0)
eq2_lhs = c(0,1,0,0,0,0,0,0,0,-b_y,0,0,0,0,0,0,0,0,0,0,0,0,0)
eq3_lhs = c(0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
eq4_lhs = c(0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
eq5_lhs = c(0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)

eq6_lhs = c(0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
eq7_lhs = c(0,0,0,0,0,0,1,0,0,0,-1,-1,0,0,0,0,0,0,0,0,0,0,0)
eq8_lhs = c(0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
eq9_lhs = c(0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,-1,0)
eq10_lhs = c(0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0)

eq11_lhs = c(0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0)
eq12_lhs = c(0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0)
eq13_lhs = c(0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0)
eq14_lhs = c(0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0)
eq15_lhs = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0)

eq16_lhs = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0)
eq17_lhs = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0)
eq18_lhs = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0)
eq19_lhs = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-1,-1,1,0,0,0,0)
eq20_lhs = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0)
eq21_lhs = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0)
eq22_lhs = c(0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,-1)
eq23_lhs = c(0,-phi_pi,0,0,0,0,0,0,0,-phi_y,0,0,0,0,0,0,0,0,0,0,0,0,1)

TT_toInv = c()
for (i in 1:23) {
  # print(i)
  # exprStr = parse(text=paste("print(length(eq",i,"_lhs))",sep=""))
  # eval(exprStr)
  exprStr = parse(text=paste("TT_toInv = c(TT_toInv,eq",i,"_lhs)",sep=""))
  eval(exprStr)
}
TT_toInv = matrix(TT_toInv,nState,nState)
TT_aux = TT_toInv
TT_toInv = solve(TT_toInv)

#Lagged Matrix
eq1_rhs = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
eq2_rhs = c(0,0,0,0,b_pi,(1-b_pi),0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
eq3_rhs = c(1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
eq4_rhs = c(0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
eq5_rhs = c(0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)

eq6_rhs = c(0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
eq7_rhs = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
eq8_rhs = c(0,0,0,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0)
eq9_rhs = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
eq10_rhs = c(0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,ar/2,ay1,ay2,0,0)

eq11_rhs = c(0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0)
eq12_rhs = c(0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0)
eq13_rhs = c(0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
eq14_rhs = c(0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0)
eq15_rhs = c(0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)

eq16_rhs = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0)
eq17_rhs = c(0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
eq18_rhs = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0)
eq19_rhs = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
eq20_rhs = c(0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0)

eq21_rhs = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0)
eq22_rhs = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
eq23_rhs = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)

TT_rhs = c()
for (i in 1:23) {
  # print(i)
  # exprStr = parse(text=paste("print(length(eq",i,"_rhs))",sep=""))
  # eval(exprStr)
  exprStr = parse(text=paste("TT_rhs = c(TT_rhs,eq",i,"_rhs)",sep=""))
  eval(exprStr)
}
TT_rhs = matrix(TT_rhs,nState,nState)

Tt = TT_toInv%*%TT_rhs

obsVarInds = c(1,2,22,23) #in State equation

nObsVar = length(obsVarInds)
Zt = matrix(0,nObsVar,nState)
for (i in 1:length(obsVarInds)) {
  Zt[i,obsVarInds[i]] = 1
}

ct = matrix(0,nObsVar,1)
dt = matrix(0,nState,1)

#Shock Matrices
Gt = matrix(0,nObsVar,nObsVar) #observation equations are all identities; all structure as state

Ht = matrix(0,nState,nState)
shockEq = c(10,2,8,11,12,23)
for (i in 1:length(shockEq)) {
  exprStr = parse(text=paste("sd",i," = parameters[",i+lastPara,"]",sep=""))
  eval(exprStr)
  exprStr = parse(text=paste("Ht[shockEq[i],shockEq[i]] = sd",i,sep=""))
  eval(exprStr)
}

#State Equation Steady State
state_ss = rep(0,nState)
#state_ss = solve(diag(nState)-Tt)%*%dt

#Initial Values
a0 = state_ss
P0 = diag(nState)

GGt = Gt%*%t(Gt)
HHt = Ht%*%t(Ht)
kf_obj = fkf(a0,P0,dt,ct,Tt,Zt,HHt,GGt,yt)

return(kf_obj$logLik)

}

aggMonthly <- function(zooObj) {
  
  #Aggregating to monthly frequency.
  
  #ATENÇÃO: ESTA FUNCAO NAO ESTA MUITO GERAL.
  #APENAS AGREGA NA FORMA LAST.
  #MAS ISSO É FÁCIL DE ALTERAR, O BORAS SABE.
  datesSample = index(zooObj)
  names = colnames(zooObj)
  aggT_ind <- as.numeric(format(time(zooObj), "%Y%m"))
  #aggregate(time-series,indice_de_agregacao,funcao_de_agregacao)
  aggData <- as.data.frame(aggregate(zooObj, aggT_ind, tail,1))
  aggDates = seq.Date(from = as.Date(as.yearmon(datesSample[1])),
                      to = as.Date(as.yearmon(tail(datesSample,1))),by="1 month")
  zooAgg = zoo(aggData,order.by = aggDates)
  
  return(zooAgg)
  
}

#Fixed Maturity using Estimated NSS

nGrid = 100
betasSurp = c()
mtyGrid = seq(from=10^-7,to=10,length=nGrid)

count=1

for (indMty in mtyGrid) {

mty = indMty

library(xlsx)
library(lubridate)

datesRel = read.xlsx("Z:\\ASSET\\ECO\\ECOFIN\\Chartbook Juros (Marcelo)\\MacroSurpresas\\divulgMacro.xlsx",
                     sheetIndex = 1, colClasses = c("Date","Date","Date")) 

#IPCA 30 Medians and Actuals
ipca = importEviews("Z:\\ASSET\\ECO\\ECOFIN\\Chartbook Juros (Marcelo)\\Eviews\\macrosurprises.csv")
ipca = squareData(ipca)

surprise = ipca[,1] - ipca[,2]
relTime = datesRel$IPCA.30
relTime = relTime[complete.cases(relTime)]

calendar = ECOFIN.env$calSQLAnbimaSP

refMonthSer = c.Date()
deltaRate = c()
surpriseSer = c()

datesInd = 188:204

for (j in datesInd) { #índices das datas cuja curva foi estimada
  
  refMonth = datesRel$month_ref[j]
  currRel = relTime[j]
  preRel = add.bizdays(currRel,-1,calendar)
  if (is.na(preRel)) { #erro porque tem um release do IPCA em feriado da ANBIMA, por ex no dia 2003-07-09; usamos a data do dia seguinte...
    preRel = currRel - 1
    currRel = currRel + 1
    #checando que o currRel novo é bizday da ANBIMA
    while (is.bizday(currRel,calendar)==FALSE) {
      currRel = currRel+1
    }
  }
  currSurprise = surprise[refMonth]
  
  if (length(currSurprise) == 0) { #alguma data potencial sem expectativas; principalmente no histórico...
    currSurprise = NA
  }
  
  currFittedDI = getNSSPoint(currRel,mty)
  preFittedDI = getNSSPoint(preRel,mty)
  deltaRate = c(deltaRate,(preFittedDI - currFittedDI)*10^4) #in bps

  surpriseSer = c(surpriseSer,currSurprise*100) #in bps
  refMonthSer = c.Date(refMonthSer,refMonth)
  
}

#controles
us_10y = as.zoo(haver.data('daily:FCM10',rtype='zoo'))
us_10y_d = delPC(us_10y,dLevel = 1)*100 #in bps
snp = as.zoo(haver.data('daily:sp100',rtype='zoo'))
snp_d = delPC(snp,dPerc=1)*100 #in bps
vix = as.zoo(haver.data('daily:spvix',rtype='zoo'))
vix_d = delPC(vix,dPerc=1)*100 #in bps

controls = merge(us_10y_d,snp_d,vix_d)

refMonthSer = c.Date()
dfControls = data.frame()
'Obtendo controles nas datas de release'
for (j in datesInd) {
  
  refMonth = datesRel$month_ref[j]
  currRel = relTime[j]
  
  timeIndControls = which(index(controls)==currRel)
  currControls = controls[timeIndControls,]
  
  dfControls = rbind(dfControls,as.data.frame(currControls))
  refMonthSer = c.Date(refMonthSer,refMonth)
  
}
rownames(dfControls) <- refMonthSer

dfReg = cbind(data.frame(deltaRate,surpriseSer,row.names = refMonthSer),dfControls)
dfReg = dfReg[complete.cases(dfReg),]

#restringindo a amostra
# dfReg = dfReg[seq(30,nrow(dfReg)),]

surpReg = lm(deltaRate ~ 0 + surpriseSer + fcm10 + sp100 + spvix,data=dfReg)

currbetaSurp = surpReg$coefficients[[1]]
betasSurp = c(betasSurp,currbetaSurp)

print(count/nGrid)
count=count+1

}

#Obtaining Fitted Curve with Known Surprise Ex-Ante

testDate = '2017-06-08' #pre surprise
fittedCurvePre = getCurvesPoints(testDate,gridNSS_eval = mtyGrid)

macroSurprise = -16 #in bps
#o sinal de menos é pela convenção como eu defini o deltaRate, sendo antes MENOS depois
fittedPostCurve = ((fittedCurvePre$numericalNSS*10^4) - betasSurp*macroSurprise)/10^4

actualCurve =  getCurvesPoints('2017-06-09',gridNSS_eval = mtyGrid)

curvesExp = data.frame(fittedCurvePre$numericalNSS_mty,fittedCurvePre$numericalNSS,
                       actualCurve$numericalNSS_mty,actualCurve$numericalNSS,
                       fittedCurvePre$numericalNSS_mty,fittedPostCurve)
write.xlsx(curvesExp,"C:\\Users\\teixjar\\Desktop\\fittedCurve.xlsx")
curvesActualExp = data.frame(actualCurve$actualMty,actualCurve$actualRates)
write.xlsx(curvesActualExp,"C:\\Users\\teixjar\\Desktop\\actualCurve.xlsx")

betasExp = data.frame(mtyGrid,betasSurp)
write.xlsx(betasExp,"C:\\Users\\teixjar\\Desktop\\betasMty.xlsx")

#Obtaining Fitted Value for Specific Contract
curvePreDate = downloadCurveDI(testDate)
chosenContract = which(curvePreDate[,1]=="DI1F19")
contractPricing = curvePreDate[chosenContract,]
contractPricingMty = contractPricing$fltSaques/252
#Estimating Beta for this Maturity
betaEst = betaMacroSurprise(contractPricingMty)
#Effect on given contract
fittedPreContract = contractPricing$fltSettlementRate
fittedPostContract = ((fittedPreContract*10^4) - betaEst*macroSurprise)/10^4


##########

# Actual Curve - to evaluate fitted macro effects
date = '2017-06-09'
preDate = '2017-06-08'
fittedCurveDate = getCurvesPoints(date,nGrid=500)
fittedCurvePreDate = getCurvesPoints(preDate,nGrid=500)
curvesFittedExp = data.frame(fittedCurveDate$numericalNSS_mty,fittedCurveDate$numericalNSS,
                             fittedCurvePreDate$numericalNSS_mty,fittedCurvePreDate$numericalNSS)
write.xlsx(curvesFittedExp,"C:\\Users\\teixjar\\Desktop\\fittedCurve.xlsx")
curvesActualExp = data.frame(fittedCurveDate$actualMty,fittedCurveDate$actualRates,
                             fittedCurvePreDate$actualMty,fittedCurvePreDate$actualRates)
write.xlsx(curvesActualExp,"C:\\Users\\teixjar\\Desktop\\actualCurve.xlsx")

library(xlsx)
library(lubridate)

datesRel = read.xlsx("Z:\\ASSET\\ECO\\ECOFIN\\Chartbook Juros (Marcelo)\\MacroSurpresas\\divulgMacro.xlsx",
                     sheetIndex = 1, colClasses = c("Date","Date","Date")) 

#IPCA 30 Medians and Actuals
ipca = importEviews("Z:\\ASSET\\ECO\\ECOFIN\\Chartbook Juros (Marcelo)\\Eviews\\macrosurprises.csv")
ipca = squareData(ipca)

surprise = ipca[,1] - ipca[,2]
relTime = datesRel$IPCA.30
relTime = relTime[complete.cases(relTime)]

calendar = ECOFIN.env$calSQLAnbimaSP

refMonthSer = c.Date()
deltaRate = c()
surpriseSer = c()

for (j in 1:(length(datesRel$month_ref)-1)) { #-1 para não pegar releases que não ocorreram ainda
  
  refMonth = datesRel$month_ref[j]
  currRel = relTime[j]
  preRel = add.bizdays(currRel,-1,calendar)
  if (is.na(preRel)) { #erro porque tem um release do IPCA em feriado da ANBIMA, por ex no dia 2003-07-09; usamos a data do dia seguinte...
    preRel = currRel - 1
    currRel = currRel + 1
    #checando que o currRel novo é bizday da ANBIMA
    while (is.bizday(currRel,calendar)==FALSE) {
      currRel = currRel+1
    }
  }
  currSurprise = surprise[refMonth]
  
  if (length(currSurprise) == 0) { #alguma data potencial sem expectativas; principalmente no histórico...
    currSurprise = NA
  }
  
  #baixa curva um dia antes e no dia do release: o release do IPCA é 9:00, então a rigor deveriamos pegar preço de abertura
  diPrevCv =  downloadCurveDI(preRel)
  diPostCv = downloadCurveDI(currRel)
  
  #surpresa no três meses
  ctrctInd = 4
  ctrctName = diPrevCv$vchSerie[[ctrctInd]]
  ratePre = diPrevCv$fltSettlementRate[[ctrctInd]]
  
  ctrctInd_post = which(diPostCv$vchSerie==ctrctName)#só pra garantir que pega o mesmo contrato, o índice muito provavelmente será o mesmo
  ratePost = diPostCv$fltSettlementRate[[ctrctInd_post]]
  
  deltaRate = c(deltaRate,(ratePre - ratePost)*10^4) #in bps
  surpriseSer = c(surpriseSer,currSurprise*100) #in bps
  refMonthSer = c.Date(refMonthSer,refMonth)

}

#controles
source('Z:/ASSET/ECO/LATAM/MX/Codes/R/delPC.R', echo=TRUE)
us_10y = as.zoo(haver.data('daily:FCM10',rtype='zoo'))
us_10y_d = delPC(us_10y,dLevel = 1)*100 #in bps
snp = as.zoo(haver.data('daily:sp100',rtype='zoo'))
snp_d = delPC(snp,dPerc=1)*100 #in bps
vix = as.zoo(haver.data('daily:spvix',rtype='zoo'))
vix_d = delPC(vix,dPerc=1)*100 #in bps

controls = merge(us_10y_d,snp_d,vix_d)

refMonthSer = c.Date()
dfControls = data.frame()
'Obtendo controles nas datas de release'
for (j in 1:(length(datesRel$month_ref)-1)) {
 
  refMonth = datesRel$month_ref[j]
  currRel = relTime[j]
  
  timeIndControls = which(index(controls)==currRel)
  currControls = controls[timeIndControls,]
  
  dfControls = rbind(dfControls,as.data.frame(currControls))
  refMonthSer = c.Date(refMonthSer,refMonth)
  
}
rownames(dfControls) <- refMonthSer
  
dfReg = cbind(data.frame(deltaRate,surpriseSer,row.names = refMonthSer),dfControls)
dfReg = dfReg[complete.cases(dfReg),]

#restringindo a amostra
dfReg = dfReg[seq(30,nrow(dfReg)),]

surpReg = lm(deltaRate ~ 0 + surpriseSer + fcm10 + sp100 + spvix,data=dfReg)

betaSurp = surpReg$coefficients[[1]]


riskPremiaACM <- function(An_RA,Bn_RA,An_RF,Bn_RF,mtyInd,factorsPr) {
  
  #Calculates risk-premium using ACM model.
  
  Pn_RA = priceBondSDF(An_RA,Bn_RA,factorsPr)
  Pn_RF = priceBondSDF(An_RF,Bn_RF,factorsPr)
  ra = priceToRate(Pn_RA,mtyInd)
  re = priceToRate(Pn_RF,mtyInd)
  
  riskPremium = (ra-re)*10^4 #in bps
  return(riskPremium)
  
}

priceBondSDF <- function(An,Bn,factorsPr) {
  
  #Prices bonds according to exponential affine representation.
  sampleT = NROW(factorsPr)
  lnpn = matrix(An,sampleT,1)+factorsPr%*%Bn
  Pn = exp(lnpn)
  
  return(Pn)
  
}
riskSeriesACM <- function(lambda0,lambda1,factors,prInnov) {
  
  #Calculates series for market prices of risk and stochastic discount factor.
  
  #Not needed for pricing but relevant objects.
  
  #Market Price of Risk
  library(expm)
  lambdaMP = cbind()
  sigmaH2 = inv(sqrtm(sigma))
  
  for (t in 1:NROW(factors)) {
    lambdaMP = cbind(lambdaMP,sigmaH2%*%(lambda0+lambda1%*%factors[t,]))
  }
  
  #Stochastic Discount Factor
  sdf = c()
  vm = t(prInnov)
  for (t in 1:NCOL(lambdaMP)) {
    sdf = c(sdf,exp(-rf[t]*0.5*t(lambdaMP[,t])%*%lambdaMP[,t]-t(lambdaMP[,t])%*%sigmaH2%*%vm[,t]))
  }

}

priceToRate <- function(price,mtyInd,faceValue=1) {
  
  #Obtains discrete compounding rate from ACM model-implied price
  cRate = (1/mtyInd)*(-log(price/faceValue))
  dRate = exp(cRate)-1
  dRateAnn = (1+dRate)^12
  return(dRateAnn-1)
  
}

panelMaturities <- function(mtyGrid,dates,cal=ECOFIN.env$calSQLAnbimaSP,faceValue=100000) {

  #Obtains a panel of maturities given in mtyGrid from fitted NSS curves.
  
  panelBonds = data.frame(matrix(NA,length(dates)+1,length(mtyGrid)))
  panelBonds_pu = data.frame(matrix(NA,length(dates)+1,length(mtyGrid)))
  colnames(panelBonds) = paste0("m.",mtyGrid)
  
  rowCounter = 1
  datesSample = c.Date()
  for (k in seq_along(dates)) {
    dt = dates[[k]]
    if (is.bizday(dt,cal=ECOFIN.env$calSQLAnbimaSP)) {
      currRates = getCurvesPoints(dt,gridNSS_eval=mtyGrid,faceValue=faceValue)
      panelBonds[rowCounter,] = currRates$numericalNSS
      panelBonds_pu[rowCounter,] = currRates$numericalPU
      rowCounter=rowCounter+1
      datesSample = c.Date(datesSample,dt)
    }
  }
  panelBonds = panelBonds[complete.cases(panelBonds),]
  panelBonds_pu = panelBonds_pu[complete.cases(panelBonds_pu),]
  row.names(panelBonds) <- datesSample
  row.names(panelBonds_pu) <- datesSample
  
  return(list(rates=panelBonds,prices=panelBonds_pu,dates=datesSample))

}

nssMtyTS <- function(mty=0,startdate=as.Date('2016-01-01'),enddate=as.Date('2016-12-31')) {
  
  startdate = as.Date(startdate)
  enddate = as.Date(enddate)
  
  dates = seq.Date(startdate,enddate,"days")
  funApply = function(dt) getNSSPoint(dt,mty=mty)
  rateTS = sapply(dates,funApply)
  rateTS = zoo(rateTS,order.by = dates)
  rateTS = squareData(rateTS)
  
  puTS = 100000/((1+rateTS)^mty)

  return(list(rate=rateTS,price=puTS))
    
}
nssga_real_fn <- function(dt,nIter=1000) {
  
  cat("\014");
  setwd("Z:/ASSET/ECO/Yuri/R/Functions/")
  
  ###################################################################################################  
  #DESCRIPTION   ####################################################################################
  ################################################################################################### 
  #Modelo de Nelson-Siegel-SVENSSON usando algoritimo gen?tico
  #A parte longa da curva esta sendo fixada como o v?rtice de maior maturidade dispon?vel para o dia t
  #o
  
  ###################################################################################################  
  #GENERIC PARAMETERS ###############################################################################
  ################################################################################################### 
  # Generic Paramenters
  dbName            <- 'db_eco_fin';
  dbServer          <- 'SPQUANT5-U';
  
  mdlName           <- 'mdlNSSGA_JuroReal_beta0Termo'   #Name of the model
  mdlCode           <- matrix(nrow = 0, ncol =  0);
  vchPathRFiles     <- 'Z:/ASSET/ECO/Yuri/R/Functions/';
  vchWorkspacePath  <- paste('Z:/ASSET/ECO/ECOFIN/Juros/Desenvolvimento/Projeto Curvas/Anbima - Replicacao Modelos/',mdlName , '.RData',sep='');
  
  dt1 <- dateDictionary(dt)
  dt2 <- dateDictionary(dt)
  durationRateShock <- 0.01
  
  ###################################################################################################  
  #PACKAGES & SOURCES ###############################################################################
  ################################################################################################### 
  #Packages
  library("bizdays")
  library('optimbase')
  library('GA')
  library('ggplot2')
  library('bizdays')
  
  #Sources
  source(paste(vchPathRFiles, 'dateManipulation.R',sep = ""))
  source(paste(vchPathRFiles, 'dbManipulation.R',  sep = ""))
  source(paste(vchPathRFiles, 'optimization.R',    sep = ""))
  source(paste(vchPathRFiles, 'pricingFI.R',       sep = ""))
  source(paste(vchPathRFiles, 'SQL.R',             sep = ""))
  source('Z:/ASSET/ECO/ECOFIN/Chartbook Juros (Marcelo)/Códigos/sourceAux/insertNSSModelData1.R', echo=TRUE)
  source('Z:/ASSET/ECO/ECOFIN/Chartbook Juros (Marcelo)/Códigos/sourceAux/inputIterations.R', echo=TRUE)
  
  ###################################################################################################  
  # FILTER PARAMETERS ###############################################################################
  ################################################################################################### 
  # Liquidity Filter Parameters
  # percentileCutTrades<-0.6
  # percentileCutDeals<-0.5
  
  ###################################################################################################  
  # GETTING THE DATA FROM THE SQL SERVER  ###########################################################
  ################################################################################################### 
  # Creating the server connection
  objConn <- connectSQLServer(dbServer, dbName)
  
  # Checking the existence of the modelName
  mdlCode <- checkCurveModelExistence(objConn, mdlName)
  
  # Getting Anbima's holidays table and generating a calendar object
  numHolidaysAnbima <- getHolidaysAnbima(objConn)
  calSQLAnbima      <- create.calendar(name       = "SQLAnbima",
                                       holidays   = xlDateNumeric2RDate_yyyymmdd(numHolidaysAnbima),
                                       start.date = "1950-01-01", 
                                       end.date   = "2078-12-25", 
                                       weekdays   = c("saturday", "sunday"))
  
  # Getting ANBIMA NTN-B Data
  # df_ntnb_data <- getNTNBANBIMAData(objConn, dt1, dt2);
  df_ntnb_data <- downloadCurveReal(dt)
  
  # Converting fltData and fltVencimento into Dates
  df_ntnb_data[,'fltData']       <- xlDateNumeric2RDate_yyyymmdd(df_ntnb_data[,'fltData']);
  df_ntnb_data[,'fltVencimento'] <- xlDateNumeric2RDate_yyyymmdd(df_ntnb_data[,'fltVencimento']);
  
  # Excluding bullet securities
  # df_ntnb_data <- df_ntnb_data[df_ntnb_data[, 'fltCodSelic'] != 760100, ];
  
  # Calculating fltSaques and appending it to df_ntnb_data
  df_ntnb_data[,'fltSaques'] <- bizdays(from = df_ntnb_data[,'fltData'],
                                        to   = df_ntnb_data[,'fltVencimento'],
                                        cal  = calSQLAnbima)
  
  # Calculating fltEffectiveDuration for each security
  df_ntnb_data[,'fltEffectiveDuration'] <- rep(NA, size(x = df_ntnb_data, n = 1));  
  df_ntnb_data[,'fltCotacao'] <- rep(NA, size(x = df_ntnb_data, n = 1));  
  lst_cash_flow_structure <- list();
  
  for(k in 1:size(x = df_ntnb_data, n = 1)){
    #Generating the cash flow structure for the bond  
    df_temp <- ntnb_cash_flow_structure(   selic_code = df_ntnb_data$fltCodSelic[k],
                                           current_date = df_ntnb_data$fltData[k],
                                           maturity_date = df_ntnb_data$fltVencimento[k],
                                           cal = calSQLAnbima);
    
    df_temp[, 'yield_maturity'] <- rep(    x = df_ntnb_data$fltTxIndicativa[k],
                                           times = length(df_temp$cash_flow));
    
    lst_cash_flow_structure[[k]] <- df_temp;
    
    #Calculating the effective duration and Cotacao
    df_ntnb_data$fltEffectiveDuration[k] <- effDur(notional = df_temp$cash_flow,
                                                   yld = df_temp$yield_maturity ,
                                                   time2expiry = df_temp$bus_days, 
                                                   deltaYield = durationRateShock);
    
    df_ntnb_data$fltCotacao[k] <- sum(df_temp$cash_flow / ((1 + df_temp$yield_maturity) ^ df_temp$bus_days));
  }
  
  
  ###################################################################################################  
  # OPTIMIZATION PARAMETERS #########################################################################
  ################################################################################################### 
  model2Select <- NSS;
  de_param     <- list(min	= c(  0 , -0.5, -1, -1, 0 , 0),
                       max	= c( 0.5,  0.5,  1,  1, 10, 10))
  
  ###################################################################################################  
  # MAIN ROUTINE ####################################################################################
  ################################################################################################### 
  curveDates  <-  unique(df_ntnb_data$fltData)
  lsOutput    <-  list(dates   = matrix(, nrow = 0, ncol = 1),
                       OFvalue = matrix(, nrow = 0, ncol = 1),
                       optTime = matrix(, nrow = 0, ncol = 1),
                       beta    = matrix(, nrow = 0, ncol = size(de_param$min,2)),
                       SSR     = matrix(, nrow = 0, ncol = 1))    
  
  #Getting data for a specific date - BMF - DI1
  objTMP  <- df_ntnb_data[df_ntnb_data$fltData == curveDates[1], ];
  lst_tmp <-lst_cash_flow_structure[df_ntnb_data$fltData == curveDates[1]];
  
  
  #Eliminating if there is a contract that has number of business days less or equal than 1   
  lst_tmp <-  lst_tmp[objTMP$fltSaques > 1];
  objTMP  <-  objTMP[objTMP$fltSaques > 1, ];
  
  #Creating the Future Cash Flow Matrix and the vector of business days to payment
  lst_tmp <- ntnb_cash_flow_matrix_vector_of_bdays_2_payment(lst_tmp);
  
  #Creating the datalist object
  dataList<-list(yM                 = objTMP$fltTxIndicativa,                # Yield to maturity rate
                 UP                 = objTMP$fltPU,                          # Unitary Price
                 mats               = objTMP$fltSaques / 252,                # Maturities in years
                 model              = model2Select,                          # Model
                 effectiveDuration  = objTMP$fltEffectiveDuration,           # Effective Duration 
                 cash_flows         = lst_tmp$cash_flows,                    # Cash Flows
                 bus_days           = lst_tmp$bus_days,                      # Business time horizon in years
                 cotacao_mkt        = as.matrix(objTMP$fltCotacao))          # Market Cotacao - Anbima                      
  
  #Defining the fitness ("objective-function")  with penalty 
  f <- function(x, data = dataList) - NSS_OF_Juro_Real_Brasil_Inverse_Duration_Weighted(x, data, lasso_factor = 0.000000001, factor_weights = c(1, 1, 1, 1, 0, 0))
  
  #Defining the suggestion of initial points and the number of iterations
  
  # Reference: Gimeno e Nave (2006) 
  # Beta0: Yield to maturity of the asset with least time until maturity
  # Beta1: Difference between Yields to maturity of the assets with the least and the most time to maturity
  # Beta2: 0 -> Assumption that there are no U shapes
  # Tau1 : Term to maturity of the asset with a closest yield to maturity of the halfway between Beta0 and Beta0+Beta1
  # Beta3: 0 -> Assumption that there are no U shapes
  # Tau2 : Equal tau1
  ini_gen_algo <- matrix(data = NA, nrow = 1 , ncol = 6);
  ini_gen_algo[1,1] <- objTMP$fltTxIndicativa[length(objTMP$fltTxIndicativa)];
  ini_gen_algo[1,2] <- objTMP$fltTxIndicativa[1] - objTMP$fltTxIndicativa[length(objTMP$fltTxIndicativa)] ;
  ini_gen_algo[1,3] <- 0;
  ini_gen_algo[1,4] <- 0;
  a <- abs((ini_gen_algo[1,1] - ini_gen_algo[1,2] + ini_gen_algo[1,1])/2-objTMP$fltTxIndicativa);
  ini_gen_algo[1,5] <- objTMP$fltSaques[which(a < c(a[2:length(a)], NA))[1]]/252;
  ini_gen_algo[1,6] <- ini_gen_algo[1,5];
  
  #Yesterday Solution
  #If exists an already optimized solution for the previous day, pull it from the database
  #else, optimize current day with 20000 Evolutions 
  strSQL       <- paste("get_data_from_tb_curve_models_parameters", as.character(mdlCode), ",", as.character(ECOFIN.env$Date_yyyymmdd2xlDateNumeric(adjust.previous(dates = curveDates[1]-1,cal = ECOFIN.env$calSQLAnbima ))));
  query_output <- sqlQuery(channel = objConn, query = strSQL);
  
  if (size(query_output,1) == 0){
    # max_iter <- 3000;
    
    delta_ini <- max(objTMP$fltTxIndicativa) - min(objTMP$fltTxIndicativa);
    
    ini_gen_algo <- rbind(ini_gen_algo, matrix(data = NA, nrow = 1, ncol = 6));
    ini_gen_algo[2,1] <- objTMP$fltTxIndicativa[length(objTMP$fltTxIndicativa)] + delta_ini;
    ini_gen_algo[2,2] <- 0;
    ini_gen_algo[2,3] <- 0;
    ini_gen_algo[2,4] <- 0;
    ini_gen_algo[2,5] <- 1;
    ini_gen_algo[2,6] <- 1;
    
    ini_gen_algo <- rbind(ini_gen_algo, matrix(data = NA, nrow = 1, ncol = 6));
    ini_gen_algo[3,1] <- objTMP$fltTxIndicativa[length(objTMP$fltTxIndicativa)] - delta_ini;
    ini_gen_algo[3,2] <- 0;
    ini_gen_algo[3,3] <- 0;
    ini_gen_algo[3,4] <- 0;
    ini_gen_algo[3,5] <- 1;
    ini_gen_algo[3,6] <- 1;
    
  }else{
    # max_iter <- 3000;
    delta_ini <- max(objTMP$fltTxIndicativa) - min(objTMP$fltTxIndicativa);
    ini_gen_algo <- rbind(ini_gen_algo, matrix(data = NA, nrow = 1, ncol = 6));
    ini_gen_algo[2,1] <- query_output$fltBeta0;
    ini_gen_algo[2,2] <- query_output$fltBeta1;
    ini_gen_algo[2,3] <- query_output$fltBeta2;
    ini_gen_algo[2,4] <- query_output$fltBeta3;
    ini_gen_algo[2,5] <- query_output$fltTau1;
    ini_gen_algo[2,6] <- query_output$fltTau2;
    
    ini_gen_algo <- rbind(ini_gen_algo, matrix(data = NA, nrow = 1, ncol = 6));
    ini_gen_algo[2,1] <- objTMP$fltTxIndicativa[length(objTMP$fltTxIndicativa)] + delta_ini;
    ini_gen_algo[2,2] <- 0;
    ini_gen_algo[2,3] <- 0;
    ini_gen_algo[2,4] <- 0;
    ini_gen_algo[2,5] <- 1;
    ini_gen_algo[2,6] <- 1;
    
    ini_gen_algo <- rbind(ini_gen_algo, matrix(data = NA, nrow = 1, ncol = 6));
    ini_gen_algo[3,1] <- objTMP$fltTxIndicativa[length(objTMP$fltTxIndicativa)] - delta_ini;
    ini_gen_algo[3,2] <- 0;
    ini_gen_algo[3,3] <- 0;
    ini_gen_algo[3,4] <- 0;
    ini_gen_algo[3,5] <- 1;
    ini_gen_algo[3,6] <- 1;
    
  }
  
  
  #Optimizing      
  elpsTime <- system.time(sol <- gaisl(type = "real-valued", 
                                       fitness = f, 
                                       data = dataList, 
                                       popSize = 15000, 
                                       maxiter = nIter,   #2000 
                                       min = de_param$min, 
                                       max = de_param$max,
                                       numIslands = 8 ,
                                       suggestions = ini_gen_algo ))
  
  
  #Saving the output
  lsOutput$dates   <- rbind(lsOutput$dates, ECOFIN.env$Date_yyyymmdd2xlDateNumeric(curveDates[k]))
  lsOutput$OFvalue <- rbind(lsOutput$OFvalue, sol@fitnessValue)
  lsOutput$optTime <- rbind(lsOutput$optTime, elpsTime[3])
  lsOutput$beta    <- rbind(lsOutput$beta, sol@solution)
  lsOutput$SSE     <- rbind(lsOutput$SSE, sum((dataList$yM - NSS(sol@solution, dataList$mats)) ^ 2))
  
  #Uploading the output into SQL
  inputNSSModelData1(objConn, 
                     ECOFIN.env$Date_yyyymmdd2xlDateNumeric(curveDates[1]), 
                     mdlCode, 
                     sum((dataList$yM - NSS(sol@solution, dataList$mats)) ^ 2),
                     sol@solution[1],
                     sol@solution[2], 
                     sol@solution[3], 
                     sol@solution[4], 
                     sol@solution[5], 
                     sol@solution[6]);
  
  inputIterations(objConn,nIter,mdlCode,dt1);
  
  #Status
  cat(paste(k, " "))
  flush.console()
  
}




nssga_fn <- function(dt,nIter = 1000) {
  
  cat("\014");
  setwd("Z:/ASSET/ECO/Yuri/R/Functions/")
  mdlName           <- 'mdlNSSGA1'   #Name of the model
  mdlCode           <- matrix(nrow = 0,ncol =  0);
  vchPathRFiles     <- 'Z:/ASSET/ECO/Yuri/R/Functions/';
  vchWorkspacePath  <- paste('Z:/ASSET/ECO/ECOFIN/Juros/Desenvolvimento/Projeto Curvas/Anbima - Replica??o Modelos/',mdlName , '.RData',sep='');
  
  
  #Sources
  source(paste(vchPathRFiles,'dateManipulation.R',sep = ""))
  source(paste(vchPathRFiles,'dbManipulation.R',sep = ""))
  source(paste(vchPathRFiles,'optimization.R',sep = ""))
  source(paste(vchPathRFiles,'pricingFI.R',sep = ""))
  source(paste(vchPathRFiles,'SQL.R',sep = ""))
  source(paste(vchPathRFiles,'SQL/getHolidaysAnbima.R',sep = ""))
  
  ###################################################################################################  
  #DESCRIPTION   ####################################################################################
  ################################################################################################### 
  #Modelo de Nelson-Siegel-SVENSSON usando algoritimo gen?tico
  #A parte longa da curva esta sendo fixada como o v?rtice de maior maturidade dispon?vel para o dia t
  #o
  
  dt1 <- dateDictionary(dt)
  dt2 <- dateDictionary(dt)
  
  durationRateShock <- 0.01
  
  dbName            <- 'db_eco_fin';
  dbServer          <- 'SPQUANT5-U';
  
  ###################################################################################################  
  #PACKAGES & SOURCES ###############################################################################
  ################################################################################################### 
  #Packages
  library("bizdays")
  library('optimbase')
  library('GA')
  library('ggplot2')
  #library('latex2exp')
  #library("doParallel")
  
  #Slaveslibraries
  #Creating Cluster
  #  cl <- makeCluster(8)
  #  clusterEvalQ(cl,library(GA))
  
  ###################################################################################################  
  #GETTING THE DATA FROM THE SQL SERVER  ############################################################
  ################################################################################################### 
  #Creating the server connection
  objConn <- connectSQLServer(dbServer, dbName)
  
  #Checking the existence of the modelName
  mdlCode <- checkCurveModelExistence(objConn, mdlName)
  
  #Getting Anbima?s holidays table
  numHolidaysAnbima <- getHolidaysAnbima(objConn)
  calSQLAnbima      <- Calendar(name = "SQLAnbima", holidays = xlDateNumeric2RDate_yyyymmdd(numHolidaysAnbima),
                                start.date = "1950-01-01", end.date = "2078-12-25", weekdays = c("saturday", "sunday"))
  
  
  ###################################################################################################  
  #OPTIMIZATION PARAMETERS ##########################################################################
  ################################################################################################### 
  model2Select=NSS
  de_param <- list(min	= c(-0.5,-1,-1,0 , 0),
                   max	= c( 0.5, 1, 1,5, 5))
  ###################################################################################################  
  #MAIN ROUTINE #####################################################################################
  ################################################################################################### 
  
  lsOutput<-list(dates=matrix(,nrow=0,ncol=1), OFvalue=matrix(,nrow=0,ncol=1),optTime=matrix(,nrow=0,ncol=1),beta=matrix(,nrow=0,ncol=1+size(de_param$min,2)),SSR=matrix(,nrow=0,ncol=1))    
  
  lsUsedData = list() #will be used to append used rates and maturities

  objTMP <- downloadCurveDI(dt)
  curveDates <- unique(objTMP$fltData)
  
  #dataList     
  dataList<-list(yM                 = objTMP$fltSettlementRate,            #Settlement Rate    
                 mats               = objTMP$fltSaques/252,              #Maturities
                 model              = model2Select,                     #Model
                 effectiveDuration  = effDurMult(objTMP$fltSettlementPrice, objTMP$fltSettlementRate, objTMP$fltSaques / 252, durationRateShock),
                 fltSettlementPrice = objTMP$fltSettlementPrice,
                 fltNumberTradesDay = 1)#, #any number >0; tem um filtro na otimização pra trades=0...
                 # objTMP$fltNumberDealsDay)
  
  
  #Defining the fitness ("objective-function")  with penalty 
  f <- function(x, data = dataList) -NSS_OF_DVTraded_b0_penalty_tau(x, data)
  
  previousBeta = getPreviousNSSSol(dt,method='nominal')
  previousBeta = previousBeta[-1] #beta_0 is fixed
  
  #Optimizing      
  elpsTime <- system.time(sol <- ga(type = "real-valued", fitness = f, data = dataList, popSize = 5000, 
                                    maxiter = nIter, min = de_param$min, max = de_param$max,
                                    suggestions = previousBeta))
  
  # plotGA = plot(sol)
  
  #Putting the fixed Beta0 into the solution vector
  sol@solution = cbind(as.matrix(dataList$yM[max(dataList$mats) == dataList$mats]), sol@solution)
  
  #Saving the output
  lsOutput$dates   = rbind(lsOutput$dates, curveDates[1])
  lsOutput$OFvalue = rbind(lsOutput$OFvalue, sol@fitnessValue)
  lsOutput$optTime = rbind(lsOutput$optTime, elpsTime[3])
  lsOutput$beta    = rbind(lsOutput$beta, sol@solution)
  lsOutput$SSE     = rbind(lsOutput$SSE, sum((dataList$yM - NSS(sol@solution, dataList$mats)) ^ 2))
  
  #Appending data used in optimization routine
  currUsedDate = list(rates=dataList$yM,mty=dataList$mats)
  lsUsedData = c(lsUsedData,list(curve=currUsedDate))
  
  #Uploading the output into SQL
  inputNSSModelData1(objConn, curveDates[1], mdlCode, sum((dataList$yM - NSS(sol@solution, dataList$mats)) ^ 2),
                     sol@solution[1], sol@solution[2], sol@solution[3], sol@solution[4], sol@solution[5], 
                     sol@solution[6])
  
  inputIterations(objConn,nIter,mdlCode,dt1)
  
  #Status
  flush.console()
  
  # return()
  
}
# nelson--siegel--svensson 1
NSS <- function(betaV,mats)
{
  
  if (sum(mats) == 0) {
    y = betaV[1] + betaV[2]
  }
  else {
    # betaV = beta1-4, lambda1-2
    gam1 <- mats / betaV[5]
    gam2 <- mats / betaV[6]	
    y <- betaV[1] + betaV[2] * ((1 - exp(-gam1)) / (gam1)) + 
      betaV[3] * (((1 - exp(-gam1)) / (gam1)) - exp(-gam1)) + 
      betaV[4] * (((1 - exp(-gam2)) / (gam2)) - exp(-gam2))
  }
  
  return(y)
}


NSS_B0 <- function(betaV,mats)
{
  y <- betaV[1]+ mats-mats
  return(y)
}

NSS_B1 <- function(betaV,mats)
{
  # betaV = beta1-4, lambda1-2
  gam1 <- mats / betaV[5]
  y <- betaV[2] * ((1 - exp(-gam1)) / (gam1))  
  return(y)
}

NSS_B2 <- function(betaV,mats)
{
  # betaV = beta1-4, lambda1-2
  gam1 <- mats / betaV[5]
  y <-  betaV[3] * (((1 - exp(-gam1)) / (gam1)) - exp(-gam1)) 
  return(y)
}

NSS_B3 <- function(betaV,mats)
{
  # betaV = beta1-4, lambda1-2
  gam2 <- mats / betaV[6]  
  y <-  betaV[4] * (((1 - exp(-gam2)) / (gam2)) - exp(-gam2)) 
  return(y)
}

downloadCurveDI <- function(dt,objConn = ECOFIN.env$objConn) {
  
  #Checks if dt is a holiday
  if (is.bizday(dt,cal=ECOFIN.env$calSQLAnbimaSP)==FALSE) {
    return(c('holiday'))
  }

  #Downloads DB via .mat Files
  source('Z:/ASSET/ECO/ECOFIN/Chartbook Juros (Marcelo)/Códigos/sourceAux/downloadMatDB.R', echo=FALSE)
  loadMatDB(0)
  
  di_inds = intersect(which(dtMETA$ASSET.GROUP=="FUTURE_CHAIN"),grep("DI",dtMETA$SHORT.NAME))
  di_names = dtMETA$SHORT.NAME[di_inds]
  di_mty = dtMETA$MATURITY[di_inds]
  metaAvailable =  match(di_names,names(dtTX))
  metaInds = index(metaAvailable)
  metaAvailable = metaAvailable[!is.na(metaAvailable)]
  di_zoo = dtTX[,metaAvailable]
  di_pu = dtPU[,metaAvailable]
  di_mty = as.numeric(di_mty[metaInds])
  di_names = di_names[metaInds]
  
  d0 = dt
  d0_num = dateDictionary(d0)
  di_mty_dt = dateDictionary(di_mty)
  
  'Sorting maturities'
  di_mty_unn = di_mty_dt
  di_mty_dt = sort(di_mty_unn)
  sortInd = match(di_mty_dt,di_mty_unn)
  di_names = di_names[sortInd]
  di_mty = di_mty[sortInd]
  
  selPrices = match(di_names,names(di_pu))
  selPrices = selPrices[!is.na(selPrices)]
  di_pu = di_pu[,selPrices]
  
  selTx = match(di_names,names(di_zoo))
  selTx = selTx[!is.na(selTx)]
  di_zoo = di_zoo[,selTx]
  
  'Excluding expired contracts'
  di_active_inds = di_mty_dt>d0
  
  fltData = d0_num
  di_mty_dt = di_mty_dt[di_active_inds]
  di_names = di_names[di_active_inds]
  di_mty = di_mty[di_active_inds]
  
  fltVencimento = di_mty
  fltSaques = bizdays(d0,di_mty_dt,cal=ECOFIN.env$calSQLAnbimaSP)
  
  fltSettlementRate = t(di_zoo[index(di_zoo)==d0,])
  fltSettlementPrice = t(di_pu[index(di_pu)==d0,])
  vchSerie = di_names
  
  fltSettlementPrice_m = data.frame(vchSerie=rownames(fltSettlementPrice),fltSettlementPrice)
  colnames(fltSettlementPrice_m) = c("vchSerie","fltSettlementPrice")
  fltSettlementRate_m = data.frame(vchSerie=rownames(fltSettlementRate),fltSettlementRate)
  colnames(fltSettlementRate_m) = c("vchSerie","fltSettlementRate")
  
  objNew = data.frame(fltData,vchSerie,fltVencimento,fltSaques)
  objNew = merge(objNew,fltSettlementPrice_m)
  objNew = merge(objNew,fltSettlementRate_m)
  
  #Appending CDI
  dtfSQLOutputDI1<-getCDICETIPData(objConn,d0_num,d0_num)
  #Getting data for a specific date - CETIP - CDI
  fltData             <- dtfSQLOutputDI1$fltData
  vchSerie            <- "CDI"
  fltVencimento       <- dateDictionary(dt)
  fltSaques           <- 1 
  fltSettlementPrice  <- 100000 / ((1 + dtfSQLOutputDI1$fltTaxaMedia) ^ (fltVencimento / 252))
  fltSettlementRate   <- dtfSQLOutputDI1$fltTaxaMedia*100
  objTMPCDI = data.frame(vchSerie,fltData,fltVencimento,fltSaques,
                         fltSettlementPrice,fltSettlementRate)
  
  objNew = rbind(objTMPCDI,objNew)
  
  #Removing rows with NAs
  objNew = objNew[complete.cases(objNew),]
  
  #Sorting by Maturity
  objNew = objNew[order(objNew$fltSaques),]
  objNew$fltSettlementRate = objNew$fltSettlementRate/100
  
  tblCurve = objNew
  return(tblCurve)
  
}
#Caso Geral
likelihood_monPol_MultiPeriod = function(x,meetings,mktYieldsUnique,mktDatesUnique,d0Rate,d0,cal) {
  
  onRate = d0Rate
  onPath = c()
  d0Mty = bizdays(d0,meetings[1],cal)
  onRateFac = (1+onRate/100)^(d0Mty/252)
  onRateAnn = (onRateFac^(252/d0Mty)-1)*100
  onRate = onRateAnn
  onPathFac = c()
  onActualPath = c()
  onRateFacActualMtyAnn = c()
  onActualPathAnn = c()
  onRateFacActualMty = c()
  onMtyPath = c()
  onRateAnnPath = c()
  for (j in 1:(length(meetings)-1)) {
  
    onRateAnn = onRateAnn+x[j]/100 #x in bps
    onMty = bizdays(meetings[j],meetings[j+1],cal)
    onMtyPath = c(onMtyPath,onMty)
    onRateFac = (1+onRateAnn/100)^(onMty/252)
    onPathFac = c(onPathFac,onRateFac)
    onRateAnnPath = c(onRateAnnPath,onRateAnn)
    
    actualMty = bizdays(d0,mktDatesUnique[j],cal)
    interMty = bizdays(meetings[j],mktDatesUnique[j],cal)
    onActualPath = c(onActualPath,(1+mktYieldsUnique[j]/100)^(actualMty/252))
    if (j == 1) {
      onRateFacActualMty = ((1+d0Rate/100)^(d0Mty/252))*((1+onRateAnn/100)^(interMty/252))
    } else {
      onRateFacActualMty = c(onRateFacActualMty,
                             ((1+d0Rate/100)^(d0Mty/252))*prod(onPathFac[-length(onPathFac)])*((1+onRateAnn/100)^(interMty/252)))
    }
    onRateFacActualMtyAnn = c(onRateFacActualMtyAnn,onRateFacActualMty[j]^(252/actualMty))
    onActualPathAnn = c(onActualPathAnn,onActualPath[j]^(252/actualMty))
    
    onRateAnn = (onRateFacActualMtyAnn[j]-1)*100
    
  }
  
  guess = onRateFacActualMty
  truth = onActualPath

  penalty = crossprod(guess-truth)
  return(penalty)
  
}
#Caso Geral
likelihood_monPol_2Period = function(x,dtMeet,ratePreMeet,mktYield,dtPreMeet,dtPostMeet,cal,
                                     onAdj=0) {
  
  #onAdj controls for overnight correction of rates, used by BBG to price the first meeting
  
  wdAdj = 252
  d0 = dtPreMeet
  ctExON = dtPostMeet
  
  #finding first contract after meeting
  ctMty = bizdays(d0,dtPostMeet,cal)
  onMty = bizdays(d0,dtMeet,cal)
  
  #Finding forward after overnight
  fwON_effPeriod = ((mktYield+1)^(ctMty/wdAdj))/((ratePreMeet+1)^(onAdj/wdAdj))
  fwON_ann = (fwON_effPeriod^(wdAdj/(ctMty-onAdj))-1)
  
  onPer = (1+ratePreMeet)^(onMty/wdAdj)
  ptMeetPer = (1+x)^((ctMty-onMty)/wdAdj)
  effRate = onPer*ptMeetPer
  annRate = effRate^(wdAdj/ctMty)
  
  guess = effRate-1
  truth = ((1+fwON_ann))^(ctMty/wdAdj)-1

  # Não muda resultadoo; transformações monotônicas da função objetivo.
  # guess = effRate^(wdAdj/ctMty)
  # truth = ((1+fwON_ann))
  
  penalty = crossprod(guess-truth)
  return(penalty)
  
}
inputNSSModelData1<-function(objDB,fltDate,intModelRef,fltSSE,fltBeta0,fltBeta1,fltBeta2,fltBeta3,fltTau1,fltTau2){
  
  #Deletando dado anterior caso já exista na base de dados
  chaSQL <- paste("DELETE FROM tb_curve_models_parameters WHERE intModelRef = ", intModelRef," AND fltDate", fltDate);   
    sqlQuery(objDB, query = chaSQL)  
  
  
  
  #Escrevendo a query,
    chaSQL <- paste("INSERT INTO tb_curve_models_parameters VALUES (", as.character(fltDate),",",as.character(intModelRef),  
                    ",",as.character(fltSSE),",",  as.character(fltBeta0),",",as.character(fltBeta1), ",", 
                    as.character(fltBeta2),",",as.character(fltBeta3),",",as.character(fltTau1)  ,",",as.character(fltTau2) ,")" ,sep=" ")  
    
  #Executando o procedimento
    sqlQuery(objDB, query = chaSQL)  
}
  
  
  
insertCOPOMMarketData<-function(objDB,intMethod,dtDate,intMeetingNumber,dtMeetingDate,dtEffectiveDate,fltChange){
  
  #Escrevendo a query,
  chaSQL<-paste("INSERT INTO tb_copom_market VALUES (",as.character(intMethod),",",
                                                       paste("'",as.character(dtDate),"'",sep=""),",",
                                                       as.character(intMeetingNumber),",",
                                                       paste("'",as.character(dtMeetingDate),"'",sep=""),",",
                                                       paste("'",as.character(dtEffectiveDate),"'",sep=""),",",
                                                       as.character(fltChange),")")

  #Executando o procedimento
    sqlQuery(objDB,query=chaSQL)  
}

insertBreakEvenMoMMarketData<-function(objDB,intMethod,dtDate,vchDataFrequency,intDataOrdering,dtReferenceDate,fltValue){
  
  #Escrevendo a query,
  chaSQL<-paste("INSERT INTO tb_implied_in_market VALUES (",as.character(intMethod),",",
                                                            paste("'",as.character(dtDate),"'",sep=""),",",
                                                            paste("'", vchDataFrequency, "'", sep = ""),",",
                                                            as.character(intDataOrdering),",",
                                                            paste("'",as.character(dtReferenceDate),"'",sep=""),",",
                                                            as.character(fltValue),")")

  #Executando o procedimento
    sqlQuery(objDB, query = chaSQL)  ;
}

inputIterations <- function(objDB,nIter,intModelRef,fltDate) {
  
  #Deletando dado anterior caso já exista na base de dados
  chaSQL <- paste("DELETE FROM tb_curve_models_iterations WHERE intModelRef = ", intModelRef," AND fltDate = ", fltDate);   
  sqlQuery(objDB, query = chaSQL)  
  
  #Escrevendo a query,
  chaSQL <- paste("INSERT INTO tb_curve_models_iterations VALUES (", as.character(fltDate),",",as.character(intModelRef),  
                  ",",as.character(nIter),")" ,sep=" ")  
  
  #Executando o procedimento
  sqlQuery(objDB, query = chaSQL) 
  
}
getUsedDataNSS <- function(dt,objConn = ECOFIN.env$objConn) {
  
  ##OLD DATA SOURCE
  # 
  # #obtains curve data for a given date used in the NSS optimization procedure
  # #objConn <- connectSQLServer('SPQUANT5-U', 'db_eco_fin')
  # 
  # #Getting DI1 data
  # dtfSQLOutputDI1FUT<-getBMFDIData(objConn,dt,NULL)
  # 
  # #Getting DI-Over data
  # dtfSQLOutputDI1<-getCDICETIPData(objConn,dt,NULL)
  # 
  # #Getting data for a specific date - BMF - DI1
  # objTMPDI1<-dtfSQLOutputDI1FUT[dtfSQLOutputDI1FUT$fltData==dt,1:size(dtfSQLOutputDI1FUT,2)]
  # 
  # #Eliminating if there is a contract that has number of business days less or equal than 1   
  # objTMPDI1<-objTMPDI1[objTMPDI1$fltSaques>1,]
  # 
  # #Getting data for a specific date - CETIP - CDI
  # objTMPCDI           <- dtfSQLOutputDI1[dtfSQLOutputDI1$fltData==dt,1:size(dtfSQLOutputDI1,2)]
  # fltData             <- objTMPCDI$fltData
  # vchSerie            <- "CDI"
  # fltVencimento       <- 1
  # fltSaques           <- 1 
  # fltOpenInterest     <- 0
  # fltNumberTradesDay  <- 0
  # fltNumberDealsDay   <- 0
  # fltSettlementPrice  <- 100000 / ((1 + objTMPCDI$fltTaxaMedia) ^ (fltVencimento / 252))
  # fltSettlementRate   <- objTMPCDI$fltTaxaMedia
  # fltOpenRate         <- 0
  # fltHighRate         <- objTMPCDI$fltTaxaMaxima
  # fltLowRate          <- objTMPCDI$fltTaxaMinima
  # fltLastRate         <- 0
  # 
  # objTMPCDI <- data.frame(fltData, vchSerie, fltVencimento, fltSaques, fltOpenInterest, fltNumberTradesDay,
  #                         fltNumberDealsDay, fltSettlementPrice, fltSettlementRate, fltOpenRate, fltHighRate, 
  #                         fltLowRate, fltLastRate)
  # 
  # objTMP    <- rbind(objTMPCDI, objTMPDI1)
  
  objTMP <- downloadCurveDI(dateDictionary(dt))
  
  #dataList     
  dataList<-list(yM                 = objTMP$fltSettlementRate,            #Settlement Rate    
                 mats               = objTMP$fltSaques/252)#,              #Maturities
                 #model              = model2Select,                     #Model
                 #effectiveDuration  = effDurMult(objTMP$fltSettlementPrice, objTMP$fltSettlementRate, objTMP$fltSaques / 252, durationRateShock),
                 #fltSettlementPrice = objTMP$fltSettlementPrice,
                 #fltNumberTradesDay = objTMP$fltNumberTradesDay,
                 #objTMP$fltNumberDealsDay)
  
  #odbcClose(objConn)
  
  vctDtRet = objTMP$fltVencimento
  vctDtRet[1] = dt
  return(list(rates=dataList$yM,mty=dataList$mats,vctDt=vctDtRet))

  
}
getPreviousNSSSol <- function(dt, method = 'nominal') {
  
  previousDate = add.bizdays(dt,-1,cal = ECOFIN.env$calSQLAnbimaSP)
  previousBeta = getBetasNSS(objDB = ECOFIN.env$objConn,previousDate,method)
  #pega o beta rodado para o dia anterior mais próximo
  while (sum(is.na(previousBeta)) > 0) {
    previousDate = add.bizdays(previousDate,-1,cal = ECOFIN.env$calSQLAnbimaSP)
    previousBeta = getBetasNSS(objDB = ECOFIN.env$objConn,previousDate,method)
  }
  return(previousBeta)
  
}
getPreviousDates <- function(dt,objDB=ECOFIN.env$objConn) {
  
  dtPrevious = add.bizdays(dt,c(-5,-22),objDB$calcSQLAnbimaSP)
  
  # chaSQL<-'SELECT DISTINCT [dtDate]  FROM [DB_ECO_FIN].[dbo].[tb_copom_market]  ORDER BY dtDate DESC'
  # 
  # #Executando procedimento
  # getPreviousDatesData<-sqlQuery(objDB,query=chaSQL)
  # 
  # dtPos = which(getPreviousDatesData == dt)
  # 
  # dtBefore = c(as.character(getPreviousDatesData[dtPos+6,1]),
  #              as.character(getPreviousDatesData[dtPos+21,1]))
  
  return(dtBefore)
  
}
##Consultar curva de juros para uma data especifica - BMF Ajuste
getNTNBANBIMAData<-function(objDB,dt1,dt2)
{
  if (is.null(dt2)==FALSE){
    #Escrevendo a query
    chaSQL<-paste('getNTNB_ANBIMA', as.character(dt1),',',as.character(dt2))
    
    #Executando procedimento
    getNTNBANBIMAData<-sqlQuery(objDB,query=chaSQL)
    
    #Samda da fungco
    return(getNTNBANBIMAData)
    
  }else{
    #Escrevendo a query
    chaSQL<-paste('getNTNB_ANBIMA', as.character(dt1))
    
    #Executando procedimento
    getNTNBANBIMAData<-sqlQuery(objDB,query=chaSQL)
    
    #Samda da fungco
    return(getNTNBANBIMAData)  
  }
}
getNSSPoint <- function(dt,mty=1/12) {
  
  #Returns NSS fitted point for a given maturity. Maturiy measured in years (i.e., one month = 1/12)
  
  dtVal = dateDictionary(dt)
  betas = getBetasNSS(ECOFIN.env$objConn,dtVal)

  nssPoint <- NSS(as.numeric(betas),mty)
  
  invisible(nssPoint)
  
}
getNominalRates <- function(dt,objDB=ECOFIN.env$objConn) {

  chaSQL = paste('SELECT [fltDate],[fltRateAverage] FROM [DB_ECO_FIN].[dbo].[tb_cdi_cetip]',
                 ' WHERE fltDate = ',dateDictionary(dt),sep="")
  cdi_on <- sqlQuery(objDB,query=chaSQL)
  cdi_on = cbind(cdi_on,dateDictionary(dt))
  names(cdi_on) = c("date","rateAv","settlementDt")
  
  bmf_fut = getBMFDIData(objDB,dateDictionary(dt))
  bmf_tab = bmf_fut[,c(1,9,3)]
  names(bmf_tab) = c("date","rateAv","settlementDt")
  
  ratesTab = rbind(cdi_on,bmf_tab)
  return(ratesTab)
  
}
getMarketCOPOMPricing <- function(dt,objDB = ECOFIN.env$objConn) {
  
  #Obtains COPOM pricing from market 
  chaSQL<-paste('SELECT *  FROM [DB_ECO_FIN].[dbo].[tb_copom_market]   WHERE dtDate = ',
                "'",as.character(dt),"'",
                sep = "")
  
  #Executando procedimento
  getMarketCOPOMPricingData<-sqlQuery(objDB,query=chaSQL)
  
  return(getMarketCOPOMPricingData)
  
}
  getInflationPrem_TS <- function(dt,objDB=ECOFIN.env$objConn) {
  
  monthsAhead = 12
  chaSQL<-paste('SELECT [dtDate],[dtReferenceDate],[fltValue] FROM [DB_ECO_FIN].[dbo].[tb_implied_in_market] WHERE intDataOrdering = ',
                "'",monthsAhead,"' AND vchDataFrequency = 'M' AND intMethod = 1",
                sep = "")
  
  #Breakeven
  getInflationPremData<-sqlQuery(objDB,query=chaSQL)
  # breakeven = getInflationPremData[,6]
  
  #Expectativas (Focus)
  chaSQL<-paste('SELECT [dtDate],[dtReferenceDate],[fltValue] FROM [DB_ECO_FIN].[dbo].[tb_bcb_expectativas] WHERE intCode = 1'
              ,sep="")
  # chaSQL<-paste('SELECT * FROM [DB_ECO_FIN].[dbo].[tb_bcb_expectativas] WHERE dtReferenceDate = ',
                # "'",as.character(dt),"'",' AND intCode = 1',sep="")
  
  getInflationExpecData<-sqlQuery(objDB,query=chaSQL)
  colnames(getInflationExpecData)[NCOL(getInflationExpecData)] <- "expec"
  
  df = merge(getInflationPremData,getInflationExpecData)
  
  # df = merge(getInflationPremData,getInflationExpecData,by="dtDate")
  
  premiumData = df$fltValue - df$expec
  
  dfPremium = data.frame(df$dtDate,premiumData,monthsAhead)
  
  # write.xlsx(dfPremium, "Z:/ASSET/ECO/ECOFIN/Chartbook Juros (Marcelo)/premiumData.xlsx",
  #            row.names = FALSE)
  return(dfPremium)
  
}
getInflationPrem <- function(dt,dtFocus,objDB=ECOFIN.env$objConn) {
  
  chaSQL<-paste('SELECT * FROM [DB_ECO_FIN].[dbo].[tb_implied_in_market] WHERE dtDate = ',
                "'",as.character(dt),"' AND vchDataFrequency = 'M' AND intMethod = 1",
                sep = "")
  
  #Breakeven
  getInflationPremData<-sqlQuery(objDB,query=chaSQL)
  breakeven = getInflationPremData[,6]
  
  #Expectativas (Focus)
  chaSQL<-paste('SELECT * FROM [DB_ECO_FIN].[dbo].[tb_bcb_expectativas] WHERE dtDate = ',
                  "'",as.character(dt),"'",' AND intCode = 1',sep="")

  getInflationExpecData<-sqlQuery(objDB,query=chaSQL)
  colnames(getInflationExpecData)[NCOL(getInflationExpecData)] <- "expec"
  
  df = merge(getInflationPremData,getInflationExpecData,by="dtReferenceDate")
  
  premiumData = df$fltValue - df$expec
  
  dfPremium = data.frame(df$dtDate.x,df$dtReferenceDate,premiumData)
  
  # write.xlsx(dfPremium, "Z:/ASSET/ECO/ECOFIN/Chartbook Juros (Marcelo)/premiumData.xlsx",
  #            row.names = FALSE)
  return(dfPremium)
  
}
getInflationExpec <- function(dtFocus,objDB=ECOFIN.env$objConn) {
  
  chaSQL<-paste('SELECT [dtReferenceDate], [fltValue] FROM [DB_ECO_FIN].[dbo].[tb_bcb_expectativas] WHERE dtDate = ',
                "'",as.character(dtFocus),"'",' AND intCode = 1',
                sep = "")
  
  #Executando procedimento
  getFocusExpecData<-sqlQuery(objDB,query=chaSQL)
  
  chaSQL<-paste('EXEC [dbo].[getLastIAMForecast]',sep="")

  getIAMExpecData<-sqlQuery(objDB,query=chaSQL)
  
  dtReferenceDate <- getIAMExpecData[(NROW(getIAMExpecData)-23):NROW(getIAMExpecData),2]
  iamZoo<-as.zoo(getIAMExpecData[(NROW(getIAMExpecData)-24):NROW(getIAMExpecData),4])
  iamZoo = as.data.frame(iamZoo/lag(iamZoo,-1)-1)
  
  getIAMExpecData <- data.frame(dtReferenceDate,iamZoo)
  colnames(getIAMExpecData) <- c("dtReferenceDate","IAM")
  
  getInflationExpecData<-merge(getFocusExpecData,getIAMExpecData,by='dtReferenceDate')
  colnames(getInflationExpecData)<- c("dtReferenceDate","Focus","IAM")
   
  return(getInflationExpecData)
  
}
getHolidaysAnbima<-function(objDB){
  
  #Escrevendo a query
    chaSQL<-"getHolidaysTable 'ANBIMA','BZ'"
  
  #Executando procedimento
    tmp<-sqlQuery(objDB,query=chaSQL)
      return(tmp$fltDateHoliday)
  
}
getHolidaysAnbimaSP<-function(objDB){
  
  #Escrevendo a query
  chaSQL<-"getHolidaysTable 'ANBIMA + SP','BZ'"
  
  #Executando procedimento
  tmp<-sqlQuery(objDB,query=chaSQL)
  return(tmp$fltDateHoliday)
  
}
getCurvesPoints <- function(dt,gridNSS_eval = NA,faceValue=100000) {

  dtVal = dateDictionary(dt)
  betas = getBetasNSS(ECOFIN.env$objConn,dtVal)
  currCurve = getUsedDataNSS(dtVal)
  
  matsCurve = currCurve$mty
  ratesCurve = currCurve$rates
  nssPlot <- function(mats) NSS(as.numeric(betas),mats)
  
  if (sum(is.na(gridNSS_eval))==0) {
    nGrid = 100
    gridNSS_eval = seq(from=10^-7,to=max(matsCurve),length.out = nGrid)
  }

  gridNSS = nssPlot(gridNSS_eval)
  
  vctDt = currCurve$vctDt
  
  #Obtaining prices
  gridNSS_pu = faceValue/((1+gridNSS)^gridNSS_eval) #rate is annualized but grid is already on the relevant time unit
  
  invisible(list(numericalNSS = gridNSS, numericalNSS_mty = gridNSS_eval, numericalPU = gridNSS_pu,
                  actualMty = matsCurve, actualRates = ratesCurve,actualVct = vctDt))
                 
  
}
getCOPOMMeetingDates<-function(objDB){
  
  #Escrevendo a query
  chaSQL<-"getCOPOMEffDate";
  
  #Executando procedimento
  tmp <- sqlQuery(objDB,query=chaSQL);
  return(as.Date(tmp$meeting_date));
  
}
##Consultar dado CDI-CETIP para um range de datas
getCDICETIPData<-function(objDB,dt1,dt2)
  {
    if (is.null(dt2)==FALSE){
      #Escrevendo a query
        chaSQL<-paste('getCDICETIP', as.character(dt1),',',as.character(dt2))
      
      #Executando procedimento
        getCDICETIPData<-sqlQuery(objDB,query=chaSQL)
      
      #Saída da função
        return(getCDICETIPData)
      
    }else{
      #Escrevendo a query
        chaSQL<-paste('getCDICETIP', as.character(dt1))
      
      #Executando procedimento
        getCDICETIPData<-sqlQuery(objDB,query=chaSQL)
      
      #Saída da função
        return(getCDICETIPData)  
    }
  }
  
  ##Consultar curva de juros para uma data especifica - BMF Ajuste
getBMFDIData<-function(objDB,dt1,dt2=NULL)
{
  if (is.null(dt2)==FALSE){
    #Escrevendo a query
      chaSQL<-paste('getBMFBD_Data_DIFuturo', as.character(dt1),',',as.character(dt2))
    
    #Executando procedimento
      getBMFDIData<-sqlQuery(objDB,query=chaSQL)
    
    #Saída da função
      return(getBMFDIData)

  }else{
    #Escrevendo a query
    chaSQL<-paste('getBMFBD_Data_DIFuturo', as.character(dt1))
    
    #Executando procedimento
    getBMFDIData<-sqlQuery(objDB,query=chaSQL)
    
    #Saída da função
    return(getBMFDIData)  
  }
}
getBetasNSS <- function(objDB=-ECOFIN.env$objConn,dt1,type = "nominal") {
  
  intModelRef = 1
  if (type=="real") {
    intModelRef = 3
  }
  
  dtFmt = dt1
  if (class(dt1) != "numeric") {
    dtFmt = dateDictionary(dt1)
  }
  chaSQL<-paste('getBetas', dtFmt,",",intModelRef)
  
  #Executando procedimento
  getBetasData<-sqlQuery(objDB,query=chaSQL)
  
  return(getBetasData[1,])
  
}
genRatesForward_TS <- function(matLines,objDB = ECOFIN.env$objConn,type="nominal") {
  
  #Obtaining dates in SQL
  chaSQL = paste('SELECT DISTINCT [fltDtGerArq] FROM [DB_ECO_FIN].[dbo].[tb_bmf_bd_final]',
                  ' ORDER BY fltDtGerArq DESC',
                 sep="")
  datesDL = dateDictionary(as.numeric(unlist(sqlQuery(objDB,chaSQL))))
  
  dfFwd = list()
  noDataDates = c()
  dataDates = c()
  for (k in 1:length(datesDL)) {
    date = datesDL[k]
    dfCurr = genRatesForward(date)
    #Checa se tem dado para a data
    if (sum(is.na(dfCurr$fwdRt)) > 0) {
      noDataDates = c(noDataDates,as.character(date))
    } else {
      dfFwd = c(dfFwd,list(dfCurr))
      dataDates = c(dataDates,as.character(date))
    }
    
  }
  
  
  #Decomposing into Time Series
  colType = 2
  if (type == "real") {
    colType = 3
  }
  
  dfReturnList = list()
  for (matLine in matLines) {
    dfFwdTS = data.frame(date = character(length(dataDates)),nominal = numeric(length(dataDates)),
                          stringsAsFactors = FALSE)
    colnames(dfFwdTS) = c("date",paste(type,"fwd_rate"))
    for (k in 1:length(dfFwd)) {
      currDF = dfFwd[[k]]
      dfFwdTS$date[k] = dataDates[k]
      dfFwdTS[k,2] = currDF[matLine,colType]
    }
    mat = currDF[matLine,1]
    dfFwdTS = data.frame(dfFwdTS,mat)
    names(dfFwdTS)[3] <- "Maturity in years"
    dfReturnList = c(dfReturnList,list(dfFwdTS))
  }
  
  return(dfReturnList)
  
}
genRatesForward <- function(dt,objDB = ECOFIN.env$objConn) {
  
  #Maturities
  mats = c(0.5,1,1.5,2) #c(1semester,1year,1.5years,2years)
  
  #Nominal
  # nominalRates = getNominalRates(dt,objDB)
  betasNSS = getBetasNSS(objDB,dt)
  nssFittedYield = NSS(as.numeric(betasNSS),mats)
  
  #Obtaining Forwards
  fwdRt = c(nssFittedYield[1])
  for (k in 1:(length(mats)-1)) {
    
    long = (1+nssFittedYield[k+1])^(mats[k+1])
    short = (1+nssFittedYield[k])^(mats[k])
    diffMat = mats[k+1] - mats[k]
    diffFwd = (long/short)^(1/diffMat)
    fwdRt = c(fwdRt,diffFwd-1)
    
  }
  
  #Real
  betasNSS = getBetasNSS(objDB,dt,"real")
  nssFittedYield = NSS(as.numeric(betasNSS),mats)
  
  #Obtaining Forwards
  fwdRtReal = c(nssFittedYield[1])
  for (k in 1:(length(mats)-1)) {
    
    long = (1+nssFittedYield[k+1])^(mats[k+1])
    short = (1+nssFittedYield[k])^(mats[k])
    diffMat = mats[k+1] - mats[k]
    diffFwd = (long/short)^(1/diffMat)
    fwdRtReal = c(fwdRtReal,diffFwd-1)
    
  }
  
  return(data.frame(cbind(mats,fwdRt,fwdRtReal)))
  
}
genPremiumForward <- function(dt,objDB) {
  
  premiumDF = getInflationPrem(dt,objDB) 
  
  dates = premiumDF[,2]
  datesNum = seq(1,from=1,to=length(dates))
  premiums = as.numeric(premiumDF[,3])
  
  premiumFac = 1+premiums
  
  premiumFacCum = cumprod(premiumFac)
  premiumFacAnn = premiumFacCum^12
  premiumCum = premiumFacCum-1
  
  #Aqui tem que ver direitinho o fato de ter um mês de vencimento ANTERIOR ao mês atual
  premiumFacCumAnn = premiumFacCum^(12/datesNum)
  return(as.data.frame(cbind(dates,premiumCum)))
  
}
genInflationForward <- function(dt,objDB = ECOFIN.env$objConn) {
  
  chaSQL<-paste('SELECT * FROM [DB_ECO_FIN].[dbo].[tb_implied_in_market] WHERE dtDate = ',
                "'",as.character(dt),"' AND vchDataFrequency = 'M' AND intMethod = 1",
                sep = "")
  
  #Breakeven
  getInflationPremData<-sqlQuery(objDB,query=chaSQL)
  
  matsMonth = getInflationPremData[,5]
  matsNum = getInflationPremData[,4]
  
  breakevenMonthlyFac = (1+as.numeric(getInflationPremData[,6]))
  
  breakevenCumFac = cumprod(breakevenMonthlyFac)
  breakevenCumFacAnn = breakevenCumFac^(12/matsNum)
  
  breakevenCum = breakevenCumFac-1
  breakevenAnn = breakevenCumFacAnn-1
  
  return(as.data.frame(cbind(matsMonth,matsNum,breakevenCum)))
  
  
}
# See vicente, J; Graminho, Fl?via (2015) - "Decompondo a infla??o impl?cita"
fn_brazil_break_even_decomposition <- function(obj_conn, date, ipca_focus_to_get = 3){
  
##### Getting data #####
  # Getting data from Haver - IPCA in monthly basis
    #Considering inflation data after 2004
      ipca <- dlxGetData(varnames = "N223PI", database = "EMERGELA", start = "2004-01-01" ) / 100;
      ipca <- data.frame(dates = index(ipca), data = as.numeric(ipca));
        day(ipca$dates) <- 1;
  
  # Getting bcb - expectations
      str_sql <- paste("SELECT * FROM tb_bcb_expectativas WHERE dtDate = '", 
                       as.character(date), "' AND intCode = ", as.character(ipca_focus_to_get), 
                       " ORDER BY dtDate ASC, dtReferenceDate ASC;", sep = "");
      
    inflation_expectations <- sqlQuery(channel = obj_conn, query = str_sql);
      inflation_expectations$dtDate <- as.Date(inflation_expectations$dtDate);
      inflation_expectations$dtReferenceDate <- as.Date(inflation_expectations$dtReferenceDate);
    
  # Getting Break-Even Data
      str_sql <- paste("SELECT * FROM tb_implied_in_market WHERE dtDate = '", 
                       as.character(date), "' AND intMethod = 1", 
                       "ORDER BY dtDate ASC, dtReferenceDate ASC;", sep = "");
      
      break_even <- sqlQuery(channel = obj_conn, query = str_sql);
        break_even$dtDate <- as.Date(break_even$dtDate);
        break_even$dtReferenceDate <- as.Date(break_even$dtReferenceDate);
      
##### Making some adjustments #####
  # IPCa
        date_ref <- as.Date(date);
        day(date_ref) <- 1;
        ipca <- ipca[ipca$dates <= date_ref,];   
        
  # Adjusting the IPCA time series to not contemplate not released data 
      date_ref <- as.Date(date);
      day(date_ref) <- 15;
      date_ref <- adjust.next(dates = date_ref, cal = ECOFIN.env$calSQLAnbima);
      day(date_ref) <- day(date_ref) + 1; date_ref <- adjust.next(dates = date_ref,  cal = ECOFIN.env$calSQLAnbima);
      
        if (adjust.next(as.Date(date) + 1, cal = ECOFIN.env$calSQLAnbima)  < date_ref){
          day(date_ref) <- 1;
            ipca <- ipca[ipca$dates < date_ref,]; 
          
        }else{
          day(date_ref) <- 1;
            ipca <- ipca[ipca$dates <= date_ref,]; 
        }
      
  #If exists an expectation.... using it!
      if (min(inflation_expectations$dtReferenceDate) == max(ipca$dates)){
        ipca <- ipca[1:length(ipca$dates)-1,];   
      }else{
        
        temp_df <- data.frame(intCode = 3, 
                              dtDate = date, 
                              dtReferencedate = last(ipca)[1], 
                              fltValue = last(ipca)[2]); names(temp_df) <- names(inflation_expectations);
        
        
        inflation_expectations <- rbind(temp_df, inflation_expectations);

      }
      
  # Adjusting bcb - expectations and break - even to have the same time horizon
    if (max(break_even$dtReferenceDate) < max(inflation_expectations$dtReferenceDate)){
      ref_date <- max(break_even$dtReferenceDate);
        break_even <- break_even[break_even$dtReferenceDate <= ref_date,];
        inflation_expectations <- inflation_expectations[inflation_expectations$dtReferenceDate <= ref_date,];
        
          rm("ref_date");
        
    }else if (max(break_even$dtReferenceDate) > max(inflation_expectations$dtReferenceDate)){
      ref_date <- max(inflation_expectations$dtReferenceDate);
        break_even <- break_even[break_even$dtReferenceDate <= ref_date,];
        inflation_expectations <- inflation_expectations[inflation_expectations$dtReferenceDate <= ref_date,];
        
          rm("ref_date");
        
    }
    
##### Projecting the monthly variance premium - Convexity component #####
      
  #Estimating the covariance through an AR(1) model
      ar_est <- ar.mle(x = log(1 + ipca$data), order.max = 1);
        theta <- ar_est$ar;
        sigma <- sqrt(ar_est$var.pred);
        
        # Calculating S1 ... Sk
          S         <- numeric(length(inflation_expectations$dtReferenceDate));
          var_t_pi  <- numeric(length(inflation_expectations$dtReferenceDate));
          convexity <- numeric(length(inflation_expectations$dtReferenceDate));
            
            for (k in 1:length(S)){
              S[k]         <- sum(rep(theta, k) ^ seq(from = 0, to = k-1, by = 1))^2;
              var_t_pi[k]  <- (sum(S)*(12/k) ^ 2 ) * sigma ^ 2;
              convexity[k] <- (1 / 2) * (k / 12) * var_t_pi[k];
            }
          
 
##### Inflation premium component #####         
  out <- list();        
  out$monthly_decomposition <- data.frame(date = rep(date, length(break_even$dtReferenceDate)),
                                          reference_month = break_even$dtReferenceDate,
                                          break_even = break_even$fltValue,
                                          expectations_component = inflation_expectations$fltValue,
                                          convexity_component = exp(convexity) - 1, 
                                          premium_component = exp(log(1 + break_even$fltValue) - log(1 +inflation_expectations$fltValue) - convexity) - 1);
          
          
          
        spot_be <- ECOFIN.env$fn_ipca_term_structure_given_future_inflation_path(ref_date = date, 
                                                                                 ipca = out$monthly_decomposition$break_even,
                                                                                 ipca_ref_months = out$monthly_decomposition$reference_month,
                                                                                 cal = ECOFIN.env$calSQLAnbima); 
                                          
        spot_expectations <- ECOFIN.env$fn_ipca_term_structure_given_future_inflation_path(ref_date = date, 
                                                                                 ipca = out$monthly_decomposition$expectations_component,
                                                                                 ipca_ref_months = out$monthly_decomposition$reference_month,
                                                                                 cal = ECOFIN.env$calSQLAnbima); 
        
        spot_convexity <- ECOFIN.env$fn_ipca_term_structure_given_future_inflation_path(ref_date = date, 
                                                                                           ipca = out$monthly_decomposition$convexity_component,
                                                                                           ipca_ref_months = out$monthly_decomposition$reference_month,
                                                                                           cal = ECOFIN.env$calSQLAnbima); 
        
        spot_premium <- ECOFIN.env$fn_ipca_term_structure_given_future_inflation_path(ref_date = date, 
                                                                                        ipca = out$monthly_decomposition$premium_component,
                                                                                        ipca_ref_months = out$monthly_decomposition$reference_month,
                                                                                        cal = ECOFIN.env$calSQLAnbima); 

                                          
                                                                            
          out$spot_curve_decomposition <- data.frame(dates = spot_be$dates, bus_days = spot_be$bus_days, break_even_spot = spot_be$spot_curve,
                                                     expectations_spot = spot_expectations$spot_curve, convexity_spot = spot_convexity$spot_curve,
                                                     inflation_premium_spot = spot_premium$spot_curve);
        
          # ggplot(data = bla, mapping = aes(x = bus_days)) + 
          #         geom_line(mapping = aes(y = break_even_spot, colour = "break-even")) + 
          #         geom_line(mapping = aes(y = expectations_spot, colour = "expectations")) + 
          #         geom_line(mapping = aes(y = convexity_spot, colour = "convexity")) + 
          #         geom_line(mapping = aes(y = inflation_premium_spot, colour = "inflpremium"))+
          #         theme(legend.position = "bottom", legend.text = element_text(size = 8)) + 
          #         scale_y_continuous(labels = scales::percent)
          #       
          #   

  }
loadMatDB <- function(reload = 0) {

  if (reload == 1) {
  
    require(R.matlab)
    library(Juros)
    source('Z:/ASSET/ECO/ECOFIN/Chartbook Juros (Marcelo)/Códigos/readPortSolMat.R', echo=TRUE)
    
    # Indices DI
    dtMETA <- ECOFIN::read_matfile("Z:\\ASSET_COMUM\\ECOFIN\\Database\\METADATA.mat",
                                   a=ECOFIN::aux_clear_whitespaces)
    dtTX = read.PortSolMat("Z:\\ASSET_COMUM\\ECOFIN\\Database\\DB_DI_TAXA.mat",FALSE)
    dtVOLUME = read.PortSolMat("Z:\\ASSET_COMUM\\ECOFIN\\Database\\DB_DI_VOLUME.mat",FALSE)
    dtPU = read.PortSolMat("Z:\\ASSET_COMUM\\ECOFIN\\Database\\DB_FUT_DI.mat",FALSE)
    dtPUNTNB = read.PortSolMat("Z:\\ASSET_COMUM\\ECOFIN\\Database\\DB_NTNB.mat",FALSE)
    dtTXNTNB = read.PortSolMat("Z:\\ASSET_COMUM\\ECOFIN\\Database\\DB_NTNB_TAXA.mat",FALSE)

    colnames(dtPUNTNB) <- gsub("\\.","_",names(dtPUNTNB))
    colnames(dtTXNTNB) <- gsub("\\.","_",names(dtTXNTNB))
    
    save(list=c('dtMETA','dtTX','dtVOLUME','dtPU','dtPUNTNB','dtTXNTNB'),
         file='Z:\\ASSET\\ECO\\ECOFIN\\Chartbook Juros (Marcelo)\\matDB\\matDB.Rdata')
    
  } else {
  
    load("Z:/ASSET/ECO/ECOFIN/Chartbook Juros (Marcelo)/matDB/matDB.RData")
    #dtMETA<<-dtMETA
    LIST <- list(dtMETA, dtPU, dtPUNTNB, dtTX, dtTXNTNB,dtVOLUME)
    NAMES <- c("dtMETA", "dtPU", "dtPUNTNB", "dtTX", "dtTXNTNB","dtVOLUME")
    invisible(lapply(seq_along(LIST), 
           function(x) {
             assign(NAMES[x], LIST[[x]], envir=.GlobalEnv)
           }
    )
    )

  }

}
downloadCurveReal <- function(dt) {
  
  #Downloads DB via .mat Files
  # source('Z:/ASSET/ECO/ECOFIN/Chartbook Juros (Marcelo)/Códigos/sourceAux/downloadMatDB.R', echo=TRUE)
  loadMatDB(0)
  
  inds = intersect(which(dtMETA$ASSET.GROUP=="BOND_CHAIN"),grep("NTNB",dtMETA$SHORT.NAME))
  names = dtMETA$SHORT.NAME[inds]
  mty = dtMETA$MATURITY[inds]
  metaAvailable =  match(names,names(dtTXNTNB))
  metaInds = index(metaAvailable[!is.na(metaAvailable)])
  metaAvailable = metaAvailable[!is.na(metaAvailable)]
  zoo = dtTXNTNB[,metaAvailable]
  pu = dtPUNTNB[,metaAvailable]
  mty = as.numeric(mty[metaInds])
  names = names[metaInds]
  
  d0 = dt
  d0_num = dateDictionary(d0)
  mty_dt = dateDictionary(mty)
  
  'Sorting maturities'
  mty_unn = mty_dt
  mty_dt = sort(mty_unn)
  sortInd = match(mty_dt,mty_unn)
  names = names[sortInd]
  mty = mty[sortInd]
  
  selPrices = match(names,names(pu))
  selPrices = selPrices[!is.na(selPrices)]
  pu = pu[,selPrices]
  
  selTx = match(names,names(zoo))
  selTx = selTx[!is.na(selTx)]
  zoo = zoo[,selTx]
  
  'Excluding expired contracts'
  active_inds = mty_dt>d0
  
  fltData = d0_num
  mty_dt = mty_dt[active_inds]
  names = names[active_inds]
  mty = mty[active_inds]
  
  fltVencimento = mty
  fltSaques = bizdays(d0,mty_dt)
  
  fltTxIndicativa = t(zoo[index(zoo)==d0,])
  fltPU = t(pu[index(pu)==d0,])
  vchSerie = names
  
  fltPU_m = data.frame(vchSerie=rownames(fltPU),fltPU)
  colnames(fltPU_m) = c("vchSerie","fltPU")
  fltTxIndicativa_m = data.frame(vchSerie=rownames(fltTxIndicativa),fltTxIndicativa)
  colnames(fltTxIndicativa_m) = c("vchSerie","fltTxIndicativa")
  
  objNew = data.frame(fltData,vchSerie,fltVencimento,fltSaques)
  objNew = merge(objNew,fltPU_m)
  objNew = merge(objNew,fltTxIndicativa_m)
  
  #Removing rows with NAs
  objNew = objNew[complete.cases(objNew),]
  
  #Sorting by Maturity
  objNew = objNew[order(objNew$fltSaques),]
  objNew$fltTxIndicativa = objNew$fltTxIndicativa/100
  
  tblCurve = objNew
  return(tblCurve)
  
}
deltasEst <- function(rf,factors) {

  #delta0 and delta1 Estimation for ACM Model
  
  regDelta <- lm(rf~1+factors)
  coeffs = coefficients(regDelta)
  delta0 = coeffs[1]
  delta1 = coeffs[-1]
  
  return(list(delta0=delta0,delta1=delta1))
  
}
dateDictionary <- function(dateInput) {
  
  #to do: tornar robusto ao tipo de data string
  
  if (class(dateInput)=="character" | class(dateInput) == "Date") {
    return(as.numeric(as.Date(dateInput)-as.Date("1899-12-30")))
  }
  
  if (class(dateInput)=="numeric") {
    return(as.Date(dateInput,origin = "1899-12-30"))
  }

  
}
#Copom Pricing Sequencial

copomPricing_fn <- function(dt,noMeetings = 5) {
  
  #Puxar dados BMF do SQL e determinar datas daí.
  # source('Z:/ASSET/ECO/ECOFIN/Chartbook Juros (Marcelo)/Códigos/matFiles.R')
  # source('Z:/ASSET/ECO/ECOFIN/Chartbook Juros (Marcelo)/Códigos/sourceAux/downloadMatDB.R', echo=TRUE)
  loadMatDB(0)
  
  # dt = 42780
  dtStr = dt
  dt = dateDictionary(dt)
  
  objDB = ECOFIN.env$objConn
  # chaSQL = paste('SELECT [fltDate],[fltRateAverage] FROM [DB_ECO_FIN].[dbo].[tb_cdi_cetip]',
  #                ' WHERE fltDate = ',dt,sep="")
  # cdi_on <- sqlQuery(objDB,query=chaSQL)
  # cdi_on = cbind(cdi_on,dt)
  # names(cdi_on) = c("date","rateAv","settlementDt")
  # 
  # names(cdi_on)
  # 
  # chaSQL = paste('SELECT [fltDate],[fltRateAverage] FROM [DB_ECO_FIN].[dbo].[tb_cdi_cetip]',
  #                ' WHERE fltDate = ',dt,sep="")
  # 
  # 
  # bmf_fut = getBMFDIData(objDB,dt)
  # bmf_tab = bmf_fut[,c(1,9,3)]
  # names(bmf_tab) = c("date","rateAv","settlementDt")
  # ratesTab = rbind(cdi_on,bmf_tab)
  
  tabDI <- downloadCurveDI(dateDictionary(dt))
  
  date = tabDI$fltData
  rateAv = tabDI$fltSettlementRate
  settlementDt = tabDI$fltVencimento
  settlementDt[1] = dt
  ratesTab = data.frame(date,rateAv,settlementDt)
  
  datesCntrct = ratesTab$settlementDt
  avgRates = ratesTab$rateAv
  datesCntrctStr = dateDictionary(datesCntrct)
  
  rateD0 = avgRates[1]
  
  #Copom Meetings
  copomDates = getCOPOMMeetingDates(objDB)
  currDate = dateDictionary(dt)
  
  nextMeetings = copomDates[which(copomDates>currDate)]
  
  #Anbima Calendar
  anbCal = ECOFIN.env$calSQLAnbimaSP
  
  #Excluding contracts expiring before any COPOM meeting
  #these rates should be very very similar to the overnight CDI rate.
  excludeCtr = which(datesCntrctStr<nextMeetings[1])
  excludeCtr = excludeCtr[-1]
  
  if (length(excludeCtr) > 0) {
    avgRates = avgRates[-excludeCtr]
    datesCntrct = datesCntrct[-excludeCtr]
    datesCntrctStr = datesCntrctStr[-excludeCtr]
  }
  
  # #Keeping the first maturity between COPOM dates only
  # ctrKeep = c()
  # ratesPos = c()
  # contractAfterMeeting = 1
  # for (k in 1:noMeetings) {
  #   currMeeting = nextMeetings[k]
  #   follMeeting = nextMeetings[k+1]
  #   datesFollCurrMeeting = datesCntrct[which(datesCntrctStr>=currMeeting)]
  #   datesBefFollMeeting = datesCntrct[which(datesCntrctStr<follMeeting)]
  #   interMeetings = intersect(datesFollCurrMeeting,datesBefFollMeeting)
  #   ratesPos = c(ratesPos,which(datesCntrctStr==dateDictionary(interMeetings[1])))
  #   ctNum = min(contractAfterMeeting,length(interMeetings))
  #   ctrKeep = c(ctrKeep,interMeetings[ctNum])
  # }
  # ctrKeep = dateDictionary(ctrKeep)
  # avgRates = avgRates[c(1,ratesPos)]
  # datesCntrctStr = datesCntrctStr[c(1,ratesPos)]
  
  #Calculating maturities in working days
  mtyRates = c()
  for (k in 1:length(avgRates)) {
    if (k==1) {
      currMty_wd = bizdays(datesCntrctStr[1],nextMeetings[1],anbCal)
      mtyRates = c(mtyRates,currMty_wd)
    } else {
      currMty_wd = bizdays(datesCntrctStr[1],datesCntrctStr[k],anbCal)
      mtyRates = c(mtyRates,currMty_wd)
    }
  }
  
  meetings = nextMeetings[seq(1,noMeetings)]
  mktYields = avgRates
  mktDates = datesCntrctStr
  cal = anbCal

  # source('Z:/ASSET/ECO/ECOFIN/Chartbook Juros (Marcelo)/Códigos/sourceAux/copomRecursive_BBG.R', echo=TRUE)
  # source('Z:/ASSET/ECO/ECOFIN/Chartbook Juros (Marcelo)/Códigos/sourceAux/likelihood_monPol_2Period.R', echo=TRUE)
  
  copomList = copomRecursive_BBG(meetings,mktYields,mktDates,cal)
  copomDelta = copomList$copomDelta
  methodID = 2
  for (l in 1:noMeetings) {
    ECOFIN.env$insertCOPOMMarketData(objDB=ECOFIN.env$objConn,methodID,dtStr,l,
                                   adjust.next(meetings[l]-1,cal=ECOFIN.env$calSQLAnbima),
                                   meetings[l],copomDelta[l]/(10^4))
  }
  
  # copomList = copomMultiPeriod(dtStr)
  # meetings = copomList$meetingVec
  # copomDelta = copomList$sol[1,]
  # noMeetings = length(meetings)
  # methodID = 2 
  # for (l in 1:noMeetings) {
  #   ECOFIN.env$insertCOPOMMarketData(objDB=ECOFIN.env$objConn,methodID,dtStr,l,
  #                                  adjust.next(meetings[l]-1,cal=ECOFIN.env$calSQLAnbima),
  #                                  meetings[l],copomDelta[l]/(10^4))
  # }
    
  return(copomList)
}
copomRecursive_BBG = function(meetings,mktYields,mktDates,cal) {
  
  #Replica precificação no estilo da BBG
  #Precisa passar todas as reuniões e yields
  
  copomImplied = c()
  copomDelta = c()
  d0 = mktDates[1]
  d0Rate = mktYields[1]
  for (k in 1:length(meetings)) {
    
    currMeeting = meetings[k]
    dtPreMeet = mktDates[1]
    if (k == 1) {
      currON = mktYields[k]
      synthPricingAnn = currON
      onAdj = 1
    } else {
      onAdj = 0
      #Achando contrato que expira mais próximo da reunião, conforme BBG
      prevMeeting = meetings[k-1]
      #contratos antes da reunião atual
      datesCurrMeeting = mktDates[which(mktDates<currMeeting)]
      #contratos após a reunião anterior
      datesPrevMeeting = mktDates[which(mktDates>prevMeeting)]
      #contratos entre reuniões
      interMeetings = dateDictionary(intersect(dateDictionary(datesPrevMeeting),
                                               dateDictionary(datesCurrMeeting)))
      
      #constrói um contrato sintético para colocar no arcabouço recursivo
      deltaDaysMeeting = bizdays(d0,meetings[1],cal)
      deltaDaysMeetingd0 = deltaDaysMeeting
      synthRate = (1+d0Rate)^(deltaDaysMeeting/252)
      d0Period = synthRate
      synthMty = deltaDaysMeeting
      # if (k>1) {
      for (j in 1:(k-1)) {
        meetingRate = copomImplied[j]
        deltaDaysMeeting = bizdays(meetings[j],meetings[j+1],cal)
        synthRate = synthRate*(1+meetingRate)^(deltaDaysMeeting/252)
        synthMty = synthMty + deltaDaysMeeting
        if (j < (k-1)) {
          d0Period = synthRate
          deltaDaysMeetingd0 = deltaDaysMeetingd0 + deltaDaysMeeting
        }
      }
      # }
      synthRateAnn = synthRate^(252/synthMty)
      
      #to do: verificando consistência de taxas entre reuniões com contratos
      # if (k > 1) {
      
      synthContractsAnn = c()
      synthContractsMty = c()
      prevMty = bizdays(d0,prevMeeting,cal)
      for (c in 1:length(interMeetings)) {
        interMty = bizdays(prevMeeting,interMeetings[c],cal)
        carryRate = copomImplied[length(copomImplied)]
        currSynth = d0Period*(1+carryRate)^(interMty/252)
        currMty = deltaDaysMeetingd0 + interMty
        synthContractsAnn = c(synthContractsAnn,currSynth^(252/currMty))
        synthContractsMty = c(synthContractsMty,currMty)
      }
      
      ratesInterMeetings = mktYields[match(interMeetings,mktDates)]
      pricingErrors = ((synthContractsAnn-1)-ratesInterMeetings)*100
      interMeetPos = which(pricingErrors==min(pricingErrors))
      
      # }
      
      #Contrato sintético recursivo
      prevMeetingRate = copomImplied[k-1]
      lastDtSynth = interMeetings[interMeetPos] #pegando o último contrato entre reuniões
      residMty = bizdays(lastDtSynth,meetings[k],cal)
      synthPrev = synthContractsAnn[length(synthContractsAnn)]-1 #pode haver mais de um contrato intermeeting
      synthCarryMty = synthContractsMty[length(synthContractsMty)]
      
      # synthPricing = synthPrev
      # synthPricingMty = synthCarryMty
      synthPricing = ((1+synthPrev)^(synthCarryMty/252))*((1+prevMeetingRate)^(residMty/252))
      synthPricingMty = synthCarryMty+residMty
      synthPricingAnn = synthPricing^(252/synthPricingMty)-1
      # synthPricingAnn = synthRateAnn-1
    }
    
    #primeiro contrato após reunião sendo precificada
    datesPricingMeeting = mktDates[which(mktDates>currMeeting)][1]
    pricingCtPos = which(mktDates==datesPricingMeeting)
    ratePricing = mktYields[pricingCtPos]
    
    # if (k > 1) {
    #   synthPricingAnn = synthRateAnn-1
    # }
    
    # if (k > 1) {
    #   synthPricingAnn = synthContractsAnn[length(synthContractsAnn)]-1
    # }
    #optimize function for univariate problems
    fwRates = optimize(likelihood_monPol_2Period,c(-1,1),
                       meetings[k],synthPricingAnn,ratePricing,d0,datesPricingMeeting,cal,
                       onAdj)
    
    if (k == 1) {
      copomDelta = c(copomDelta,(fwRates$minimum-d0Rate)*10^4)
    } else {
      copomDelta = c(copomDelta,(fwRates$minimum-copomImplied[k-1])*10^4)
    }
    
    copomImplied = c(copomImplied,fwRates$minimum)
    
  }
  
  return(list(copomImplied=copomImplied,copomDelta=copomDelta))
  
}
#Copom Pricing Sequencial

copomPricing_multiPeriod_fn <- function(dt,noMeetings = 5) {
  
  loadMatDB(0)
  
  dtStr = dt
  dt = dateDictionary(dt)
  
  objDB = ECOFIN.env$objConn
  
  #FONTE ANTIGA DE DADOS
  # chaSQL = paste('SELECT [fltDate],[fltRateAverage] FROM [DB_ECO_FIN].[dbo].[tb_cdi_cetip]',
  #                ' WHERE fltDate = ',dt,sep="")
  # cdi_on <- sqlQuery(objDB,query=chaSQL)
  # cdi_on = cbind(cdi_on,dt)
  # names(cdi_on) = c("date","rateAv","settlementDt")
  # 
  # names(cdi_on)
  # 
  # chaSQL = paste('SELECT [fltDate],[fltRateAverage] FROM [DB_ECO_FIN].[dbo].[tb_cdi_cetip]',
  #                ' WHERE fltDate = ',dt,sep="")
  # 
  # 
  # bmf_fut = getBMFDIData(objDB,dt)
  # bmf_tab = bmf_fut[,c(1,9,3)]
  # names(bmf_tab) = c("date","rateAv","settlementDt")
  # ratesTab = rbind(cdi_on,bmf_tab)
  
  tabDI <- downloadCurveDI(dateDictionary(dt))
  
  date = tabDI$fltData
  rateAv = tabDI$fltSettlementRate
  settlementDt = tabDI$fltVencimento
  settlementDt[1] = dt
  ratesTab = data.frame(date,rateAv,settlementDt)
  
  datesCntrct = ratesTab$settlementDt
  avgRates = ratesTab$rateAv
  datesCntrctStr = dateDictionary(datesCntrct)
  
  rateD0 = avgRates[1]
  
  #Copom Meetings
  copomDates = getCOPOMMeetingDates(objDB)
  currDate = dateDictionary(dt)
  
  nextMeetings = copomDates[which(copomDates>currDate)]
  
  #Anbima Calendar
  anbCal = ECOFIN.env$calSQLAnbimaSP
  
  #Excluding contracts expiring before any COPOM meeting
  #these rates should be very very similar to the overnight CDI rate.
  excludeCtr = which(datesCntrctStr<nextMeetings[1])
  excludeCtr = excludeCtr[-1]
  
  if (length(excludeCtr) > 0) {
    avgRates = avgRates[-excludeCtr]
    datesCntrct = datesCntrct[-excludeCtr]
    datesCntrctStr = datesCntrctStr[-excludeCtr]
  }
  
  # #Keeping the first maturity between COPOM dates only
  # ctrKeep = c()
  # ratesPos = c()
  # contractAfterMeeting = 1
  # for (k in 1:noMeetings) {
  #   currMeeting = nextMeetings[k]
  #   follMeeting = nextMeetings[k+1]
  #   datesFollCurrMeeting = datesCntrct[which(datesCntrctStr>=currMeeting)]
  #   datesBefFollMeeting = datesCntrct[which(datesCntrctStr<follMeeting)]
  #   interMeetings = intersect(datesFollCurrMeeting,datesBefFollMeeting)
  #   ratesPos = c(ratesPos,which(datesCntrctStr==dateDictionary(interMeetings[1])))
  #   ctNum = min(contractAfterMeeting,length(interMeetings))
  #   ctrKeep = c(ctrKeep,interMeetings[ctNum])
  # }
  # ctrKeep = dateDictionary(ctrKeep)
  # avgRates = avgRates[c(1,ratesPos)]
  # datesCntrctStr = datesCntrctStr[c(1,ratesPos)]
  
  #Calculating maturities in working days
  mtyRates = c()
  for (k in 1:length(avgRates)) {
    if (k==1) {
      currMty_wd = bizdays(datesCntrctStr[1],nextMeetings[1],anbCal)
      mtyRates = c(mtyRates,currMty_wd)
    } else {
      currMty_wd = bizdays(datesCntrctStr[1],datesCntrctStr[k],anbCal)
      mtyRates = c(mtyRates,currMty_wd)
    }
  }
  
  meetings = nextMeetings[seq(1,noMeetings)]
  mktYields = avgRates
  mktDates = datesCntrctStr
  cal = anbCal
  
  copomList = copomMultiPeriod(dtStr)
  meetings = copomList$meetingVec
  copomDelta = copomList$sol[1,]
  noMeetings = length(meetings)
  methodID = 3
  for (l in 1:noMeetings) {
    ECOFIN.env$insertCOPOMMarketData(objDB=ECOFIN.env$objConn,methodID,dtStr,l,
                                   adjust.next(meetings[l]-1,cal=ECOFIN.env$calSQLAnbima),
                                   meetings[l],copomDelta[l]/(10^4))
  }
  
  return(copomList)
}
copomPricing_fn_yuri <- function(dt,nIter = 1000) {
  
cat("\014");

dt1                        <- as.Date(dt);
dt2                        <- as.Date(dt);
timeHorizon                <- 20*252;
dayCountConventionBasis    <- 252;
durationRateShock          <- 0.01;
numberOfMeetingsToConsider <- 65;
methodID                   <- 1;

#Connecting to th SQL Server
ECOFIN.env$objConn <- ECOFIN.env$connectSQLServer(dbServer = ECOFIN.env$dbServer, dbName = ECOFIN.env$dbName); 

#Getting DI1 data
dfSQLOutputDI1FUT               <- ECOFIN.env$getBMFDIData(objDB = ECOFIN.env$objConn, 
                                                           dt1 = ECOFIN.env$Date_yyyymmdd2xlDateNumeric(dt1), 
                                                           dt2 = ECOFIN.env$Date_yyyymmdd2xlDateNumeric(dt2));

dfSQLOutputDI1FUT$fltData       <- ECOFIN.env$xlDateNumeric2RDate_yyyymmdd(numDate = dfSQLOutputDI1FUT$fltData);
dfSQLOutputDI1FUT$fltVencimento <- ECOFIN.env$xlDateNumeric2RDate_yyyymmdd(numDate = dfSQLOutputDI1FUT$fltVencimento);

#Getting DI-Over data
dfSQLOutputDI1         <- ECOFIN.env$getCDICETIPData(objDB = ECOFIN.env$objConn, 
                                                     dt1 = ECOFIN.env$Date_yyyymmdd2xlDateNumeric(dt1), 
                                                     dt2 = ECOFIN.env$Date_yyyymmdd2xlDateNumeric(dt2));

dfSQLOutputDI1$fltData <- ECOFIN.env$xlDateNumeric2RDate_yyyymmdd(numDate = dfSQLOutputDI1$fltData)

#Getting COPOM Meeting Dates
copomMeetingDates <- ECOFIN.env$getCOPOMMeetingDates(objDB = ECOFIN.env$objConn);  

#Closing SQL server connection
  # odbcClose(ECOFIN.env$objConn)
  # rm(objConn, envir= ECOFIN.env )


#OPTIMIZATION PARAMETERS ##########################################################################
################################################################################################### 
model2Select=ECOFIN.env$expectedTermStructureGivenScenario
  
###################################################################################################  
#MAIN ROUTINE #####################################################################################
################################################################################################### 
curveDates<-unique(dfSQLOutputDI1FUT$fltData)
lsOutput<-list(dates=matrix(,nrow=0,ncol=1), OFvalue=matrix(,nrow=0,ncol=1),optTime=matrix(,nrow=0,ncol=1),mktExpectation=list())    

for (k in 1:length(curveDates)){
#Creating the vector of Bdates
  tmpSeqBDates<- bizseq(from=curveDates[k], to=ECOFIN.env$xlDateNumeric2RDate_yyyymmdd(ECOFIN.env$Date_yyyymmdd2xlDateNumeric(curveDates[k])+round(x=(timeHorizon/dayCountConventionBasis)*370,0))   ,cal=ECOFIN.env$calSQLAnbima);
    SeqBDatesM<- tmpSeqBDates[1:timeHorizon+1]; 
      rm(tmpSeqBDates); 
  
#Getting data for a specific date - BMF - DI1
  objTMPDI1 <- dfSQLOutputDI1FUT[dfSQLOutputDI1FUT$fltData==curveDates[k],1:dim(dfSQLOutputDI1FUT)[2]]
  
#Getting next meeting dates
  objTMPNextMeetingDates <- copomMeetingDates[copomMeetingDates>=curveDates[k]]
  objTMPNextMeetingDates <- objTMPNextMeetingDates[1:numberOfMeetingsToConsider]

#Eliminating if there is a contract that has number of business days less or equal than 1   
  objTMPDI1<-objTMPDI1[objTMPDI1$fltSaques>1,]
    
#Getting data for a specific date - CETIP - CDI
  objTMPCDI<-dfSQLOutputDI1[dfSQLOutputDI1$fltData==curveDates[k],1:size(dfSQLOutputDI1,2)]
  fltData<-objTMPCDI$fltData
  vchSerie<-"CDI"
  fltVencimento<-adjust.next(ECOFIN.env$xlDateNumeric2RDate_yyyymmdd(ECOFIN.env$Date_yyyymmdd2xlDateNumeric(objTMPCDI$fltData)+1),cal=ECOFIN.env$calSQLAnbima);
  fltSaques<-1
  fltOpenInterest<-0
  fltNumberTradesDay<-0
  fltNumberDealsDay<-0
  fltSettlementPrice<-100000/((1+objTMPCDI$fltTaxaMedia)^(1/252))
  fltSettlementRate<-objTMPCDI$fltTaxaMedia
  fltOpenRate<-0
  fltHighRate<-objTMPCDI$fltTaxaMaxima
  fltLowRate<-objTMPCDI$fltTaxaMinima
  fltLastRate<-0
  
  objTMPCDI<-data.frame(fltData,vchSerie,fltVencimento,fltSaques,fltOpenInterest,fltNumberTradesDay,fltNumberDealsDay,fltSettlementPrice,fltSettlementRate,fltOpenRate,fltHighRate,fltLowRate,fltLastRate)    
  objTMP<-rbind(objTMPCDI,objTMPDI1)
    rm(objTMPCDI,objTMPDI1, fltData,vchSerie,fltVencimento,fltSaques,fltOpenInterest,fltNumberTradesDay,fltNumberDealsDay,fltSettlementPrice,fltSettlementRate,fltOpenRate,fltHighRate,fltLowRate,fltLastRate)    
  
#BudgetBounds
  BudgetBounds<-list(min=-0.01*as.numeric(ones(nx =length(objTMPNextMeetingDates),ny=1)) ,
                     max=0.01*as.numeric(ones(nx =length(objTMPNextMeetingDates),ny=1)))

#DataList
  dataList<-list(yM=objTMP$fltSettlementRate,            #Settlement Rate    
                 mats=objTMP$fltSaques/252,              #Maturities
                 model=model2Select,                     #Model
                 effectiveDuration=ECOFIN.env$effDur(objTMP$fltSettlementPrice,objTMP$fltSettlementRate,objTMP$fltSaques/252,durationRateShock),
                 fltSettlementPrice=objTMP$fltSettlementPrice,
                 dtExpiry=ECOFIN.env$Date_yyyymmdd2xlDateNumeric(objTMP$fltVencimento),
                 fltNumberTradesDay=objTMP$fltNumberTradesDay,
                 fltNumberDealsDay=objTMP$fltNumberDealsDay,
                 SeqBDatesM=ECOFIN.env$Date_yyyymmdd2xlDateNumeric(SeqBDatesM),
                 dtNextMeetingDates=ECOFIN.env$Date_yyyymmdd2xlDateNumeric(objTMPNextMeetingDates),
                 dayCountConventionBasis=dayCountConventionBasis)
  
    
  #Defining the fitness ("objective-function")  Com penalty 
    f<-function(x,data=dataList) -ECOFIN.env$ExpectativaMercadoCopom_OF(x,data)
  
  #Optimizing      
    elpsTime<-system.time(sol<-ga(type="real-valued",fitness=f,popSize=5000, maxiter=nIter,min=BudgetBounds$min,max=BudgetBounds$max))
    intMethod=(as.matrix(seq(from=1, to=length(objTMPNextMeetingDates))) )

  #Connecting to th SQL Server
    ECOFIN.env$objConn<-ECOFIN.env$connectSQLServer(ECOFIN.env$dbServer,ECOFIN.env$dbName)  
    
  #Inserting results into the SQL server
    for (l in 1:length(sol@solution)){
      ECOFIN.env$insertCOPOMMarketData(objDB=ECOFIN.env$objConn,methodID,objTMP$fltData[1],intMethod[l],objTMPNextMeetingDates[l],
                                      adjust.next(objTMPNextMeetingDates[l]+1,cal=ECOFIN.env$calSQLAnbima),sol@solution[l])
      
    }
    
    #Closing SQL server connection
      # odbcClose(ECOFIN.env$objConn)
      # rm(objConn, envir= ECOFIN.env)

}

}
copomMultiPeriod = function(dt,noMeetings=8) {

  #Puxar dados BMF do SQL e determinar datas daí.
  # source('Z:/ASSET/ECO/ECOFIN/Chartbook Juros (Marcelo)/Códigos/matFiles.R')
  # source('Z:/ASSET/ECO/ECOFIN/Chartbook Juros (Marcelo)/Códigos/sourceAux/downloadMatDB.R', echo=TRUE)
  loadMatDB(0)
  
  # dt = 42780
  dtStr = dt
  dt = dateDictionary(dt)
  
  objDB = ECOFIN.env$objConn
  
  tabDI <- downloadCurveDI(dateDictionary(dt))
  
  date = tabDI$fltData
  rateAv = tabDI$fltSettlementRate
  settlementDt = tabDI$fltVencimento
  settlementDt[1] = dt
  ratesTab = data.frame(date,rateAv,settlementDt)
  
  datesCntrct = ratesTab$settlementDt
  avgRates = ratesTab$rateAv
  datesCntrctStr = dateDictionary(datesCntrct)
  
  rateD0 = avgRates[1]
  
  #Copom Meetings
  copomDates = getCOPOMMeetingDates(objDB)
  currDate = dateDictionary(dt)
  
  nextMeetings = copomDates[which(copomDates>currDate)]
  
  #Anbima Calendar
  anbCal = ECOFIN.env$calSQLAnbimaSP
  
  #Excluding contracts expiring before any COPOM meeting
  #these rates should be very very similar to the overnight CDI rate.
  excludeCtr = which(datesCntrctStr<nextMeetings[1])
  excludeCtr = excludeCtr[-1]
  
  if (length(excludeCtr) > 0) {
    avgRates = avgRates[-excludeCtr]
    datesCntrct = datesCntrct[-excludeCtr]
    datesCntrctStr = datesCntrctStr[-excludeCtr]
  }
  
  meetings = nextMeetings[seq(1,noMeetings)]
  mktDates = datesCntrctStr
  mktYields = avgRates
  
  mktDatesUnique = c.Date()
  for (k in 1:length(meetings)) {
    
    currMeeting = meetings[k]
    dateRateMeeting = mktDates[which(mktDates>currMeeting)][1]
    mktDatesUnique = c.Date(mktDatesUnique,dateRateMeeting)
    
  }
  
  mktYieldsUnique = mktYields[match(mktDatesUnique,mktDates)]*100
  d0Rate = rateD0*100
  d0 = dtStr
  cal = ECOFIN.env$calSQLAnbimaSP

  source('Z:/ASSET/ECO/ECOFIN/Chartbook Juros (Marcelo)/Códigos/sourceAux/likelihood_monPol_MultiPeriod.R', echo=TRUE)
  x = rep(0,noMeetings)
  # copomOpt = optim(x,likelihood_monPol_MultiPeriod,gr=NULL,meetings,mktYieldsUnique,mktDatesUnique,
                                                        # d0Rate,d0,cal)
  fitGA = function(x) -1*likelihood_monPol_MultiPeriod(x,meetings,mktYieldsUnique,mktDatesUnique,
                                                    d0Rate,d0,cal) 
  
  minVec = rep(-125,noMeetings)
  maxVec = rep(125,noMeetings)
  copomOpt = ga(type="real-valued",fitness=fitGA,min=minVec,max=maxVec,maxiter = 200)
  
  return(list(sol = copomOpt@solution,meetingVec = meetings))
    
}
## Conectar ao servidor SQL
connectSQLServer<-function(dbServer,dbName)
{
  library("RODBC") 
  connectSQLServer <- odbcDriverConnect(paste('driver={SQL Server};server=',dbServer,';database=',dbName,';trusted_connection=true',sep=""))
}
compoundingTrans <- function(rate) {
  
  #Transforms compounding rate to continous compounding provided they are in the same frequency
  return(log(1+rate))
  
}
checkCurveModelExistence<-function(objDB,mdlName){
  
  #Escrevendo a query
    chaSQL<-paste("SELECT intModelRef FROM tb_curve_models_meta WHERE vchModelName=" ,paste("'",mdlName,"'",sep="") ,sep=" ")    
  
  #Executando procedimento
    tmp<-sqlQuery(objDB,query=chaSQL)
      return(tmp$intModelRef)
  
}
checkBMFData <- function(dt=NULL,objDB=ECOFIN.env$objConn) {
  
  #Downloads DB via .mat Files
  loadMatDB(1)
  
  availableDates = index(dtTX)
  
  # chaSQL = paste('SELECT DISTINCT [fltDtGerArq] FROM [DB_ECO_FIN].[dbo].[tb_bmf_bd_final]',
  #                ' ORDER BY fltDtGerArq DESC',
  #                sep="")
  # 
  # availableDates = sqlQuery(objDB,chaSQL)
  
  return(availableDates)
  
}
#   dtFmt = dt
#   if (class(dt) == "Date") {
#     dtFmt = dateDictionary(dt)
#   }
#   
#   if ((dtFmt %in% availableDates$fltDtGerArq) == TRUE) {
#     return(TRUE)
#   } else {
#     return(FALSE)
#   }
#   
# }
breakevenDecomp_fn <- function(dt) { 

cat("\014");

 dt1                     <- as.Date(dt)
 dt2                     <- as.Date(dt) 

timeHorizon             <- 5;
dayCountConventionBasis <- 252;
array_timeHorizon       <- as.matrix(seq(from = 1, to = dayCountConventionBasis * timeHorizon, by = 1)) / dayCountConventionBasis;  
methodID <- 1;

#Vector of business dates
dates         <- ECOFIN.env$Date_yyyymmdd2xlDateNumeric(bizdays::bizseq(from = dt1, to = dt2, cal = ECOFIN.env$calSQLAnbima));
array_timeHorizon <- matrix(data = rep(x = array_timeHorizon, length(dates)), nrow = size(array_timeHorizon), byrow = FALSE); 

#Connecting to th SQL Server
ECOFIN.env$objConn <- ECOFIN.env$connectSQLServer(dbServer = ECOFIN.env$dbServer,dbName = ECOFIN.env$dbName);  

#Calculating the Break-Even Inflation Curve
be_inflation <- ECOFIN.env$fn_break_even_inflation_brasil_NSS(obj_conn = ECOFIN.env$objConn , dates = dates, time_horizon = array_timeHorizon);

#Closing SQL server connection
odbcClose(ECOFIN.env$objConn);
rm(objConn, envir = ECOFIN.env);

#OPTIMIZATION FUNCTION ##########################################################################
################################################################################################### 
model2Select <- ECOFIN.env$fn_ipca_term_structure_given_future_inflation_path;

###################################################################################################  
#MAIN ROUTINE #####################################################################################
################################################################################################### 
curveDates <- be_inflation$curve_dates;
lsOutput   <- list(dates=matrix(,nrow=0,ncol=1), OFvalue=matrix(,nrow=0,ncol=1),optTime=matrix(,nrow=0,ncol=1),mktExpectation=list());

for (k in 1:length(be_inflation$curve_dates)) {
  #Procedure Step Number
    print(sprintf(paste("Total number of steps:",  length(be_inflation$curve_dates), " Interation: %d", sep = ""), k)) ;
    flush.console();
    Sys.sleep(1);
  
  #Vector of dates
    mats_dates <- bizseq(from = adjust.next(curveDates[[k]] + 1, ECOFIN.env$calSQLAnbima),
                         to  = bizdays::add.bizdays(dates = adjust.next(curveDates[[k]] + 1, ECOFIN.env$calSQLAnbima), n = timeHorizon * dayCountConventionBasis - 1, cal = ECOFIN.env$calSQLAnbima), 
                         cal = ECOFIN.env$calSQLAnbima);
  
  #Creating the vector of months
  #Minimum date
    dt_ref_1 <- adjust.next(curveDates[[k]] + 1, cal = ECOFIN.env$calSQLAnbima);
    dt_ref_1_check <- dt_ref_1; day(dt_ref_1_check) <- 15; dt_ref_1_check <- adjust.next(adjust.next(dt_ref_1_check, cal = ECOFIN.env$calSQLAnbima) + 1, cal = ECOFIN.env$calSQLAnbima);
    
      if  (dt_ref_1 >= dt_ref_1_check){
        min_horizon_curve <- dt_ref_1;
        day(min_horizon_curve) <- 1; month(min_horizon_curve) <- month(min_horizon_curve); 
        
      }else{
        min_horizon_curve <- dt_ref_1;
        day(min_horizon_curve) <- 1; month(min_horizon_curve) <- month(min_horizon_curve) - 1;
      }
  
  #Maximum date
  dt_ref_1 <- max(mats_dates);
  dt_ref_1_check <- dt_ref_1; day(dt_ref_1_check) <- 15; dt_ref_1_check <- adjust.next(dt_ref_1_check , cal = ECOFIN.env$calSQLAnbima);
  
      if  (dt_ref_1 > dt_ref_1_check){
        max_horizon_curve <- dt_ref_1;
        month(max_horizon_curve) <- month(max_horizon_curve); day(max_horizon_curve) <- 1;
        
      }else{
        max_horizon_curve <- dt_ref_1;
        month(max_horizon_curve) <- month(max_horizon_curve) - 1; day(max_horizon_curve) <- 1;
      }
    

  #Vector of months
  vector_of_months <- seq(from = min_horizon_curve, to = max_horizon_curve, by = 1);
  vector_of_months <- vector_of_months[day(vector_of_months) == 1];
  
  #BudgetBounds
  BudgetBounds <- list(min = -0.2*as.numeric(ones(nx = length(vector_of_months), ny = 1)) ,
                       max = 0.2*as.numeric(ones(nx = length(vector_of_months), ny = 1)));
  
  #DataList
  dataList <- list(yM               = be_inflation$spot_b_e[[k]],         #Break_Even    
                   mats             = be_inflation$time_to_maturity[[k]], #Maturities
                   mats_dates       = mats_dates,
                   model            = model2Select,                       #Model
                   ref_date         = curveDates[[k]],                    #reference date
                   vector_of_months = vector_of_months,                   #Months ipca    
                   cal              = ECOFIN.env$calSQLAnbima);           #Anbima calendar
  
#Optimizing
  elps_time <- system.time(sol <- ECOFIN.env$expectativa_ipca_inflacao_implicita_sequencial(dataList));

#Connecting to th SQL Server
  ECOFIN.env$objConn <- ECOFIN.env$connectSQLServer(ECOFIN.env$dbServer,ECOFIN.env$dbName);  

#Inserting results into the SQL server
for (l in 1:length(sol$meses_referencia)){
  ECOFIN.env$insertBreakEvenMoMMarketData(objDB            = ECOFIN.env$objConn,
                               intMethod        = methodID,
                               dtDate           = be_inflation$curve_dates[[k]],
                               vchDataFrequency = "M",
                               intDataOrdering  = l,
                               dtReferenceDate  = sol$meses_referencia[l],
                               fltValue         = sol$inflacao_mensal[l])
  
}

}


}







 














betaMacroSurprise <- function(mty) {
  
library(xlsx)
library(lubridate)

datesRel = read.xlsx("Z:\\ASSET\\ECO\\ECOFIN\\Chartbook Juros (Marcelo)\\MacroSurpresas\\divulgMacro.xlsx",
                     sheetIndex = 1, colClasses = c("Date","Date","Date")) 

#IPCA 30 Medians and Actuals
ipca = importEviews("Z:\\ASSET\\ECO\\ECOFIN\\Chartbook Juros (Marcelo)\\Eviews\\macrosurprises.csv")
ipca = squareData(ipca)

surprise = ipca[,1] - ipca[,2]
relTime = datesRel$IPCA.30
relTime = relTime[complete.cases(relTime)]

calendar = ECOFIN.env$calSQLAnbimaSP

refMonthSer = c.Date()
deltaRate = c()
surpriseSer = c()

datesInd = 188:204

for (j in datesInd) { #índices das datas cuja curva foi estimada
  
  refMonth = datesRel$month_ref[j]
  currRel = relTime[j]
  preRel = add.bizdays(currRel,-1,calendar)
  if (is.na(preRel)) { #erro porque tem um release do IPCA em feriado da ANBIMA, por ex no dia 2003-07-09; usamos a data do dia seguinte...
    preRel = currRel - 1
    currRel = currRel + 1
    #checando que o currRel novo é bizday da ANBIMA
    while (is.bizday(currRel,calendar)==FALSE) {
      currRel = currRel+1
    }
  }
  currSurprise = surprise[refMonth]
  
  if (length(currSurprise) == 0) { #alguma data potencial sem expectativas; principalmente no histórico...
    currSurprise = NA
  }
  
  currFittedDI = getNSSPoint(currRel,mty)
  preFittedDI = getNSSPoint(preRel,mty)
  deltaRate = c(deltaRate,(preFittedDI - currFittedDI)*10^4) #in bps
  
  surpriseSer = c(surpriseSer,currSurprise*100) #in bps
  refMonthSer = c.Date(refMonthSer,refMonth)
  
}

#controles
us_10y = as.zoo(haver.data('daily:FCM10',rtype='zoo'))
us_10y_d = delPC(us_10y,dLevel = 1)*100 #in bps
snp = as.zoo(haver.data('daily:sp100',rtype='zoo'))
snp_d = delPC(snp,dPerc=1)*100 #in bps
vix = as.zoo(haver.data('daily:spvix',rtype='zoo'))
vix_d = delPC(vix,dPerc=1)*100 #in bps

controls = merge(us_10y_d,snp_d,vix_d)

refMonthSer = c.Date()
dfControls = data.frame()
'Obtendo controles nas datas de release'
for (j in datesInd) {
  
  refMonth = datesRel$month_ref[j]
  currRel = relTime[j]
  
  timeIndControls = which(index(controls)==currRel)
  currControls = controls[timeIndControls,]
  
  dfControls = rbind(dfControls,as.data.frame(currControls))
  refMonthSer = c.Date(refMonthSer,refMonth)
  
}
rownames(dfControls) <- refMonthSer

dfReg = cbind(data.frame(deltaRate,surpriseSer,row.names = refMonthSer),dfControls)
dfReg = dfReg[complete.cases(dfReg),]

#restringindo a amostra
# dfReg = dfReg[seq(30,nrow(dfReg)),]

surpReg = lm(deltaRate ~ 0 + surpriseSer + fcm10 + sp100 + spvix,data=dfReg)

currbetaSurp = surpReg$coefficients[[1]]

return(currbetaSurp)

}
affineCoefs <- function(lambda0,lambda1,delta0,delta1) {
  
  #Calculates An and Bn recursions to obtain coeffiicients of exponentially affine representation of prices.
  
  A0 = 0
  B0 = matrix(0,nrow=nFactors,ncol=1)
  # delta0 = -1*mean(log(aggPU)[,1]) #dá um delta0 maior que o estimado como no paper
  Ans = list()
  Bns = list()
  Bfirst = betaHatBold[,1]
  # delta1 = -1*Bfirst #é parecido com o delta estimado
  Afirst = -delta0
  # Ans = c(Ans,list(Afirst))
  
  #B(n)'s: eq 28
  Bns_betas = list()
  for (n in 1:nBetas) {
    Bns_betas = c(Bns_betas,list(betaHatBold[,n]))
  }
  
  for (n in 1:nBetas) {
    
    if (n == 1) {
      prevAn  = A0
      prevBn = B0
    }  else {
      prevAn = Ans[[n-1]]
      prevBn = Bns[[n-1]]
      # prevBn = Bns_betas[[n-1]]
    }
    #currBn = Bns_betas[[n]]
    currBn = t(t(prevBn)%*%(Phi-lambda1)-t(delta1))
    currAn = prevAn + t(prevBn)%*%(mu-lambda0) + 0.5*(t(prevBn)%*%sigma%*%prevBn+sigma2Hat) - delta0
    
    Ans = c(Ans,list(currAn))
    Bns = c(Bns,list(currBn))
  }
  
  return(list(Ans=Ans,Bns=Bns,Bns_betas=Bns_betas))
  
}
#Plot Prêmio de Inflação

dt_d0 = '2013-07-31'

expec = getInflationPrem(ECOFIN.env$objConn,dt_d0)

iam = expec[,seq(2,17)]
focus = expec[,seq(18,NCOL(expec))]
xPlt = seq(from=30,to=480,by=30)

yPlt = iam[1,]
plot(xPlt,yPlt,type="p",xlab="Maturity (days)",ylab="Rate",xaxt='n',col='orange',ylim = c(0.04,0.08))

yPlt = focus[1,]
points(xPlt,yPlt,type="p",xlab="Maturity (days)",ylab="Rate",xaxt='n',col='black')

axis(1,at = xPlt,labels=xPlt,tick=FALSE)

title(dt_d0)
legend("bottomright",legend=c('IAM','Focus'),col=c('orange,black'))
#Plot Expectativa de Inflação

dt_d0 = '2013-07-31'

expec = getInflationExpec(ECOFIN.env$objConn,dt_d0)

iam = expec[,seq(2,17)]
focus = expec[,seq(18,NCOL(expec))]
xPlt = seq(from=30,to=480,by=30)

yPlt = iam[1,]
plot(xPlt,yPlt,type="p",xlab="Maturity (days)",ylab="Rate",xaxt='n',col='orange',ylim = c(0.04,0.08))

yPlt = focus[1,]
points(xPlt,yPlt,type="p",xlab="Maturity (days)",ylab="Rate",xaxt='n',col='black')

axis(1,at = xPlt,labels=xPlt,tick=FALSE)

title(dt_d0)
legend("bottomright",legend=c('IAM','Focus'),col=c('orange,black'))
  
library(reshape)
longData = melt(expec)

write.xlsx(longData, "Z:/ASSET/ECO/ECOFIN/Chartbook Juros (Marcelo)/infExpData.xlsx",
           row.names = FALSE)

#Plota 1d, 1W, 1m Curva

# To do: determinar datas que existam na base
dt_d0 = 37130
dt_1w = dt_d0 - 7
dt_1m = dt_d0 - 28

dbName   <- 'db_eco_fin';
dbServer <- 'SPQUANT5-U';

#objConn <- connectSQLServer(dbServer, dbName)

betas_d0 = getBetasNSS(ECOFIN.env$objConn,dt_d0)
betas_1w = getBetasNSS(ECOFIN.env$objConn,dt_1w)

currCurve = getUsedDataNSS(dt_d0)
plotNSS(betas_d0,currCurve$mty,currCurve$rates,colGr="red")
data_d0 = data.frame(vct=currCurve$vctDt,rates=currCurve$rates,date=dt_d0)
currCurve = getUsedDataNSS(dt_1w)
plotNSS(betas_1w,currCurve$mty,currCurve$rates,addGr=TRUE)
df_1w = data.frame(mty=currCurve$mty,rates=currCurve$rates,"blue")
data_1w = data.frame(vct=currCurve$vctDt,rates=currCurve$rates,date=dt_1w)
legend("bottomright",legend=c(dt_d0,dt_1w))

dataCurves = rbind(data_d0,data_1w)
#Exporta Maturidades e Taxas
library(xlsx)
write.xlsx(dataCurves, "Z:/ASSET/ECO/ECOFIN/Chartbook Juros (Marcelo)/curvesData.xlsx",
           row.names = FALSE)
library(xlsx)
library(lubridate)

datesRel = read.xlsx("Z:\\ASSET\\ECO\\ECOFIN\\Chartbook Juros (Marcelo)\\MacroSurpresas\\divulgMacro.xlsx",
                     sheetIndex = 1, colClasses = c("Date","Date","Date")) 

#IPCA 30 Medians and Actuals
ipca = importEviews("Z:\\ASSET\\ECO\\ECOFIN\\Chartbook Juros (Marcelo)\\Eviews\\macrosurprises.csv")
ipca = squareData(ipca)

surprise = ipca[,1] - ipca[,2]
relTime = datesRel$IPCA.30
relTime = relTime[complete.cases(relTime)]

calendar = ECOFIN.env$calSQLAnbimaSP

refMonthSer = c.Date()
deltaRate = c()
surpriseSer = c()

for (j in 1:(length(datesRel$month_ref)-1)) { #-1 para não pegar releases que não ocorreram ainda
  
  refMonth = datesRel$month_ref[j]
  currRel = relTime[j]
  preRel = add.bizdays(currRel,-1,calendar)
  if (is.na(preRel)) { #erro porque tem um release do IPCA em feriado da ANBIMA, por ex no dia 2003-07-09; usamos a data do dia seguinte...
    preRel = currRel - 1
    currRel = currRel + 1
    #checando que o currRel novo é bizday da ANBIMA
    while (is.bizday(currRel,calendar)==FALSE) {
      currRel = currRel+1
    }
  }
  currSurprise = surprise[refMonth]
  
  if (length(currSurprise) == 0) { #alguma data potencial sem expectativas; principalmente no histórico...
    currSurprise = NA
  }
  
  #baixa curva um dia antes e no dia do release: o release do IPCA é 9:00, então a rigor deveriamos pegar preço de abertura
  diPrevCv =  downloadCurveDI(preRel)
  diPostCv = downloadCurveDI(currRel)
  
  #surpresa no três meses
  ctrctInd = 4
  ctrctName = diPrevCv$vchSerie[[ctrctInd]]
  ratePre = diPrevCv$fltSettlementRate[[ctrctInd]]
  
  ctrctInd_post = which(diPostCv$vchSerie==ctrctName)#só pra garantir que pega o mesmo contrato, o índice muito provavelmente será o mesmo
  ratePost = diPostCv$fltSettlementRate[[ctrctInd_post]]
  
  deltaRate = c(deltaRate,(ratePre - ratePost)*10^4) #in bps
  surpriseSer = c(surpriseSer,currSurprise*100) #in bps
  refMonthSer = c.Date(refMonthSer,refMonth)

}

#controles
source('Z:/ASSET/ECO/LATAM/MX/Codes/R/delPC.R', echo=TRUE)
us_10y = as.zoo(haver.data('daily:FCM10',rtype='zoo'))
us_10y_d = delPC(us_10y,dLevel = 1)*100 #in bps
snp = as.zoo(haver.data('daily:sp100',rtype='zoo'))
snp_d = delPC(snp,dPerc=1)*100 #in bps
vix = as.zoo(haver.data('daily:spvix',rtype='zoo'))
vix_d = delPC(vix,dPerc=1)*100 #in bps

controls = merge(us_10y_d,snp_d,vix_d)

refMonthSer = c.Date()
dfControls = data.frame()
'Obtendo controles nas datas de release'
for (j in 1:(length(datesRel$month_ref)-1)) {
 
  refMonth = datesRel$month_ref[j]
  currRel = relTime[j]
  
  timeIndControls = which(index(controls)==currRel)
  currControls = controls[timeIndControls,]
  
  dfControls = rbind(dfControls,as.data.frame(currControls))
  refMonthSer = c.Date(refMonthSer,refMonth)
  
}
rownames(dfControls) <- refMonthSer
  
dfReg = cbind(data.frame(deltaRate,surpriseSer,row.names = refMonthSer),dfControls)
dfReg = dfReg[complete.cases(dfReg),]

#restringindo a amostra
dfReg = dfReg[seq(30,nrow(dfReg)),]

surpReg = lm(deltaRate ~ 0 + surpriseSer + fcm10 + sp100 + spvix,data=dfReg)

betaSurp = surpReg$coefficients[[1]]

#Fixed Maturity using Estimated NSS

nGrid = 100
betasSurp = c()
mtyGrid = seq(from=10^-7,to=10,length=nGrid)

count=1

for (indMty in mtyGrid) {

mty = indMty

library(xlsx)
library(lubridate)

datesRel = read.xlsx("Z:\\ASSET\\ECO\\ECOFIN\\Chartbook Juros (Marcelo)\\MacroSurpresas\\divulgMacro.xlsx",
                     sheetIndex = 1, colClasses = c("Date","Date","Date")) 

#IPCA 30 Medians and Actuals
ipca = importEviews("Z:\\ASSET\\ECO\\ECOFIN\\Chartbook Juros (Marcelo)\\Eviews\\macrosurprises.csv")
ipca = squareData(ipca)

surprise = ipca[,1] - ipca[,2]
relTime = datesRel$IPCA.30
relTime = relTime[complete.cases(relTime)]

calendar = ECOFIN.env$calSQLAnbimaSP

refMonthSer = c.Date()
deltaRate = c()
surpriseSer = c()

datesInd = 188:204

for (j in datesInd) { #índices das datas cuja curva foi estimada
  
  refMonth = datesRel$month_ref[j]
  currRel = relTime[j]
  preRel = add.bizdays(currRel,-1,calendar)
  if (is.na(preRel)) { #erro porque tem um release do IPCA em feriado da ANBIMA, por ex no dia 2003-07-09; usamos a data do dia seguinte...
    preRel = currRel - 1
    currRel = currRel + 1
    #checando que o currRel novo é bizday da ANBIMA
    while (is.bizday(currRel,calendar)==FALSE) {
      currRel = currRel+1
    }
  }
  currSurprise = surprise[refMonth]
  
  if (length(currSurprise) == 0) { #alguma data potencial sem expectativas; principalmente no histórico...
    currSurprise = NA
  }
  
  currFittedDI = getNSSPoint(currRel,mty)
  preFittedDI = getNSSPoint(preRel,mty)
  deltaRate = c(deltaRate,(preFittedDI - currFittedDI)*10^4) #in bps

  surpriseSer = c(surpriseSer,currSurprise*100) #in bps
  refMonthSer = c.Date(refMonthSer,refMonth)
  
}

#controles
us_10y = as.zoo(haver.data('daily:FCM10',rtype='zoo'))
us_10y_d = delPC(us_10y,dLevel = 1)*100 #in bps
snp = as.zoo(haver.data('daily:sp100',rtype='zoo'))
snp_d = delPC(snp,dPerc=1)*100 #in bps
vix = as.zoo(haver.data('daily:spvix',rtype='zoo'))
vix_d = delPC(vix,dPerc=1)*100 #in bps

controls = merge(us_10y_d,snp_d,vix_d)

refMonthSer = c.Date()
dfControls = data.frame()
'Obtendo controles nas datas de release'
for (j in datesInd) {
  
  refMonth = datesRel$month_ref[j]
  currRel = relTime[j]
  
  timeIndControls = which(index(controls)==currRel)
  currControls = controls[timeIndControls,]
  
  dfControls = rbind(dfControls,as.data.frame(currControls))
  refMonthSer = c.Date(refMonthSer,refMonth)
  
}
rownames(dfControls) <- refMonthSer

dfReg = cbind(data.frame(deltaRate,surpriseSer,row.names = refMonthSer),dfControls)
dfReg = dfReg[complete.cases(dfReg),]

#restringindo a amostra
# dfReg = dfReg[seq(30,nrow(dfReg)),]

surpReg = lm(deltaRate ~ 0 + surpriseSer + fcm10 + sp100 + spvix,data=dfReg)

currbetaSurp = surpReg$coefficients[[1]]
betasSurp = c(betasSurp,currbetaSurp)

print(count/nGrid)
count=count+1

}

#Obtaining Fitted Curve with Known Surprise Ex-Ante

testDate = '2017-06-08' #pre surprise
fittedCurvePre = getCurvesPoints(testDate,gridNSS_eval = mtyGrid)

macroSurprise = -16 #in bps
#o sinal de menos é pela convenção como eu defini o deltaRate, sendo antes MENOS depois
fittedPostCurve = ((fittedCurvePre$numericalNSS*10^4) - betasSurp*macroSurprise)/10^4

actualCurve =  getCurvesPoints('2017-06-09',gridNSS_eval = mtyGrid)

curvesExp = data.frame(fittedCurvePre$numericalNSS_mty,fittedCurvePre$numericalNSS,
                       actualCurve$numericalNSS_mty,actualCurve$numericalNSS,
                       fittedCurvePre$numericalNSS_mty,fittedPostCurve)
write.xlsx(curvesExp,"C:\\Users\\teixjar\\Desktop\\fittedCurve.xlsx")
curvesActualExp = data.frame(actualCurve$actualMty,actualCurve$actualRates)
write.xlsx(curvesActualExp,"C:\\Users\\teixjar\\Desktop\\actualCurve.xlsx")

betasExp = data.frame(mtyGrid,betasSurp)
write.xlsx(betasExp,"C:\\Users\\teixjar\\Desktop\\betasMty.xlsx")

#Obtaining Fitted Value for Specific Contract
curvePreDate = downloadCurveDI(testDate)
chosenContract = which(curvePreDate[,1]=="DI1F19")
contractPricing = curvePreDate[chosenContract,]
contractPricingMty = contractPricing$fltSaques/252
#Estimating Beta for this Maturity
betaEst = betaMacroSurprise(contractPricingMty)
#Effect on given contract
fittedPreContract = contractPricing$fltSettlementRate
fittedPostContract = ((fittedPreContract*10^4) - betaEst*macroSurprise)/10^4


##########

# Actual Curve - to evaluate fitted macro effects
date = '2017-06-09'
preDate = '2017-06-08'
fittedCurveDate = getCurvesPoints(date,nGrid=500)
fittedCurvePreDate = getCurvesPoints(preDate,nGrid=500)
curvesFittedExp = data.frame(fittedCurveDate$numericalNSS_mty,fittedCurveDate$numericalNSS,
                             fittedCurvePreDate$numericalNSS_mty,fittedCurvePreDate$numericalNSS)
write.xlsx(curvesFittedExp,"C:\\Users\\teixjar\\Desktop\\fittedCurve.xlsx")
curvesActualExp = data.frame(fittedCurveDate$actualMty,fittedCurveDate$actualRates,
                             fittedCurvePreDate$actualMty,fittedCurvePreDate$actualRates)
write.xlsx(curvesActualExp,"C:\\Users\\teixjar\\Desktop\\actualCurve.xlsx")

#Required Packages
require("bizdays");
require("GA");
require("RODBC");
require("Rcpp");
require("Quandl");
require("ggplot2");
require('optimbase');
require("lubridate");
require("vars");
require("tsDyn");
require("xts");
require("chron");
require("scales");
require("gridExtra");
require("MTS");
require("plm");
require("Matrix");
require("DataCombine");
require("lmtest");
require("sandwich");
require("dlm");
require("xtable");
require("moments");
require("doParallel");
require("parallel");
require("devEMF");
require("reshape2");
require("ggthemes");
require("excel.link");
require("XML");
require("Haver");
require("methods");

#Set Working directory
setwd("\\\\fswcorp\\WMSASSET\\ASSET\\ECO\\Yuri\\R\\Functions")

#Defining th ECOFIN Enveironment
ECOFIN.env<-new.env(parent=globalenv())

#Basic Set of Packages
ECOFIN.env$vchPathRFiles<-"\\\\fswcorp\\WMSASSET\\ASSET\\ECO\\Yuri\\R\\Functions\\";

source(paste(ECOFIN.env$vchPathRFiles,'dateManipulation.R',sep = ""),local=ECOFIN.env);
source(paste(ECOFIN.env$vchPathRFiles,'dbManipulation.R',sep = ""));
source(paste(ECOFIN.env$vchPathRFiles,'optimization.R',sep = ""));
source(paste(ECOFIN.env$vchPathRFiles,'pricingFI.R',sep = ""));
source(paste(ECOFIN.env$vchPathRFiles,'SQL.R',sep = ""));
source(paste(ECOFIN.env$vchPathRFiles,'timeSeriesAnalysis.R',sep = ""));

varGlobNames<-(ls(.GlobalEnv));
varGlobNames<-varGlobNames[1-as.numeric(varGlobNames=="ECOFIN.env")==1];

for (k in 1:length(varGlobNames)){
  assign(varGlobNames[k], match.fun(varGlobNames[k]),envir = ECOFIN.env);
  rm(list=varGlobNames[k]);
}

rm(list=c("k","varGlobNames"));


#Creating SERVER connection
ECOFIN.env$dbServer<-"spquant5-u" ;
ECOFIN.env$dbName<-"DB_ECO_FIN";
ECOFIN.env$objConn<-ECOFIN.env$connectSQLServer(ECOFIN.env$dbServer,ECOFIN.env$dbName)

#Getting Anbima?s holidays table
holidaysAnbima<-ECOFIN.env$getHolidaysAnbima(ECOFIN.env$objConn)
ECOFIN.env$calSQLAnbima<-create.calendar(name="SQLAnbima",holidays=ECOFIN.env$xlDateNumeric2RDate_yyyymmdd(holidaysAnbima),
                                         start.date = "1950-01-01", end.date ="2078-12-25",weekdays=c("saturday", "sunday"))

rm(list="holidaysAnbima")

#Getting Anbima?s SP holidays table
holidaysAnbimaSP<-ECOFIN.env$getHolidaysAnbimaSP(ECOFIN.env$objConn)
ECOFIN.env$calSQLAnbimaSP<-create.calendar(name="SQLAnbimaSP",holidays=ECOFIN.env$xlDateNumeric2RDate_yyyymmdd(holidaysAnbimaSP),
                                           start.date = "1950-01-01", end.date ="2078-12-25",weekdays=c("saturday", "sunday"))

rm(list="holidaysAnbimaSP")


#xtables - features
options(xtable.floating = FALSE)
options(xtable.timestamp = "")

#Haver Analytics
library("Haver")
haver.path(set="\\\\fswcorp\\WMSASSET\\ASSET_COMUM\\HAVER\\DATA")

#Closing the Opened
#odbcClose(ECOFIN.env$objConn)
#rm(objConn, envir= ECOFIN.env )
library(xlsx)
library(plyr)

#Pega dados gerados pelos algoritmos no SQL para montar o relatório.
listCurves = list()
listExp = list()
listCOPOM = list()
listPrem = list()
listInfFwd = list()
listRtFwd = list()
listPremFwd = list()
pllCounter = 1

for (date in datesDaily) {
  
  dtFocus = datesDailyFocus[pllCounter]
  
  listPrem = c(listPrem,list(getInflationPrem(date,dtFocus)))
  listCurves = c(listCurves,list(getCurvesPoints(date)))
  listExp = c(listExp,list(getInflationExpec(dtFocus)))
  listCOPOM = c(listCOPOM,list(getMarketCOPOMPricing(date)))
  listInfFwd = c(listInfFwd,list(genInflationForward(date)))
  listRtFwd = c(listRtFwd,list(genRatesForward(date)))
  listPremFwd = c(listPremFwd,list(genPremiumForward(date)))
  
  pllCounter = pllCounter + 1
}
names(listCurves) = datesDaily
names(listExp) = datesDaily
names(listCOPOM) = datesDaily
names(listPrem) = datesDaily

#Premium Time-Series
premTS = list()
inflationPrem_tsDt = c('2017-06-01','2017-12-01','2018-06-01')
for (date in inflationPrem_tsDt){
  premTS = c(premTS,list(getInflationPrem_TS(date)))
}
write.xlsx(premTS[[1]], paste("Z:/ASSET/ECO/ECOFIN/Chartbook Juros (Marcelo)/rawData/premTS.xlsx",sep=""),
           row.names = FALSE)
#Forwards
write.xlsx(listInfFwd[[1]], paste("Z:/ASSET/ECO/ECOFIN/Chartbook Juros (Marcelo)/rawData/infFwd.xlsx",sep=""),row.names = FALSE)
write.xlsx(listRtFwd[[1]], paste("Z:/ASSET/ECO/ECOFIN/Chartbook Juros (Marcelo)/rawData/rtFwd.xlsx",sep=""),row.names = FALSE)
write.xlsx(listPremFwd[[1]], paste("Z:/ASSET/ECO/ECOFIN/Chartbook Juros (Marcelo)/rawData/premFwd.xlsx",sep=""),row.names = FALSE)

fwdTS = genRatesForward_TS(c(1,2,3,4),ECOFIN.env$objConn,"nominal")

longFwd = ldply(fwdTS)
write.xlsx(longFwd, paste("Z:/ASSET/ECO/ECOFIN/Chartbook Juros (Marcelo)/rawData/fwdTS.xlsx",sep=""),row.names = FALSE)

#Organizando output
unlink("Z:/ASSET/ECO/ECOFIN/Chartbook Juros (Marcelo)/rawData/*")

for (currDate in datesDaily) {
  
  #Estimated NSS Curve
  eval(parse(text=paste("nssNumerical_y = listCurves$`",currDate,"`$numericalNSS",sep="")))
  eval(parse(text=paste("nssNumerical_x = listCurves$`",currDate,"`$numericalNSS_mty",sep="")))
  dfNSS = data.frame(nssNumerical_y,nssNumerical_x)
  write.xlsx(dfNSS, paste("Z:/ASSET/ECO/ECOFIN/Chartbook Juros (Marcelo)/rawData/numericalNSS_",currDate,".xlsx",sep=""),
             row.names = FALSE)
  write.csv(dfNSS, file = paste("Z:/ASSET/ECO/ECOFIN/Chartbook Juros (Marcelo)/rawData/numericalNSS_",currDate,".csv",sep=""))
  
  #Actual Yield curve points
  eval(parse(text=paste("curve_y = listCurves$`",currDate,"`$actualRates",sep="")))
  eval(parse(text=paste("curve_x = listCurves$`",currDate,"`$actualMty",sep="")))
  eval(parse(text=paste("vct = listCurves$`",currDate,"`$actualVct",sep="")))
  dfActual = data.frame(curve_y,curve_x,vct)
  write.xlsx(dfActual, paste("Z:/ASSET/ECO/ECOFIN/Chartbook Juros (Marcelo)/rawData/curveActual_",currDate,".xlsx",sep=""),
             row.names = FALSE)
  write.csv(dfActual, file = paste("Z:/ASSET/ECO/ECOFIN/Chartbook Juros (Marcelo)/rawData/curveActual_",currDate,".csv",sep=""))
  
  'D0 Actual Curve'
  eval(parse(text=paste("vct = listCurves$`",currDate,"`$actualVct",sep="")))
  eval(parse(text=paste("rates = listCurves$`",currDate,"`$actualRates",sep="")))
  dateDisplay = dateDictionary(vct)
  dfTabRates = data.frame(dateDisplay,rates)
  write.csv(dfTabRates, file = paste("Z:/ASSET/ECO/ECOFIN/Chartbook Juros (Marcelo)/rawData/ratesTab_",currDate,".csv",sep=""))
  
  #Copom Pricing
  eval(parse(text=paste("dfCOPOM = data.frame(listCOPOM$`",currDate,"`$dtMeetingDate,listCOPOM$`",currDate,"`$fltChange)",sep="")))
  colnames(dfCOPOM) <- c("meeting",paste("movExp_",currDate,sep=""))
  write.xlsx(dfCOPOM, paste("Z:/ASSET/ECO/ECOFIN/Chartbook Juros (Marcelo)/rawData/copomPricing_",currDate,".xlsx",sep=""),
             row.names = FALSE)
  write.csv(dfCOPOM, file =  paste("Z:/ASSET/ECO/ECOFIN/Chartbook Juros (Marcelo)/rawData/copomPricing_",currDate,".csv",sep=""))
  
  
  #Inflation Premium
  eval(parse(text=paste("dfPrem = data.frame(listPrem$`",currDate,"`$df.dtReferenceDate,listPrem$`",currDate,"`$premiumData)",sep="")))
  colnames(dfPrem) <-c("maturity",paste("prem_",currDate,sep=""))
  write.xlsx(dfPrem, paste("Z:/ASSET/ECO/ECOFIN/Chartbook Juros (Marcelo)/rawData/infPremium_",currDate,".xlsx",sep=""),
             row.names = FALSE)
  write.csv(dfPrem, file = paste("Z:/ASSET/ECO/ECOFIN/Chartbook Juros (Marcelo)/rawData/infPremium_",currDate,".csv",sep=""))
  
  
  library("reshape")
  #Inflation Expectations
  eval(parse(text=paste("dfExp = listExp$`",currDate,"`",sep="")))
  #eval(parse(text=paste("dfExp = as.data.frame(melt(listExp$`",currDate,"`,id=\"dtReferenceDate\"))",sep="")))
  write.xlsx(dfExp, paste("Z:/ASSET/ECO/ECOFIN/Chartbook Juros (Marcelo)/rawData/infExp_",currDate,".xlsx",sep=""),
             row.names = FALSE)
  write.csv(dfExp, file = paste("Z:/ASSET/ECO/ECOFIN/Chartbook Juros (Marcelo)/rawData/infExp_",currDate,".csv",sep=""))
  
}
#Gera Relatório EcoFin Diário

rm(list=ls())

library(Marcelo)
source('Z:/ASSET/ECO/ECOFIN/Chartbook Juros (Marcelo)/Códigos/ecofinPack.R', echo=FALSE)
sourceEntireFolder('Z:\\ASSET\\ECO\\ECOFIN\\Chartbook Juros (Marcelo)\\Códigos\\sourceAux')

datesd0 = c('2017-07-14')

for (d0 in datesd0) {
  
if (is.bizday(d0,cal = ECOFIN.env$calSQLAnbimaSP)) {
  
# d0 = '2017-06-09' #yyyy-mm-dd
# while (is.bizday(d0,ECOFIN.env$calSQLAnbimaSP) == FALSE) {
#   d0 = add.bizdays(d0,-1,ECOFIN.env$calSQLAnbimaSP)
# }
dtPrevious = add.bizdays(d0,c(-5,-22),ECOFIN.env$calSQLAnbimaSP)

#dados do Focus disponíveis em D-1.
d0_Focus = as.character(add.bizdays(d0,-1,ECOFIN.env$calSQLAnbimaSP))

datesDailyFocus = c(d0_Focus,as.character(dtPrevious))
# dtPrevious = add.bizdays(d0,c(-1),ECOFIN.env$calSQLAnbimaSP)
datesDaily = c(d0,as.character(dtPrevious))

start.time <- Sys.time()

nIter = 1000 # as vezes coloco apenas uma iteração para as curvas de NSS só para testar

# availableDates = checkBMFData()
# datesSeq = seq.Date(as.Date('2016-01-01'),as.Date('2016-10-31'),"days")
# for (k in seq_along(datesSeq)) {
#   date = datesSeq[[k]]
#   dateNum = dateDictionary(date)
#   if (dateNum %in% availableDates) {
#     nssga_fn(date,100)
#     # print("nominal")
#     nssga_real_fn(date,100)
#   }
# }

for (date in datesDaily) {
  
    nssga_fn(date,nIter)
    nssga_real_fn(date,nIter)
    breakevenDecomp_fn(date)
    copomPricing_fn(date)
    copomPricing_multiPeriod_fn(date)
  }

}
}

end.time <- Sys.time()
time.taken <- end.time - start.time
time.taken

#Salva .xlsx relevantes
source('Z:/ASSET/ECO/ECOFIN/Chartbook Juros (Marcelo)/Códigos/ecofinDaily_excel.R', echo=TRUE)
# ECOFIN.env$fn_ipca_term_structure_given_future_inflation_path() #calcula inflação acumulada anualizada dada uma base

cat("\014");

 dt1                     <- as.Date("2017-01-30")    #as.Date("2000-11-25")
 dt2                     <- as.Date("2017-01-30") 

timeHorizon             <- 5;
dayCountConventionBasis <- 252;
array_timeHorizon       <- as.matrix(seq(from = 1, to = dayCountConventionBasis * timeHorizon, by = 1)) / dayCountConventionBasis;  
methodID <- 1;

#Vector of business dates
dates         <- ECOFIN.env$Date_yyyymmdd2xlDateNumeric(bizdays::bizseq(from = dt1, to = dt2, cal = ECOFIN.env$calSQLAnbima));
array_timeHorizon <- matrix(data = rep(x = array_timeHorizon, length(dates)), nrow = size(array_timeHorizon), byrow = FALSE); 

#Connecting to th SQL Server
ECOFIN.env$objConn <- ECOFIN.env$connectSQLServer(dbServer = ECOFIN.env$dbServer,dbName = ECOFIN.env$dbName);  

#Calculating the Break-Even Inflation Curve
be_inflation <- ECOFIN.env$fn_break_even_inflation_brasil_NSS(obj_conn = ECOFIN.env$objConn , dates = dates, time_horizon = array_timeHorizon);

#Closing SQL server connection
odbcClose(ECOFIN.env$objConn);
rm(objConn, envir = ECOFIN.env);

#OPTIMIZATION FUNCTION ##########################################################################
################################################################################################### 
model2Select <- ECOFIN.env$fn_ipca_term_structure_given_future_inflation_path;

###################################################################################################  
#MAIN ROUTINE #####################################################################################
################################################################################################### 
curveDates <- be_inflation$curve_dates;
lsOutput   <- list(dates=matrix(,nrow=0,ncol=1), OFvalue=matrix(,nrow=0,ncol=1),optTime=matrix(,nrow=0,ncol=1),mktExpectation=list());

for (k in 1:length(be_inflation$curve_dates)){
  #Procedure Step Number
    print(sprintf(paste("Total number of steps:",  length(be_inflation$curve_dates), " Interation: %d", sep = ""), k)) ;
    flush.console();
    Sys.sleep(1);
  
  #Vector of dates
    mats_dates <- bizseq(from = adjust.next(curveDates[[k]] + 1, ECOFIN.env$calSQLAnbima),
                         to  = bizdays::add.bizdays(dates = adjust.next(curveDates[[k]] + 1, ECOFIN.env$calSQLAnbima), n = timeHorizon * dayCountConventionBasis - 1, cal = ECOFIN.env$calSQLAnbima), 
                         cal = ECOFIN.env$calSQLAnbima);
  
  #Creating the vector of months
  #Minimum date
    dt_ref_1 <- adjust.next(curveDates[[k]] + 1, cal = ECOFIN.env$calSQLAnbima);
    dt_ref_1_check <- dt_ref_1; day(dt_ref_1_check) <- 15; dt_ref_1_check <- adjust.next(adjust.next(dt_ref_1_check, cal = ECOFIN.env$calSQLAnbima) + 1, cal = ECOFIN.env$calSQLAnbima);
    
      if  (dt_ref_1 >= dt_ref_1_check){
        min_horizon_curve <- dt_ref_1;
        day(min_horizon_curve) <- 1; month(min_horizon_curve) <- month(min_horizon_curve); 
        
      }else{
        min_horizon_curve <- dt_ref_1;
        day(min_horizon_curve) <- 1; month(min_horizon_curve) <- month(min_horizon_curve) - 1;
      }
  
  #Maximum date
  dt_ref_1 <- max(mats_dates);
  dt_ref_1_check <- dt_ref_1; day(dt_ref_1_check) <- 15; dt_ref_1_check <- adjust.next(dt_ref_1_check , cal = ECOFIN.env$calSQLAnbima);
  
      if  (dt_ref_1 > dt_ref_1_check){
        max_horizon_curve <- dt_ref_1;
        month(max_horizon_curve) <- month(max_horizon_curve); day(max_horizon_curve) <- 1;
        
      }else{
        max_horizon_curve <- dt_ref_1;
        month(max_horizon_curve) <- month(max_horizon_curve) - 1; day(max_horizon_curve) <- 1;
      }
    

  #Vector of months
  vector_of_months <- seq(from = min_horizon_curve, to = max_horizon_curve, by = 1);
  vector_of_months <- vector_of_months[day(vector_of_months) == 1];
  
  #BudgetBounds
  BudgetBounds <- list(min = -0.2*as.numeric(ones(nx = length(vector_of_months), ny = 1)) ,
                       max = 0.2*as.numeric(ones(nx = length(vector_of_months), ny = 1)));
  
  #DataList
  dataList <- list(yM               = be_inflation$spot_b_e[[k]],         #Break_Even    
                   mats             = be_inflation$time_to_maturity[[k]], #Maturities
                   mats_dates       = mats_dates,
                   model            = model2Select,                       #Model
                   ref_date         = curveDates[[k]],                    #reference date
                   vector_of_months = vector_of_months,                   #Months ipca    
                   cal              = ECOFIN.env$calSQLAnbima);           #Anbima calendar
  
#Optimizing
  elps_time <- system.time(sol <- ECOFIN.env$expectativa_ipca_inflacao_implicita_sequencial(dataList));

#Connecting to th SQL Server
  ECOFIN.env$objConn <- ECOFIN.env$connectSQLServer(ECOFIN.env$dbServer,ECOFIN.env$dbName);  

#Inserting results into the SQL server
for (l in 1:length(sol$meses_referencia)){
  ECOFIN.env$insertBreakEvenMoMMarketData(objDB            = ECOFIN.env$objConn,
                               intMethod        = methodID,
                               dtDate           = be_inflation$curve_dates[[k]],
                               vchDataFrequency = "M",
                               intDataOrdering  = l,
                               dtReferenceDate  = sol$meses_referencia[l],
                               fltValue         = sol$inflacao_mensal[l])
  
}

#Closing SQL server connection
odbcClose(ECOFIN.env$objConn)
rm(objConn, envir= ECOFIN.env )
}










 














#ACM Model

rm(list=ls())

library(Marcelo)
source('Z:/ASSET/ECO/ECOFIN/Chartbook Juros (Marcelo)/Códigos/ecofinPack.R', echo=FALSE)
sourceEntireFolder('Z:\\ASSET\\ECO\\ECOFIN\\Chartbook Juros (Marcelo)\\Códigos\\sourceAux')

library(data.table)
library(vars)

#Panel Data constructed from NSS estimates curves
nGrid = 1000
mtyGrid = seq(from=1e-07,to=21*120/252,length.out = nGrid)
mtyFactors = seq(from=21*1/252,to=21*15/252,by=21*1/252)
mtyPanel = seq(from=21*1/252,to=21*10/252,by=21*1/252)
dates = seq.Date(as.Date('2015-01-01'),as.Date('2016-12-31'),"days")

mtyNames = paste0("n",mtyPanel)
pricingMty = mtyPanel[-1]

betaNamesDic = data.frame(mtyPanel[-1],seq(mtyPanel[-1]*252/21))
colnames(betaNamesDic) <- c("Years","n")
nBetas = NROW(betaNamesDic)

panelBonds = data.frame(matrix(NA,length(dates)+1,length(mtyPanel)))
panelBonds_pu = data.frame(matrix(NA,length(dates)+1,length(mtyPanel)))
colnames(panelBonds) = paste0("m.",mtyPanel)

panelGrid = panelMaturities(mtyPanel,dates,faceValue=1)
#Aggregating to monthly frequency
aggRates <- aggMonthly(zoo(panelGrid$rates,order.by=panelGrid$dates))
aggPU <- aggMonthly(zoo(panelGrid$prices,order.by=panelGrid$dates))
aggDates = index(aggPU)
datesSample = panelGrid$dates

#Obtaining Principal Components - Factors
panelFactors <- panelMaturities(mtyFactors,dates,faceValue=1)
aggRatesFactors <- aggMonthly(zoo(panelFactors$rates,order.by=panelFactors$dates))
aggPUFactors <- aggMonthly(zoo(panelFactors$prices,order.by=panelFactors$dates))
aggDatesFactors = index(aggPU)

# pca_fac = princomp(as.data.frame(aggRatesFactors),scores=TRUE)
pca_fac = prcomp(as.data.frame(aggRatesFactors),center=FALSE, scale. = FALSE, retx=TRUE)
nFactors = 3
factors = pca_fac$x[,seq(1,nFactors)]

#ACM Regression (1)
library(vars)
eq1 = vars::VAR(factors,p=1,type="const")
sampleT = NROW(eq1$datamat)
prInnov = residuals(eq1)
summVAR = summary(eq1)
Phi = as.matrix(Bcoef(eq1)[,-NCOL(Bcoef(eq1))],nFactors,nFactors)
mu = as.matrix(Bcoef(eq1)[,NCOL(Bcoef(eq1))],nFactors,1)
sigma = summVAR$covres
sigmaManual = t(Vhat)%*%Vhat/sampleT #sanity check
datesReg = row.names(eq1$datamat)
iniDateReg = datesReg[1]
endDateReg = tail(datesReg,1)

#risk free rate - using one month maturity
rfObj = nssMtyTS(21/252,startdate = datesSample[1],enddate = tail(datesSample,1))
# rf = rfObj$rate[seq(1,NROW(rfObj$price)-1)]
rfDiscrAnn = aggMonthly(rfObj$rate)
rfCont = compoundingTrans(rf)
puRate = 12*-log(aggPU[,1]) #sanity check
rfContHPR = -log(aggPU[,1])
rf = rfContHPR[-length(rfContHPR)]
#building excess return
#matrix bondLag is the bondPU data without the last maturity and lagged one period
#matrix bondLead is the bondPU data without the first maturity and leading one period
bondZoo = zoo(aggPU,order.by = aggDates)
bondLag = bondZoo[seq(1,NROW(bondZoo)-1),-1]
bondLead = bondZoo[seq(2,NROW(bondZoo)),-NCOL(bondZoo)]
returnZoo = as.matrix(log(bondLead)) - as.matrix(log(bondLag))
returnZooDiscreteAnn = (1+(as.matrix(bondLead) - as.matrix(bondLag))/as.matrix(bondLag))^(12)-1
rfMatAnn = (1+matrix(rfDiscrAnn,nrow=NROW(rfDiscrAnn),ncol=NCOL(returnZooDiscrete)))^(12/12) - 1
rxZoo = returnZooDiscrete - rfMatAnn
rxZooCont = returnZoo - matrix(rf,NROW(returnZoo),NCOL(returnZoo))
#ACM Regression (2)
dfExcRet = data.frame(rxZooCont)
row.names(dfExcRet) <- datesReg
factorsReg = factors[-NROW(factors),] #X_t
dfReg2 = data.frame(prInnov,factorsReg)
regCols <- function(i,dfY,dfX) {
  
  y = dfY[,i]
  dfReg = data.frame(y,dfX)
  return(lm(y ~ 1 + .,data=dfReg))
  
}
eq2 = lapply(seq(1,NCOL(dfExcRet)),regCols,dfExcRet,dfReg2)
nMatsEst = length(eq2)
#Collecting Residuals
eRes = matrix(0,nrow=NROW(dfExcRet),ncol=length(eq2))
for (j in 1:length(eq2)) {
  eRes[,j] = residuals(eq2[[j]])
}
sigma2Hat = sum(diag((t(eRes)%*%eRes)))/(NROW(eRes)*NCOL(eRes))

library(matlib)
#Calculating covariance in equations 9 (Beta)
# prBetas_t = cov(dfExcRet,prInnov)%*%inv(sigma)
# prBetas = t(prBetas_t)

#Manual Regression 2 - sanity check
lau_T = matrix(1,sampleT,1)
zTilde = as.matrix(cbind(lau_T,prInnov,factorsReg))
rxY = as.matrix(dfExcRet)
reg2 = inv((t(zTilde)%*%zTilde))%*%t(zTilde)%*%rxY

#Calculating Bstar
aHatBold = matrix(unlist(lapply(seq(1,length(eq2)),function(i) { eq2[[i]]$coefficients[[1]] })),nMatsEst,1)
betaHatBold = matrix(unlist(lapply(seq(1,length(eq2)),function(i) { eq2[[i]]$coefficients[seq(2,nFactors+1)] })),nFactors,nMatsEst)
cHatBold = matrix(unlist(lapply(seq(1,length(eq2)),function(i) { eq2[[i]]$coefficients[seq(length(eq2[[i]]$coefficients)-nFactors+1,length(eq2[[i]]$coefficients))] })),nMatsEst,nFactors,byrow = TRUE)

Bstar = t(apply(betaHatBold,2,function(colBeta) { c(colBeta%*%t(colBeta)) }))

#ACM Regression (3)
lau_N = matrix(1,nMatsEst,1)
lambda0Hat = inv((betaHatBold%*%t(betaHatBold)))%*%betaHatBold%*%(aHatBold+0.5*(Bstar%*%c(sigma)+sigma2Hat*lau_N))
lambda1Hat = inv((betaHatBold%*%t(betaHatBold)))%*%betaHatBold%*%cHatBold

#estimaçaõ dos deltas: parágrafo imediatamente anterior a seção 3.2
deltas = deltasEst(rf,factorsReg)
delta0 = deltas$delta0
delta1 = deltas$delta1

coef_RA = affineCoefs(lambda0Hat,lambda1Hat,delta0,delta1)
coef_RF = affineCoefs(0,0,delta0,delta1)

#Calculating Prices from Ans and Bns
A_RA = coef_RA$Ans
B_RA = coef_RA$Bns
A_RF = coef_RF$Ans
B_RF = coef_RF$Bns

#Calculating Risk-Premia
nM=2
rp = riskPremiaACM(A_RA[[nM]],B_RA[[nM]],A_RF[[nM]],B_RF[[nM]],nM,factorsReg)

  #Growth Pricing
  
  setwd("Z:/ASSET/ECO/Marcelo/Growth Pricing")
  
  library(zoo)
  library(xlsx)
  library(vars)
  library(VARsignR)
  
  source('Z:/ASSET/ECO/LATAM/MX/Codes/R/sourceFuns.R', echo=TRUE)
  
  yield_10y_us = haver.data('weekly:fcm10',rtype='zoo')
  
  data = importEviews("Z:/ASSET/ECO/Marcelo/Financial VAR/signData.csv")
  data = data[index(data)>"2005-01-01"]
  
  freq = 12
  cDummy= TRUE
  
  #Downloading data
  yld_10y <- data[,4]
  brlusd <- data[,1]
  cds_br = data[,2]
  br_1y = data[,5]
  usd_tw = data[,6]
  
  # yld_10y <- (yld_10y - lag(yld_10y,k=-freq))
  brlusd <- ((brlusd - lag(brlusd,k=-freq))/lag(brlusd,k=-freq))*100
  cds_br <- ((cds_br - lag(cds_br,k=-freq))/lag(cds_br,k=-freq))*100
  # br_1y = (br_1y - lag(br_1y,k=-freq))
  usd_tw <- (usd_tw - lag(usd_tw,k=-freq))/lag(usd_tw,k=-freq)
  
  simContr = 20000
  noLags = 4
  startSample = "2005-01-01"
  
  refDateDelta = "2016-11-08"
  
  #######################################
  #Identifying Policy Shock
  dataZoo = merge(yld_10y,brlusd,cds_br,br_1y,usd_tw)
  dataSRSVAR = dataZoo
  dataZoo = dataZoo[index(dataZoo)>startSample]
  dataZoo = squareData(dataZoo)
  
  dataSRSVAR = dataSRSVAR[index(dataSRSVAR)>startSample]
  dataSRSVAR = squareData(dataSRSVAR)
  dfSRSVAR = as.data.frame(dataSRSVAR)
  dataSRSVAR = ts(dataSRSVAR)
  
  constr <- c(-1,+2,-4) # shock of interest enters first.
  
  modelSRSVAR <- uhlig.reject(Y=dataSRSVAR, nlags=noLags, draws=simContr,
                              KMIN=1,KMAX=4, constrained=constr, constant=cDummy,
                              steps=36)
  
  fevd = modelSRSVAR$FEVDS
  irfs = modelSRSVAR$IRFS
  idShockDraws = modelSRSVAR$SHOCKS
  idShockMedian = apply(idShockDraws,2,median)
  
  irfMedian = apply(irfs,c(2,3),median)
  
  vl <- c("10 Yr Yield","BRL USD","CDS_BR","BR_1Y","USD_TW")
  
  us10yr_irf = as.zoo(irfMedian[,1])
  brl_usd_irf = as.zoo(irfMedian[,2])

irfplot(irfdraws=irfs, type="median", labels=vl, save=FALSE, bands=c(0.16, 0.84),
        grid=TRUE, bw=FALSE)

fevdplot(fevd, label=vl, save=FALSE, bands=c(0.16, 0.84), grid=TRUE,
         bw=FALSE, table=FALSE, periods=NULL)

fevd.table <- fevdplot(fevd, table=TRUE, label=vl, periods=c(1,10,20,30,40,50,60))

print(fevd.table)

depVarInd = 2
depVar= dataZoo[,depVarInd]
irfDraws = modelSRSVAR$IRFS[,,depVarInd]
irfMedian = apply(irfDraws,2,median)
nObs = NROW(dataZoo) - noLags

#Reduced Form
varOLS <- VAR(dataSRSVAR,p=noLags)
redRes = residuals(varOLS)
yldRes = redRes[,depVarInd]

#Identified Shock
idShockDraws = modelSRSVAR$SHOCKS
idShockMedian = apply(idShockDraws,2,median)
#stdShock = 1
stdShock = sd(idShockMedian)

coeffsDraws = modelSRSVAR$BDraws[,,depVarInd]
coeffsMedian = apply(coeffsDraws,2,median)

dateStart = refDateDelta
iniYld = depVar[which(index(depVar)==dateStart)]
#iniYld = window(yld,start = as.Date(dateStart), end = as.Date(dateStart))

dateEnd = index(dataZoo)[NROW(dataZoo)]
posDateStart = which(index(dataZoo)==dateStart)
posDateEnd = which(index(dataZoo)==dateEnd)
nShocksElem = posDateEnd - posDateStart
datesContrib = index(dataZoo[seq(posDateStart,posDateEnd)])
endYld = depVar[which(index(depVar)==dateEnd)]

contRedRes = yldRes[posDateStart]
totDepContrib = as.numeric(endYld[1]) - as.numeric(iniYld[1])
iniShock = idShockMedian[posDateStart-noLags]/stdShock
iniPolicyShockContrib = irfMedian[1]*iniShock
#iniResid - calcular do VAR

dataZooCreate = dataZoo
'Adding Lags to dataZoo'
for (l in 1:noLags) {
  currLag = lag(dataZooCreate,k=-l)
  colnames(currLag) <- c(paste("yld_l",l,sep=""),paste("msci_l",l,sep=""))
  dataZoo = merge(dataZoo,currLag)
}

shockContrib = rep(NA,nShocksElem)
policyLevContrib = rep(NA,nShocksElem)
deltaShockContrib = rep(NA,nShocksElem)
for (j in 1:nShocksElem) {
  
  policyLevContrib[j] = dataZoo[posDateStart+j-1,seq(3,(noLags+1)*2,by=2)]%*%coeffsMedian[seq(1,noLags*2,by=2)]
  currShock = idShockMedian[posDateStart+j-noLags]/stdShock
  shockContrib[j] = currShock%*%irfMedian[1]
  
  currVecShock = idShockMedian[(posDateStart+1-noLags):(posDateStart+j-noLags)]/stdShock
  currIrf = rev(irfMedian[1:j])
  deltaShockContrib[j] = currVecShock%*%currIrf
  
}

deltaPolicyContrib = deltaShockContrib
shockPolicyCumContrib = sum(shockContrib)
policyContrib = policyLevContrib + shockContrib
policyContrib = as.zoo(policyContrib,order.by = datesContrib)

#############################################
#Identifying Growth Shock
dataZoo = merge(msci,yld)
dataSRSVAR = dataZoo
dataZoo = dataZoo[index(dataZoo)>startSample]
dataZoo = squareData(dataZoo)

dataSRSVAR = dataSRSVAR[index(dataSRSVAR)>startSample]
dataSRSVAR = squareData(dataSRSVAR)
dfSRSVAR = as.data.frame(dataSRSVAR)
dataSRSVAR = ts(dataSRSVAR)

constr <- c(+1,+2)

modelSRSVAR <- uhlig.reject(Y=dataSRSVAR, nlags=noLags, draws=simContr, subdraws=simContr, nkeep=simContr*10, KMIN=1,
                            KMAX=3, constrained=constr, constant=cDummy, steps=100)


fevd = modelSRSVAR$FEVDS
irfs = modelSRSVAR$IRFS

vl <- c("MSCI","10 Yr Yield")


irfplot(irfdraws=irfs, type="median", labels=vl, save=FALSE, bands=c(0.16, 0.84),
        grid=TRUE, bw=FALSE)


fevdplot(fevd, label=vl, save=FALSE, bands=c(0.16, 0.84), grid=TRUE,
         bw=FALSE, table=FALSE, periods=NULL)

fevd.table <- fevdplot(fevd, table=TRUE, label=vl, periods=c(1,10,20,30,40,50,60))

print(fevd.table)

depVarInd = 2
depVar= dataZoo[,depVarInd]
irfDraws = modelSRSVAR$IRFS[,,depVarInd]
irfMedian = apply(irfDraws,2,median)

idShockDraws = modelSRSVAR$SHOCKS
idShockMedian = apply(idShockDraws,2,median)
#stdShock = 1
stdShock = sd(idShockMedian)

coeffsDraws = modelSRSVAR$BDraws[,,depVarInd]
coeffsMedian = apply(coeffsDraws,2,median)

dateStart = refDateDelta
iniYld = depVar[which(index(depVar)==dateStart)]
#iniYld = window(yld,start = as.Date(dateStart), end = as.Date(dateStart))

dateEnd = index(dataZoo)[NROW(dataZoo)]
posDateStart = which(index(dataZoo)==dateStart)
posDateEnd = which(index(dataZoo)==dateEnd)
nShocksElem = posDateEnd - posDateStart
datesContrib = index(dataZoo[seq(posDateStart,posDateEnd)])

iniShock = idShockMedian[posDateStart-noLags]/stdShock
iniGrowthShockContrib = irfMedian[1]*iniShock

dataZooCreate = dataZoo
'Adding Lags to dataZoo'
for (l in 1:noLags) {
  currLag = lag(dataZooCreate,k=-l)
  colnames(currLag) <- c(paste("msci_l",l,sep=""),paste("yld_l",l,sep=""))
  dataZoo = merge(dataZoo,currLag)
}

shockContrib = rep(NA,nShocksElem)
growthLevContrib = rep(NA,nShocksElem)
deltaShockContrib = rep(NA,nShocksElem)
deltaHead = rep(NA,nShocksElem)
deltaHeadSeq = rep(NA,nShocksElem)
for (j in 1:nShocksElem) {
  
  growthLevContrib[j] = dataZoo[posDateStart+j-1,seq(3,(noLags+1)*2,by=2)]%*%coeffsMedian[seq(1,noLags*2,by=2)]
  currShock = idShockMedian[posDateStart+j-noLags]/stdShock #shock lost due to lag creation
  shockContrib[j] = currShock%*%irfMedian[1]
  
  currVecShock = idShockMedian[(posDateStart+1-noLags):(posDateStart+j-noLags)]/stdShock
  currIrf = rev(irfMedian[1:j])
  deltaShockContrib[j] = currVecShock%*%currIrf
  
  deltaHead[j] = as.numeric(depVar[posDateStart+j]) - as.numeric(iniYld)
  deltaHeadSeq[j] = as.numeric(depVar[posDateStart+j]) - as.numeric(depVar[posDateStart+j-1])
  
}

deltaGrowthContrib = deltaShockContrib
shockGrowthCumContrib = sum(shockContrib)
growthContrib = growthLevContrib + shockContrib
growthContrib = as.zoo(growthContrib,order.by = datesContrib)

library(ggplot2)
library(reshape2)

contShock = iniPolicyShockContrib + iniGrowthShockContrib
explDepContrib = shockGrowthCumContrib + shockPolicyCumContrib

datesContribPlot = datesContrib[2:length(datesContrib)]
datesPlot = as.Date(datesContribPlot,'%m/%d/%Y')
value <- deltaGrowthContrib + deltaPolicyContrib
#value <- dataZoo[datesContrib,2]
variable = "Actual"
actualDF <- as.data.frame(value)
actualDF = cbind(actualDF,datesPlot,variable)
plotDF <- as.data.frame(cbind(deltaPolicyContrib,deltaGrowthContrib))
plotDF = cbind(plotDF,datesPlot)
meltedPlot <- melt(plotDF, id.vars = c('datesPlot'))

plot = ggplot(meltedPlot, aes(x=datesPlot,y=value)) +
  geom_area(aes(colour=variable, fill=variable)) +
  geom_line(data = actualDF,aes(y=value))
#scale_y_continuous(limits = c(-0.5, 1))

plot


#######################################
#Identifying Growth Shock
dataZoo = merge(msci,yld)
dataSRSVAR = dataZoo
dataZoo = dataZoo[index(dataZoo)>"2015-01-01"]
dataZoo = squareData(dataZoo)

dataSRSVAR = dataSRSVAR[index(dataSRSVAR)>"2015-01-01"]
dataSRSVAR = squareData(dataSRSVAR)
dfSRSVAR = as.data.frame(dataSRSVAR)
dataSRSVAR = ts(dataSRSVAR)

constr <- c(+1,+2)
varOLS <- VAR(dataSRSVAR)
modelSRSVAR <- uhlig.reject(Y=dataSRSVAR, nlags=noLags, draws=simContr, subdraws=simContr,
                            nkeep=simContr*10, KMIN=1,KMAX=3, constrained=constr, constant=cDummy, steps=60)

fevd = modelSRSVAR$FEVDS
irfs = modelSRSVAR$IRFS

vl <- c("MSCI","10 Yr Yield")


irfplot(irfdraws=irfs, type="median", labels=vl, save=FALSE, bands=c(0.16, 0.84),
        grid=TRUE, bw=FALSE)


fevdplot(fevd, label=vl, save=FALSE, bands=c(0.16, 0.84), grid=TRUE,
         bw=FALSE, table=FALSE, periods=NULL)

fevd.table <- fevdplot(fevd, table=TRUE, label=vl, periods=c(1,10,20,30,40,50,60))

print(fevd.table)

depVarInd = 1
depVar= dataZoo[,depVarInd]
irfDraws = modelSRSVAR$IRFS[,,depVarInd]
irfMedian = apply(irfDraws,2,median)

idShockDraws = modelSRSVAR$SHOCKS
idShockMedian = apply(idShockDraws,2,median)

coeffsDraws = modelSRSVAR$BDraws[,,depVarInd]
coeffsMedian = apply(coeffsDraws,2,median)

dateStart = "2016-11-08"
iniYld = depVar[which(index(depVar)==dateStart)]
#iniYld = window(yld,start = as.Date(dateStart), end = as.Date(dateStart))

dateEnd = index(dataZoo)[NROW(dataZoo)]
posDateStart = which(index(dataZoo)==dateStart)
posDateEnd = which(index(dataZoo)==dateEnd)
nShocksElem = posDateEnd - posDateStart + 1
datesContrib = index(dataZoo[seq(posDateStart,posDateEnd)])

dataZooCreate = dataZoo
'Adding Lags to dataZoo'
for (l in 1:noLags) {
  currLag = lag(dataZooCreate,k=-l)
  colnames(currLag) <- c(paste("yld_l",l,sep=""),paste("msci_l",l,sep=""))
  dataZoo = merge(dataZoo,currLag)
}

#stdShock = 1
stdShock = sd(idShockMedian)
shockContrib = rep(NA,nShocksElem)
growthLevContrib = rep(NA,nShocksElem)
for (j in 1:nShocksElem) {
  
  growthLevContrib[j] = dataZoo[posDateStart+j-1,seq(3,(noLags+1)*2,by=2)]%*%coeffsMedian[seq(1,noLags*2,by=2)]
  currShock = idShockMedian[posDateStart+j-1-noLags]/stdShock
  shockContrib[j] = currShock%*%irfMedian[1]
  
}

shockCumContrib = cumsum(shockContrib)
growthContrib = growthLevContrib + shockContrib
growthContrib = as.zoo(growthContrib,order.by = datesContrib)

#############################################
#Identifying Policy Shock
dataZoo = merge(yld,msci)
dataSRSVAR = dataZoo
dataZoo = dataZoo[index(dataZoo)>"2010-01-01"]
dataZoo = squareData(dataZoo)

dataSRSVAR = dataSRSVAR[index(dataSRSVAR)>"2010-01-01"]
dataSRSVAR = squareData(dataSRSVAR)
dfSRSVAR = as.data.frame(dataSRSVAR)
dataSRSVAR = ts(dataSRSVAR)

constr <- c(+1,-2)

modelSRSVAR <- uhlig.reject(Y=dataSRSVAR, nlags=noLags, draws=simContr, subdraws=simContr, nkeep=simContr*10, KMIN=1,
                            KMAX=3, constrained=constr, constant=cDummy, steps=60)


fevd = modelSRSVAR$FEVDS
irfs = modelSRSVAR$IRFS

vl <- c("10 Yr Yield","MSCI")


irfplot(irfdraws=irfs, type="median", labels=vl, save=FALSE, bands=c(0.16, 0.84),
        grid=TRUE, bw=FALSE)


fevdplot(fevd, label=vl, save=FALSE, bands=c(0.16, 0.84), grid=TRUE,
         bw=FALSE, table=FALSE, periods=NULL)

fevd.table <- fevdplot(fevd, table=TRUE, label=vl, periods=c(1,10,20,30,40,50,60))

print(fevd.table)

depVarInd = 2
depVar= dataZoo[,depVarInd]
irfDraws = modelSRSVAR$IRFS[,,depVarInd]
irfMedian = apply(irfDraws,2,median)

idShockDraws = modelSRSVAR$SHOCKS
idShockMedian = apply(idShockDraws,2,median)

coeffsDraws = modelSRSVAR$BDraws[,,depVarInd]
coeffsMedian = apply(coeffsDraws,2,median)

dateStart = "2016-11-08"
iniYld = depVar[which(index(depVar)==dateStart)]
#iniYld = window(yld,start = as.Date(dateStart), end = as.Date(dateStart))

dateEnd = index(dataZoo)[NROW(dataZoo)]
posDateStart = which(index(dataZoo)==dateStart)
posDateEnd = which(index(dataZoo)==dateEnd)
nShocksElem = posDateEnd - posDateStart + 1
datesContrib = index(dataZoo[seq(posDateStart,posDateEnd)])

dataZooCreate = dataZoo
'Adding Lags to dataZoo'
for (l in 1:noLags) {
  currLag = lag(dataZooCreate,k=-l)
  colnames(currLag) <- c(paste("msci_l",l,sep=""),paste("yld_l",l,sep=""))
  dataZoo = merge(dataZoo,currLag)
}

#stdShock = 1
stdShock = sd(idShockMedian)
shockContrib = rep(NA,nShocksElem)
polictLevContrib = rep(NA,nShocksElem)
for (j in 1:nShocksElem) {
  
  policyLevContrib[j] = dataZoo[posDateStart+j-1,seq(3,(noLags+1)*2,by=2)]%*%coeffsMedian[seq(1,noLags*2,by=2)]
  currShock = idShockMedian[posDateStart+j-1-noLags]/stdShock #first shock lost due to lag creation
  shockContrib[j] = currShock%*%irfMedian[1]
  
}

shockCumContrib = cumsum(shockContrib)
policyContrib = policyLevContrib + shockContrib
policyContrib = as.zoo(policyContrib,order.by = datesContrib)

library(ggplot2)
library(reshape2)

datesPlot = as.Date(datesContrib,'%m/%d/%Y')
value <- dataZoo[datesContrib,2]
variable = "Actual"
actualDF <- as.data.frame(value)
actualDF = cbind(actualDF,datesPlot,variable)
plotDF <- as.data.frame(cbind(policyContrib,growthContrib))
plotDF = cbind(plotDF,datesPlot)
meltedPlot <- melt(plotDF, id.vars = c('datesPlot'))

plot = ggplot(meltedPlot, aes(x=datesPlot,y=value)) +
  geom_area(aes(colour=variable, fill=variable)) +
  geom_line(data = actualDF,aes(y=value))
#scale_y_continuous(limits = c(-0.5, 1))

plot
library(Haver)
library(apt)
source('Z:/ASSET/ECO/LATAM/MX/Codes/R/haverZoo.R', echo=TRUE)

brep_rate = haverZoo('EMERGELA:N233RDE')
cloans_rate = haverZoo('EMERGELA:N233RLC')

tarDF = squareData(merge(brep_rate,cloans_rate))

timeIndex = index(tarDF[,1])

y = ts(tarDF[,2])
x = ts(tarDF[,1])

tarObj <- ciTarFit(y, x, model = c('tar'), lag = 0, thresh = 0)
mtarObj <- ciTarFit(y, x, model = c('mtar'), lag = 0, thresh = 0)

mktRate = zoo(y,order.by = timeIndex)
residual = zoo(mtarObj$z,order.by=timeIndex)
cb_rate = zoo(x,order.by = timeIndex)

require(lubridate)
#appending scenario to cb_rate
newDates = timeIndex[length(timeIndex)] %m+% months(1)
values = as.numeric(cb_rate)
cb_rate_scn = as.zoo(c(values,5.25),order.by = c(timeIndex,newDates))

expRates = as.data.frame(merge(cb_rate,residual),row.names=timeIndex)
library(xlsx)
write.xlsx(expRates,'Z:\\ASSET\\ECO\\Marcelo\\TAR Rates\\resTar.xlsx')

nLag = 6
lagReg = as.data.frame(squareData(merge(lag(cb_rate,k=-nLag),residual)))
colnames(lagReg) <- c(paste("cb_rate_l",nLag,sep=""),"residual")
lagEff = lm(residual ~ 0 + cb_rate_l6,data=lagReg)
summary(lagEff)

tarObj = mtarObj

#Forecasting residual
ciCoefs = summary(tarObj$CI)$coefficients[,1]
horizon = 12
currRes = residual[length(residual)]
preRes = residual[length(residual)-1]
critRes = as.numeric(currRes) - as.numeric(preRes) #momentum model
fctRes = c()
for (t in 1:horizon) {
  if (critRes > 0) {
    uptRes = (1+ciCoefs[1])*currRes
  }
  else {
    uptRes = (1+ciCoefs[2])*currRes
  }
  # if (t==6) { #epsilon shock in residual, if wanted
  #   uptRes = uptRes + summary(lagEff)$coefficients[1]*(-0.25)
  # }
  critRes = as.numeric(uptRes) - as.numeric(currRes)
  fctRes = c(fctRes,uptRes)
  currRes = uptRes
}
fctResSer = appZoo(fctRes,residual)

#Forecasting from tarObj - Long-Run
lrCoefs = summary(tarObj$LR)$coefficients[,1]
horizon = 12
cbScen = rep(5.25,horizon)
currSer = mktRate[length(mktRate)]
fctSer = c()
for (t in 1:horizon) {
  currSer = sum(c(1,cbScen[t])*lrCoefs) + fctRes[t]
  fctSer = c(fctSer,currSer)
  currSer = currSer
}
fctRate = appZoo(fctSer,mktRate)
fctCbRate = appZoo(cbScen,cb_rate)

expRates = as.data.frame(merge(fctCbRate,fctResSer),row.names=index(fctCbRate))
library(xlsx)
write.xlsx(expRates,'Z:\\ASSET\\ECO\\Marcelo\\TAR Rates\\resTar.xlsx')

appZoo = function(newValues,zooObj) {
  
  require(lubridate)
  newDates = index(zooObj)[length(zooObj)] %m+% months(seq(1,length(newValues)))
  newTimeIndex = c(timeIndex,newDates)
  newSer = as.zoo(c(as.numeric(zooObj),newValues),order.by = newTimeIndex)
  return(newSer)
  
}
source('Z:\\ASSET\\ECO\\LATAM\\MX\\Codes\\R\\importEviews.R')

x = importEviews('Z:\\ASSET\\ECO\\Marcelo\\teste_x.csv')
y = importEviews('Z:\\ASSET\\ECO\\Marcelo\\teste_y.csv')

source('Z:\\ASSET\\ECO\\LATAM\\MX\\Codes\\R\\bestSubsetFn.R')
source('Z:\\ASSET\\ECO\\LATAM\\MX\\Codes\\R\\squareData.R')
source('Z:\\ASSET\\ECO\\LATAM\\MX\\Codes\\R\\oosLin.R')

bestObj <-bestSubsetFn(y,x,recordModels = TRUE, horizon = 1,dRolling = 1,
                          subsetMin = 2,subsetMax = 2)

models = bestObj$modelsPerformance
library(xlsx)
write.xlsx(models, "modelsPerformance.xlsx")

# library(Haver)
# 
# source('Z:/ASSET/ECO/LATAM/MX/Codes/R/bestSubsetFn.R', echo=TRUE)
# 
# ip = as.zoo(haver.data('USECON:ip',rtype='zoo'))
# 
# ar1 = lag(ip,k=-1)
# ar2 = lag(ip,k=-2)
# ar3 = lag(ip,k=-3)
# car_prod = as.zoo(haver.data('USECON:PCRTM',rtype='zoo'))
# 
# y = ip
# x = merge(ar1,ar2,ar3,car_prod)
  #Selecting predictors for INPC Lines
  
  #Clears everything.
  rm(list = ls())
  
  source('Z:/ASSET/ECO/LATAM/MX/Codes/R/sourceFuns.R', echo=TRUE)
  
  library("zoo")
  
  'Inflation'
  dataDepPC <- importEviews("Z:/ASSET/ECO/LATAM/MX/Inflação/REL/R Selection/inpclines.csv")
  candidates <- importEviews("Z:/ASSET/ECO/LATAM/MX/Inflação/REL/R Selection/candidates.csv")
  
  'Generic'
  #dataDepPC <- importEviews("Z:/ASSET/ECO/LATAM/MX/Trackings/Selection/depVar.csv")
  #candidates <- importEviews("Z:/ASSET/ECO/LATAM/MX/Trackings/Selection/candidates_predictive.csv")
  
  minVar = 7
  maxVar = 12
  
  selList = list()
  nDep = NCOL(dataDepPC)
  depNames = names(dataDepPC)
  predNames = names(as.data.frame(candidates))
  
  candidates = candidates[index(candidates) >= "2010-01-01"]
  
  for (i in 1:nDep) {
  
    dep <- dataDepPC[,i]
    #Seasonality Term for Inflation
    depSAR <- lag(dep,-24)
    depAROne <- lag(dep,-1)
    
    selDF <- as.data.frame(cbind(dep,depSAR,depAROne,candidates))
    dataDF <- squareData(as.data.frame(selDF))
    y = as.matrix(dataDF[,1])
    x = as.matrix(dataDF[,-1])
    npInds = c(1)
    
    #Grid Lasso
    #adaObj <- gridLasso(x,y,minVar,maxVar,npInds)
    #selCV <- adaObj$selectedIndcs
    #lassoObj <- adaObj$lassoObj
    
    #Ada Lasso
    adaObj <- adaLasso(selDF,npInds)
    selCV <- lassoCV(adaObj,x,y,pen=4)
    nVarsSel = length(selCV)
  
    selNames = names(selCV)
    
    currList = c(depNames[i],"Selected Variables",selNames)
    selList[[i]] = currList
    
    print(i)
    
  }
  
  'Organzing output into Rel Table'
  lenSel <- sapply(selList,length)
  maxSel <- max( lenSel )
  naFill <- maxSel - lenSel
  tableSel <- mapply( function(x,y) c( x , rep( "" , y ) ) , selList , naFill)
  tableSel <- data.frame(t(tableSel))

library(xlsx)
if (ncol(tableSel) > 1) {
  write.xlsx(tableSel, "Z:/ASSET/ECO/LATAM/MX/Trackings/Selection/lassoSel.xlsx",
             row.names=FALSE,col.names = FALSE)
  write.csv(tableSel, "Z:/ASSET/ECO/LATAM/MX/Trackings/Selection/csvSel.csv",
            row.names=FALSE)
}

tr = "@pcy"
selTr = list()
library("stringr")

for (j in 2:ncol(tableSel)) {

  varName = as.character(tableSel[,j])
  
  #Identifying number of lags
  lagsStr = str_locate_all(varName,"_l")
  
  lagNum = 0
  lags = lagsStr[[1]]
  if (is.na(lags[1]) == FALSE) {
    for (k in 1:nrow(lags)) {
      lagNum = lagNum + as.integer(substr(varName,lags[k,2]+1,lags[k,2]+1))
    }
    lagNum = paste("(-",as.character(lagNum),")")  
  } else {
    lagNum = as.character()  
  }
  
  varHaver = substr(varName,1,regexpr('_',varName)-1)
  if (substr(varName,nchar(varName)-1,nchar(varName)-1) == "p") {
    power = substr(varName,nchar(varName),nchar(varName))
    trVar = paste("@pcy(EMERGELA::",varHaver,lagNum,")^",as.character(power))
  } else {
    trVar = paste("EMERGELA::",varHaver,lagNum)
  }
  trVar = gsub(" ", "", trVar, fixed = TRUE)
  selTr[[j-1]] = trVar
}

lenSel <- sapply(selTr,length)
maxSel <- max( lenSel )
naFill <- maxSel - lenSel
tableSelTr <- mapply( function(x,y) c( x , rep( "" , y ) ) , selTr , naFill)
tableSelTr <- data.frame(t(tableSelTr))
# 'Plotting Selected Series'
# plotList <- selList[[2]]
# plotDep <- plotList[1]
# plotPred <- plotList[3]
# depPC.df <- as.data.frame(dataDepPC)
# pred.df <- as.data.frame(candidates[-1,])
# plotSeriesDep <- depPC.df[plotDep]
# plotSeriesPred <- pred.df[plotPred]
# dates = rownames(depPC.df)
# plotZoo <- na.omit(zoo(as.zoo(cbind(plotSeriesDep,plotSeriesPred)),order.by=dates))
# p <- autoplot.zoo(plotZoo, facet = NULL)
